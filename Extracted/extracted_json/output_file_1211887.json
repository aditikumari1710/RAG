{
    "title": "Attention-Based Models for Speech Recognition",
    "content": {
        "page_content": "Attention-Based Models for Speech Recognition\nJan Chorowski\nUniversity of Wrocław, Poland\njan.chorowski@ii.uni.wroc.pl\nDzmitry Bahdanau\nJacobs University Bremen, Germany\nDmitriy Serdyuk\nUniversit´e de Montr´eal\nKyunghyun Cho\nUniversit´e de Montr´eal\nYoshua Bengio\nUniversit´e de Montr´eal\nCIFAR Senior Fellow\nAbstract\nRecurrent sequence generators conditioned on input data through an attention\nmechanism have recently shown very good performance on a range of tasks in-\ncluding machine translation, handwriting synthesis [1, 2] and image caption gen-\neration [3]. We extend the attention-mechanism with features needed for speech\nrecognition. We show that while an adaptation of the model used for machine\ntranslation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the\nTIMIT phoneme recognition task, it can only be applied to utterances which are\nroughly as long as the ones it was trained on. We offer a qualitative explanation of\nthis failure and propose a novel and generic method of adding location-awareness\nto the attention mechanism to alleviate this issue. The new method yields a model\nthat is robust to long inputs and achieves 18% PER in single utterances and 20%\nin 10-times longer (repeated) utterances. Finally, we propose a change to the at-\ntention mechanism that prevents it from concentrating too much on single frames,\nwhich further reduces PER to 17.6% level.\n1 Introduction\nRecently, attention-based recurrent networks have been successfully applied to a wide variety of\ntasks, such as handwriting synthesis [1], machine translation [2], image caption generation [3] and\nvisual object classiﬁcation [4]. 1 Such models iteratively process their input by selecting relevant\ncontent at every step. This basic idea signiﬁcantly extends the applicability range of end-to-end\ntraining methods, for instance, making it possible to construct networks with external memory [6, 7].\nWe introduce extensions to attention-based recurrent networks that make them applicable to speech\nrecognition. Learning to recognize speech can be viewed as learning to generate a sequence (tran-\nscription) given another sequence (speech). From this perspective it is similar to machine translation\nand handwriting synthesis tasks, for which attention-based methods have been found suitable [2, 1].\nHowever, compared to machine translation, speech recognition principally differs by requesting\nmuch longer input sequences (thousands of frames instead of dozens of words), which introduces a\nchallenge of distinguishing similar speech fragments2 in a single utterance. It is also different from\nhandwriting synthesis, since the input sequence is much noisier and does not have as clear structure.\nFor these reasons speech recognition is an interesting testbed for developing new attention-based\narchitectures capable of processing long and noisy inputs.\nApplication of attention-based models to speech recognition is also an important step toward build-\ning fully end-to-end trainable speech recognition systems, which is an active area of research. The\n1An early version of this work was presented at the NIPS 2014 Deep Learning Workshop [5].\n2Explained in more detail in Sec. 2.1.\n1\narXiv:1506.07503v1  [cs.CL]  24 Jun 2015dominant approach is still based on hybrid systems consisting of a deep neural acoustic model, a tri-\nphone HMM model and an n-gram language model [8, 9]. This requires dictionaries of hand-crafted\npronunciation and phoneme lexicons, and a multi-stage training procedure to make the components\nwork together. Excellent results by an HMM-less recognizer have recently been reported, with the\nsystem consisting of a CTC-trained neural network and a language model [10]. Still, the language\nmodel was added only at the last stage in that work, thus leaving open a question of how much an\nacoustic model can beneﬁt from being aware of a language model during training.\nIn this paper, we evaluate attention-based models on a phoneme recognition task using the widely-\nused TIMIT dataset. At each time step in generating an output sequence (phonemes), an attention\nmechanism selects or weighs the signals produced by a trained feature extraction mechanism at po-\ntentially all of the time steps in the input sequence (speech frames). The weighted feature vector then\nhelps to condition the generation of the next element of the output sequence. Since the utterances\nin this dataset are rather short (mostly under 5 seconds), we measure the ability of the considered\nmodels in recognizing much longer utterances which were created by artiﬁcially concatenating the\nexisting utterances.\nWe start with a model proposed in [2] for the machine translation task as the baseline. This model\nseems entirely vulnerable to the issue of similar speech fragments but despite our expectations it\nwas competitive on the original test set, reaching 18.7% phoneme error rate (PER). However, its\nperformance degraded quickly with longer, concatenated utterances. We provide evidence that this\nmodel adapted to track the absolute location in the input sequence of the content it is recognizing, a\nstrategy feasible for short utterances from the original test set but inherently unscalable.\nIn order to circumvent this undesired behavior, in this paper, we propose to modify the attention\nmechanism such that it explicitly takes into account both (a) the location of the focus from the\nprevious step, as in [6] and (b) the features of the input sequence, as in [2]. This is achieved by\nadding as inputs to the attention mechanism auxiliary convolutional features which are extracted by\nconvolving the attention weights from the previous step with trainable ﬁlters. We show that a model\nwith such convolutional features performs signiﬁcantly better on the considered task (18.0% PER).\nMore importantly, the model with convolutional features robustly recognized utterances many times\nlonger than the ones from the training set, always staying below 20% PER.\nTherefore, the contribution of this work is three-fold. For one, we present a novel purely neural\nspeech recognition architecture based on an attention mechanism, whose performance is compara-\nble to that of the conventional approaches on the TIMIT dataset. Moreover, we propose a generic\nmethod of adding location awareness to the attention mechanism. Finally, we introduce a modiﬁ-\ncation of the attention mechanism to avoid concentrating the attention on a single frame, and thus\navoid obtaining less “effective training examples”, bringing the PER down to 17.6%.\n2 Attention-Based Model for Speech Recognition\n2.1 General Framework\nAn attention-based recurrent sequence generator (ARSG) is a recurrent neural network that stochas-\ntically generates an output sequence (y1,...,y T ) from an input x. In practice, xis often processed\nby an encoder which outputs a sequential input representation h = (h1,...,h L) more suitable for\nthe attention mechanism to work with.\nIn the context of this work, the outputyis a sequence of phonemes, and the inputx= (x1,...,x L′ )\nis a sequence of feature vectors. Each feature vector is extracted from a small overlapping window\nof audio frames. The encoder is implemented as a deep bidirectional recurrent network (BiRNN),\nto form a sequential representation hof length L= L′.\nAt the i-th step an ARSG generates an output yi by focusing on the relevant elements of h:\nαi = Attend(si−1,αi−1,h) (1)\ngi =\nL∑\nj=1\nαi,jhj (2)\nyi ∼Generate(si−1,gi), (3)\n2yi\ngi\nhj-1 hj hj+1\ngi+1\nyi+1\nsi+1\n×αi-1,j-1 ×αi-1,j ×αi-1,j+1\nsi-1 si\n×αi,j-1 ×αi,j ×αi,j+1\nαi\nFigure 1: Two steps of the pro-\nposed attention-based recurrent se-\nquence generator (ARSG) with a hy-\nbrid attention mechanism (computing\nα), based on both content ( h) and lo-\ncation (previous α) information. The\ndotted lines correspond to Eq. (1),\nthick solid lines to Eq. (2) and dashed\nlines to Eqs. (3)–(4).\nwhere si−1 is the (i−1)-th state of the recurrent neural network to which we refer as the gener-\nator, αi ∈RL is a vector of the attention weights, also often called the alignment [2]. Using the\nterminology from [4], we call gi a glimpse. The step is completed by computing a new generator\nstate:\nsi = Recurrency(si−1,gi,yi) (4)\nLong short-term memory units (LSTM, [11]) and gated recurrent units (GRU, [12]) are typically\nused as a recurrent activation, to which we refer as a recurrency. The process is graphically illus-\ntrated in Fig. 1.\nInspired by [6] we distinguish between location-based, content-based and hybrid attention mecha-\nnisms. Attendin Eq. (1) describes the most generic, hybrid attention. If the term αi−1 is dropped\nfrom Attendarguments, i.e., αi = Attend(si−1,h), we call it content-based (see, e.g., [2] or [3]).\nIn this case, Attendis often implemented by scoring each element in hseparately and normalizing\nthe scores:\nei,j = Score(si−1,hj), (5)\nαi,j = exp(ei,j)\n/ L∑\nj=1\nexp(ei,j) . (6)\nThe main limitation of such scheme is that identical or very similar elements ofhare scored equally\nregardless of their position in the sequence. This is the issue of “similar speech fragments” raised\nabove. Often this issue is partially alleviated by an encoder such as e.g. a BiRNN [2] or a deep\nconvolutional network [3] that encode contextual information into every element of h. However,\ncapacity of helements is always limited, and thus disambiguation by context is only possible to a\nlimited extent.\nAlternatively, a location-based attention mechanism computes the alignment from the generator\nstate and the previous alignment only such that αi = Attend(si−1,αi−1). For instance, Graves\n[1] used the location-based attention mechanism using a Gaussian mixture model in his handwriting\nsynthesis model. In the case of speech recognition, this type of location-based attention mechanism\nwould have to predict the distance between consequent phonemes usingsi−1 only, which we expect\nto be hard due to large variance of this quantity.\nFor these limitations associated with both content-based and location-based mechanisms, we argue\nthat a hybrid attention mechanism is a natural candidate for speech recognition. Informally, we\nwould like an attention model that uses the previous alignmentαi−1 to select a short list of elements\nfrom h, from which the content-based attention, in Eqs. (5)–(6), will select the relevant ones without\nconfusion.\n2.2 Proposed Model: ARSG with Convolutional Features\nWe start from the ARSG-based model with the content-based attention mechanism proposed in [2].\nThis model can be described by Eqs. (5)–(6), where\nei,j = w⊤tanh(Wsi−1 + Vhj + b). (7)\nwand bare vectors, W and V are matrices.\n3We extend this content-based attention mechanism of the original model to be location-aware by\nmaking it take into account the alignment produced at the previous step. First, we extract kvectors\nfi,j ∈ Rk for every position j of the previous alignment αi−1 by convolving it with a matrix\nF ∈Rk×r:\nfi = F ∗αi−1. (8)\nThese additional vectors fi,j are then used by the scoring mechanism ei,j:\nei,j = w⊤tanh(Wsi−1 + Vhj + Ufi,j + b) (9)\n2.3 Score Normalization: Sharpening and Smoothing\nThere are three potential issues with the normalization in Eq. (6).\nFirst, when the input sequence his long, the glimpse gi is likely to contain noisy information from\nmany irrelevant feature vectors hj, as the normalized scores αi,j are all positive and sum to 1. This\nmakes it difﬁcult for the proposed ARSG to focus clearly on a few relevant frames at each time\ni. Second, the attention mechanism is required to consider all the Lframes each time it decodes a\nsingle output yi while decoding the output of length T, leading to a computational complexity of\nO(LT). This may easily become prohibitively expensive, when input utterances are long (and issue\nthat is less serious for machine translation, because in that case the input sequence is made of words,\nnot of 20ms acoustic frames).\nThe other side of the coin is that the use of softmax normalization in Eq. (6) prefers to mostly focus\non only a single feature vector hj. This prevents the model from aggregating multiple top-scored\nframes to form a glimpse gi.\nSharpening There is a straightforward way to address the ﬁrst issue of a noisy glimpse by “sharp-\nening” the scoresαi,j. One way to sharpen the weights is to introduce aninverse temperatureβ >1\nto the softmax function such that\nai,j = exp(βei,j)\n/ L∑\nj=1\nexp(βei,j) ,\nor to keep only the top- kframes according to the scores and re-normalize them. These sharpening\nmethods, however, still requires us to compute the score of every frame each time ( O(LT)), and\nthey worsen the second issue, of overly narrow focus.\nWe also propose and investigate a windowing technique. At each time i, the attention mechanism\nconsiders only a subsequence˜h= (hpi−w,...,h pi+w−1) of the whole sequenceh, where w≪Lis\nthe predeﬁned window width and pi is the median of the alignment αi−1. The scores for hj /∈˜hare\nnot computed, resulting in a lower complexity of O(L+ T). This windowing technique is similar\nto taking the top-kframes, and similarly, has the effect of sharpening.\nThe proposed sharpening based on windowing can be used both during training and evaluation.\nLater, in the experiments, we only consider the case where it is used during evaluation.\nSmoothing We observed that the proposed sharpening methods indeed helped with long utter-\nances. However, all of them, and especially selecting the frame with the highest score, negatively\naffected the model’s performance on the standard development set which mostly consists of short ut-\nterances. This observations let us hypothesize that it is helpful for the model to aggregate selections\nfrom multiple top-scored frames. In a sense this brings more diversity, i.e., more effective training\nexamples, to the output part of the model, as more input locations are considered. To facilitate this\neffect, we replace the unbounded exponential function of the softmax function in Eq. (6) with the\nbounded logistic sigmoid σsuch that\nai,j = σ(ei,j)\n/ L∑\nj=1\nσ(ei,j) .\nThis has the effect of smoothing the focus found by the attention mechanism.\n4● ●●● ● ● ● ● ●●● ● ● ● ● ●●● ● ● ●\nBaseline Conv Feats Smooth Focus\n16\n17\n18\n19\n1 2 5 10 20 50 100 1 2 5 10 20 50 100 1 2 5 10 20 50 100\nBeam width\nPhoneme Error Rate [%]\nDataset\n● dev\ntest\nDependency of error rate on beam search width.\nFigure 2: Decoding performance w.r.t. the beam size. For rigorous comparison, if decoding failed\nto generate ⟨eos⟩, we considered it wrongly recognized without retrying with a larger beams size.\nThe models, especially with smooth focus, perform well even with a beam width as small as 1.\n3 Related Work\nSpeech recognizers based on the connectionist temporal classiﬁcation (CTC, [13]) and its extension,\nRNN Transducer [14], are the closest to the ARSG model considered in this paper. They follow\nearlier work on end-to-end trainable deep learning over sequences with gradient signals ﬂowing\nthrough the alignment process [15]. They have been shown to perform well on the phoneme recog-\nnition task [16]. Furthermore, the CTC was recently found to be able to directly transcribe text from\nspeech without any intermediate phonetic representation [17].\nThe considered ARSG is different from both the CTC and RNN Transducer in two ways. First,\nwhereas the attention mechanism deterministically aligns the input and the output sequences, the\nCTC and RNN Transducer treat the alignment as a latent random variable over which MAP (max-\nimum a posteriori) inference is performed. This deterministic nature of the ARSG’s alignment\nmechanism allows beam search procedure to be simpler. Furthermore, we empirically observe that\na much smaller beam width can be used with the deterministic mechanism, which allows faster\ndecoding (see Sec. 4.2 and Fig. 2). Second, the alignment mechanism of both the CTC and RNN\nTransducer is constrained to be “monotonic” to keep marginalization of the alignment tractable. On\nthe other hand, the proposed attention mechanism can result in non-monotonic alignment, which\nmakes it suitable for a larger variety of tasks other than speech recognition.\nA hybrid attention model using a convolution operation was also proposed in [6] for neural Turing\nmachines (NTM). At each time step, the NTM computes content-based attention weights which are\nthen convolved with a predicted shifting distribution. Unlike the NTM’s approach, the hybrid mech-\nanism proposed here lets learning ﬁgure out how the content-based and location-based addressing\nbe combined by a deep, parametric function (see Eq. (9).)\nSukhbaatar et al. [18] describes a similar hybrid attention mechanism, where location embeddings\nare used as input to the attention model. This approach has an important disadvantage that the model\ncannot work with an input sequence longer than those seen during training. Our approach, on the\nother hand, works well on sequences many times longer than those seen during training (see Sec. 5.)\n4 Experimental Setup\nWe closely followed the procedure in [16]. All experiments were performed on the TIMIT corpus\n[19]. We used the train-dev-test split from the Kaldi [20] TIMIT s5 recipe. We trained on the\nstandard 462 speaker set with all SA utterances removed and used the 50 speaker dev set for early\nstopping. We tested on the 24 speaker core test set. All networks were trained on 40 mel-scale ﬁlter-\nbank features together with the energy in each frame, and ﬁrst and second temporal differences,\nyielding in total 123 features per frame. Each feature was rescaled to have zero mean and unit\nvariance over the training set. Networks were trained on the full 61-phone set extended with an\nextra “end-of-sequence” token that was appended to each target sequence. Similarly, we appended\nan all-zero frame at the end of each input sequence to indicate the end of the utterance. Decoding\nwas performed using the 61+1 phoneme set, while scoring was done on the 39 phoneme set.\n4.1 Training Procedure\nOne property of ARSG models is that different subsets of parameters are reused different number of\ntimes; Ltimes for those of the encoder, LT for the attention weights and T times for all the other\n5h# m ay kcl k el kcl k ah l er dcl dhix bcl b eh dcl d r ux m w ao l w ix th kcl k r ey aa n s h#\nFDHC0_SX209: Michael colored the bedroom wall with crayons.\nFigure 3: Alignments produced by the baseline model. The vertical bars indicate ground truth\nphone location from TIMIT. Each row of the upper image indicates frames selected by the attention\nmechanism to emit a phone symbol. The network has clearly learned to produce a left-to-right\nalignment with a tendency to look slightly ahead, and does not confuse between the repeated “kcl-\nk” phrase. Best viewed in color.\nparameters of the ARSG. This makes the scales of derivatives w.r.t. parameters vary signiﬁcantly,\nand we handle it by using an adaptive learning rate algorithm, AdaDelta [21] which has two hyper-\nparameters ϵand ρ. All the weight matrices were initialized from a normal Gaussian distribution\nwith its standard deviation set to 0.01. Recurrent weights were furthermore orthogonalized.\nAs TIMIT is a relatively small dataset, proper regularization is crucial. We used the adaptive weight\nnoise as a main regularizer [22]. We ﬁrst trained our models with a column norm constraint [23] with\nthe maximum norm 1 until the lowest development negative log-likelihood is achieved.3 During this\ntime, ϵ and ρ are set to 10−8 and 0.95, respectively. At this point, we began using the adaptive\nweight noise, and scaled down the model complexity cost LC by a factor of 10, while disabling\nthe column norm constraints. Once the new lowest development log-likelihood was reached, we\nﬁne-tuned the model with a smaller ϵ = 10−10, until we did not observe the improvement in the\ndevelopment phoneme error rate (PER) for 100K weight updates. Batch size 1 was used throughout\nthe training.\n4.2 Details of Evaluated Models\nWe evaluated the ARSGs with different attention mechanisms. The encoder was a 3-layer BiRNN\nwith 256 GRU units in each direction, and the activations of the 512 top-layer units were used as the\nrepresentation h. The generator had a single recurrent layer of 256 GRU units. Generatein Eq. (3)\nhad a hidden layer of 64 maxout units. The initial states of both the encoder and generator were\ntreated as additional parameters.\nOur baseline model is the one with a purely content-based attention mechanism (See Eqs. (5)–(7).)\nThe scoring network in Eq. (7) had 512 hidden units. The other two models use the convolutional\nfeatures in Eq. (8) with k= 10and r= 201. One of them uses the smoothing from Sec. 2.3.\nDecoding Procedure A left-to-right beam search over phoneme sequences was used during de-\ncoding [24]. Beam search was stopped when the “end-of-sequence” token ⟨eos⟩was emitted. We\nstarted with a beam width of 10, increasing it up to 40 when the network failed to produce⟨eos⟩with\nthe narrower beam. As shown in Fig. 2, decoding with a wider beam gives little-to-none beneﬁt.\n5 Results\nAll the models achieved competitive PERs (see Table 1). With the convolutional features, we see\n3.7% relative improvement over the baseline and further 5.9% with the smoothing.\nTo our surprise (see Sec. 2.1.), the baseline model learned to align properly. An alignment pro-\nduced by the baseline model on a sequence with repeated phonemes (utterance FDHC0 SX209) is\npresented in Fig. 3 which demonstrates that the baseline model is not confused by short-range repe-\ntitions. We can also see from the ﬁgure that it prefers to select frames that are near the beginning or\n3 Applying the weight noise from the beginning of training caused severe underﬁtting.\n6Table 1: Phoneme error rates (PER). The bold-faced PER corresponds to the best error rate with an\nattention-based recurrent sequence generator (ARSG) incorporating convolutional attention features\nand a smooth focus.\nModel Dev Test\nBaseline Model 15.9% 18.7%\nBaseline + Conv. Features 16.1% 18.0%\nBaseline + Conv. Features + Smooth Focus 15.8% 17.6%\nRNN Transducer [16] N/A 17.7%\nHMM over Time and Frequency Convolutional Net [25] 13.9% 16.7%\nFigure 4: Results of force-aligning the concatenated utterances. Each dot represents a single utter-\nance created by either concatenating multiple copies of the same utterance, or of different, randomly\nchosen utterances. We clearly see that the highest robustness is achieved when the hybrid attention\nmechanism is combined with the proposed sharpening technique (see the bottom-right plot.)\neven slightly before the phoneme location provided as a part of the dataset. The alignments produced\nby the other models were very similar visually.\n5.1 Forced Alignment of Long Utterances\nThe good performance of the baseline model led us to the question of how it distinguishes between\nrepetitions of similar phoneme sequences and how reliably it decodes longer sequences with more\nrepetitions. We created two datasets of long utterances; one by repeating each test utterance, and the\nother by concatenating randomly chosen utterances. In both cases, the waveforms were cross-faded\nwith a 0.05s silence inserted as the “pau” phone. We concatenated up to15 utterances.\nFirst, we checked the forced alignment with these longer utterances by forcing the generator to emit\nthe correct phonemes. Each alignment was considered correct if 90% of the alignment weight lies\ninside the ground-truth phoneme window extended by 20 frames on each side. Under this deﬁnition,\nall phones but the ⟨eos⟩shown in Fig. 3 are properly aligned.\nThe ﬁrst column of Fig. 4 shows the number of correctly aligned frames w.r.t. the utterance length\n(in frames) for some of the considered models. One can see that the baseline model was able to\ndecode sequences up to about 120 phones when a single utterance was repeated, and up to about 150\nphones when different utterances were concatenated. Even when it failed, it correctly aligned about\n50 phones. On the other hand, the model with the hybrid attention mechanism with convolutional\nfeatures was able to align sequences up to 200 phones long. However, once it began to fail, the\nmodel was not able to align almost all phones. The model with the smoothing behaved similarly to\nthe one with convolutional features only.\nWe examined failed alignments to understand these two different modes of failure. Some of the\nexamples are shown in the Supplementary Materials.\nWe found that the baseline model properly aligns about 40 ﬁrst phones, then makes a jump to the\nend of the recording and cycles over the last 10 phones. This behavior suggests that it learned to\ntrack its approximate location in the source sequence. However, the tracking capability is limited to\nthe lengths observed during training. Once the tracker saturates, it jumps to the end of the recording.\n7Baseline Conv Feats Smooth Focus\n●\n●\n●\n●●\n●\n●\n●\n●●\n●\n●●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●●\n●\n●\n18\n20\n22\n24\n3 6 9 3 6 9 3 6 9\nNumber of repetitions\nPhoneme error rate [%]\nDecoding algorithm\n●\n●\nKeep 20\nKeep 50\nWin ± 75\nWin ± 150\nDataset\nMixed Utt.\nSame Utt.\nPhoneme error rates on long utterances\nFigure 5: Phoneme error rates obtained on decoding long sequences. Each network was de-\ncoded with alignment sharpening techniques that produced proper forced alignments. The proposed\nARSG’s are clearly more robust to the length of the utterances than the baseline one is.\nIn contrast, when the location-aware network failed it just stopped aligning – no particular frames\nwere selected for each phone. We attribute this behavior to the issue of noisy glimpse discussed in\nSec. 2.3. With a long utterance there are many irrelevant frames negatively affecting the weight as-\nsigned to the correct frames. In line with this conjecture, the location-aware network works slightly\nbetter on the repetition of the same utterance, where all frames are somehow relevant, than on the\nconcatenation of different utterances, where each misaligned frame is irrelevant.\nTo gain more insight we applied the alignment sharpening schemes described in Sec. 2.3. In the\nremaining columns of Fig. 4, we see that the sharpening methods help the location-aware network\nto ﬁnd proper alignments, while they show little effect on the baseline network. The windowing\ntechnique helps both the baseline and location-aware networks, with the location-aware network\nproperly aligning nearly all sequences.\nDuring visual inspection, we noticed that in the middle of very long utterances the baseline model\nwas confused by repetitions of similar content within the window, and that such confusions did not\nhappen in the beginning. This supports our conjecture above.\n5.2 Decoding Long Utterances\nWe evaluated the models on long sequences. Each model was decoded using the alignment sharp-\nening techniques that helped to obtain proper forced alignments. The results are presented in Fig. 5.\nThe baseline model fails to decode long utterances, even when a narrow window is used to constrain\nthe alignments it produces. The two other location-aware networks are able to decode utterances\nformed by concatenating up to 11 test utterances. Better results were obtained with a wider window,\npresumably because it resembles more the training conditions when at each step the attention mech-\nanism was seeing the whole input sequence. With the wide window, both of the networks scored\nabout 20% PER on the long utterances, indicating that the proposed location-aware attention mecha-\nnism can scale to sequences much longer than those in the training set with only minor modiﬁcations\nrequired at the decoding stage.\n6 Conclusions\nWe proposed and evaluated a novel end-to-end trainable speech recognition architecture based on a\nhybrid attention mechanism which combines both content and location information in order to select\nthe next position in the input sequence for decoding. One desirable property of the proposed model\nis that it can recognize utterances much longer than the ones it was trained on. In the future, we\nexpect this model to be used to directly recognize text from speech [10, 17], in which case it may\nbecome important to incorporate a monolingual language model to the ARSG architecture [26].\nThis work has contributed two novel ideas for attention mechanisms: a better normalization ap-\nproach yielding smoother alignments and a generic principle for extracting and using features from\nthe previous alignments. Both of these can potentially be applied beyond speech recognition. For\ninstance, the proposed attention can be used without modiﬁcation in neural Turing machines, or by\nusing 2–D convolution instead of 1–D, for improving image caption generation [3].\n8Acknowledgments\nAll experiments were conducted using Theano [27, 28], PyLearn2 [29], and Blocks [30] libraries.\nThe authors would like to acknowledge the support of the following agencies for research fund-\ning and computing support: National Science Center (Poland), NSERC, Calcul Qu ´ebec, Compute\nCanada, the Canada Research Chairs and CIFAR. Bahdanau also thanks Planet Intelligent Systems\nGmbH and Yandex.\nReferences\n[1] Alex Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, August 2013.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning\nto align and translate. arXiv:1409.0473, September 2014.\n[3] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard\nZemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention.\narXiv:1502.03044, February 2015.\n[4] V olodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In Advances\nin Neural Information Processing Systems, pages 2204–2212, 2014.\n[5] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end continuous speech\nrecognition using attention-based recurrent NN: First results.arXiv:1412.1602 [cs, stat], December 2014.\n[6] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv:1410.5401, 2014.\n[7] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv:1410.3916, 2014.\n[8] Mark Gales and Steve Young. The application of hidden markov models in speech recognition. Found.\nTrends Signal Process., 1(3):195–304, January 2007.\n[9] G. Hinton, Li Deng, Dong Yu, G.E. Dahl, A Mohamed, N. Jaitly, A Senior, V . Vanhoucke, P. Nguyen,\nT.N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The\nshared views of four research groups. IEEE Signal Processing Magazine, 29(6):82–97, November 2012.\n[10] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,\nSanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deepspeech: Scaling up end-to-end speech\nrecognition. arXiv preprint arXiv:1412.5567, 2014.\n[11] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural. Comput., 9(8):1735–1780, 1997.\n[12] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua\nBengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation.\nIn EMNLP 2014, October 2014. to appear.\n[13] Alex Graves, Santiago Fern ´andez, Faustino Gomez, and J ¨urgen Schmidhuber. Connectionist temporal\nclassiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In ICML-06, 2006.\n[14] Alex Graves. Sequence transduction with recurrent neural networks. In ICML-12, 2012.\n[15] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient based learning applied to document recognition.\nProc. IEEE, 1998.\n[16] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent\nneural networks. In ICASSP 2013, pages 6645–6649. IEEE, 2013.\n[17] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks.\nIn ICML-14, pages 1764–1772, 2014.\n[18] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. Weakly supervised memory net-\nworks. arXiv preprint arXiv:1503.08895, 2015.\n[19] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren. DARPA TIMIT\nacoustic phonetic continuous speech corpus, 1993.\n[20] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko\nHannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, and others. The kaldi speech recognition toolkit.\nIn Proc. ASRU, pages 1–4, 2011.\n[21] Matthew D Zeiler. ADADELTA: An adaptive learning rate method. arXiv:1212.5701, 2012.\n[22] Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R.S. Zemel, P.L.\nBartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems\n24, pages 2348–2356. Curran Associates, Inc., 2011.\n9[23] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-\nnov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580, 2012.\n[24] Ilya Sutskever, Oriol Vinyals, and Quoc V . Le. Sequence to sequence learning with neural networks.\narXiv preprint arXiv:1409.3215, 2014.\n[25] L ´aszl´o T´oth. Combining time-and frequency-domain convolution in convolutional neural network-based\nphone recognition. In ICASSP 2014, pages 190–194, 2014.\n[26] Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. On using monolingual corpora in neural machine translation.arXiv\npreprint arXiv:1503.03535, 2015.\n[27] James Bergstra, Olivier Breuleux, Fr ´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Des-\njardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expres-\nsion compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy) , June 2010.\nOral Presentation.\n[28] Fr ´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron,\nNicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning\nand Unsupervised Feature Learning NIPS 2012 Workshop, 2012.\n[29] Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pas-\ncanu, James Bergstra, Fr ´ed´eric Bastien, and Yoshua Bengio. Pylearn2: a machine learning research\nlibrary. arXiv preprint arXiv:1308.4214, 2013.\n[30] Bart van Merri ¨enboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan\nChorowski, and Yoshua Bengio. Blocks and fuel: Frameworks for deep learning. arXiv:1506.00619 [cs,\nstat], June 2015.\n10A Additional Figures\nBaseline\nh# m ay kcl k el kcl k ah l er dcl dhix bcl b eh dcl d r ux m w ao l w ix th kcl k r ey aa n s h#\nFDHC0_SX209: Michael colored the bedroom wall with crayons.\nConvolutional Features\nh# m ay kcl k el kcl k ah l er dcl dhix bcl b eh dcl d r ux m w ao l w ix th kcl k r ey aa n s h#\nFDHC0_SX209: Michael colored the bedroom wall with crayons.\nSmooth Focus\nh# m ay kcl k el kcl k ah l er dcl dhix bcl b eh dcl d r ux m w ao l w ix th kcl k r ey aa n s h#\nFDHC0_SX209: Michael colored the bedroom wall with crayons.\nFigure 6: Alignments produced by evaluated models on the FDHC0 SX209 test utterance. The ver-\ntical bars indicate ground truth phone location from TIMIT. Each row of the upper image indicates\nframes selected by the attention mechanism to emit a phone symbol. Compare with Figure 3. in the\nmain text.\n11Baseline\nh# w ah dx aw q f ix tcl d uh sh iy dcl d r ay v f ao h#\nFAEM0_SI2022: What outfit does she drive for?\nConvolutional Features\nh# w ah dx aw q f ix tcl d uh sh iy dcl d r ay v f ao h#\nFAEM0_SI2022: What outfit does she drive for?\nSmooth Focus\nh# w ah dx aw q f ix tcl d uh sh iy dcl d r ay v f ao h#\nFAEM0_SI2022: What outfit does she drive for?\nFigure 7: Alignments produced by evaluated models on the FAEM0 SI2022 train utterance. The\nvertical bars indicate ground truth phone location from TIMIT. Each row of the upper image indi-\ncates frames selected by the attention mechanism to emit a phone symbol. Compare with Figure 3.\nin the main text.\n12Same Utt. Mixed Utt.\n0\n100\n200\n300\n400\n0\n100\n200\n300\n400\nBaselineConv Feats\n0 200 400 600 0 200 400 600\nUtterance length [phones]\nNumber of correctly aligned phones\nNumber of incorrectly aligned phones vs utterance length and model.\nFigure 8: Close-up on the two failure modes of ARSG. Results of force-aligning concatenated\nTIMIT utterances. Each dot represents a single utterance. The left panels show results for con-\ncatenations of the same utterance. The right panels show results for concatenations of randomly\nchosen utterances. We compare the baseline network having a content-based only attention mech-\nanism (top row) with a hybrid attention mechanism that uses convolutional features (bottom row).\nWhile neither model is able to properly align long sequences, they fail in different ways: the baseline\nnetwork always aligns about 50 phones, while the location-aware network fails to align any phone.\nCompare with Figure 4 form the main paper.\n13Figure 9: The baseline network fails to align more than 3 repetitions of FDHC0 SX209.\n14Figure 10: The baseline network aligns a concatenation of 3 different utterances, but fails to align 5.\n15Figure 11: Forced alignment of 7 repetitions of the phrase “Michael colored” performed with the baseline model with windowing enabled (the alignment was\nconstrained to ±75 frames from the expected position of the generator at the last step. The window is wider than the pattern and the net confuses similar content.\nStrangely, the ﬁrst two repetitions are aligned without any confusion with subsequent ones – the network starts to confound phoneme location only starting from\nthe third repetition (as seen by the parallel strand of alignment which starts when the network starts to emit the phrase for the third time).\n16Figure 12: The location-aware network correctly aligns 7 and 11 repetitions of FDHC0 SX209, butfails to align 15 repetitions of FDHC0 SX209.\n17Figure 13: The location-aware network aligns a concatenation of 3 different utterances, but fails to align 5.\n18B Detailed results of experiments\nTable 2: Phoneme error rates while decoding with various modiﬁcations. Compare with Figure 5\nfrom the main paper.\nPlain Keep 1 Keep 10 Keep 50 β = 2 Win. ±75 Win. ±150\nBaseline dev 15.9% 17.6% 15.9% 15.9% 16.1% 15.9% 15.9%\ntest 18.7% 20.2% 18.7% 18.7% 18.9% 18.7% 18.6%\nConv Feats dev 16.1% 19.4% 16.2% 16.1% 16.7% 16.0% 16.1%\ntest 18.0% 22.3% 17.9% 18.0% 18.7% 18.0% 18.0%\nSmooth Focus dev 15.8% 21.6% 16.5% 16.1% 16.2% 16.2% 16.0%\ntest 17.6% 24.7% 18.7% 17.8% 18.4% 17.7% 17.6%\n19"
    }
}