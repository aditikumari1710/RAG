{
    "title": "Towards K-means-friendly Spaces: Simultaneous Deep",
    "content": {
        "page_content": "Towards K-means-friendly Spaces: Simultaneous Deep\nLearning and Clustering\nBo Yang1 Xiao Fu1 Nicholas D. Sidiropoulos1 Mingyi Hong2\nAbstract\nMost learning approaches treat dimensionality\nreduction (DR) and clustering separately (i.e., se-\nquentially), but recent research has shown that\noptimizing the two tasks jointly can substantially\nimprove the performance of both. The premise\nbehind the latter genre is that the data samples are\nobtained via linear transformation of latent repre-\nsentations that are easy to cluster; but in practice,\nthe transformation from the latent space to the\ndata can be more complicated. In this work, we\nassume that this transformation is an unknown\nand possibly nonlinear function. To recover the\n‘clustering-friendly’ latent representations and to\nbetter cluster the data, we propose a joint DR and\nK-means clustering approach in which DR is ac-\ncomplished via learning a deep neural network\n(DNN). The motivation is to keep the advantages\nof jointly optimizing the two tasks, while exploit-\ning the deep neural network’s ability to approxi-\nmate any nonlinear function. This way, the pro-\nposed approach can work well for a broad class\nof generative models. Towards this end, we care-\nfully design the DNN structure and the associ-\nated joint optimization criterion, and propose an\neffective and scalable algorithm to handle the for-\nmulated optimization problem. Experiments us-\ning different real datasets are employed to show-\ncase the effectiveness of the proposed approach.\n1Department of Electrical and Computer Engineering, Univer-\nsity of Minnesota, Minneapolis MN 55455, USA. 2Department\nof Industrial and Manufacturing Systems Engineering, Iowa\nState University, Ames, IA 50011, USA. Correspondence to:\nBo Yang <yang4173@umn.edu>, Xiao Fu <xfu@umn.edu>,\nNicholas D. Sidiropoulos <nikos@ece.um.edu>, Mingyi Hong\n<mingyi@iastate.edu>.\nProceedings of the 34 th International Conference on Machine\nLearning, Sydney, Australia, PMLR 70, 2017. Copyright 2017\nby the author(s).\n1. Introduction\nClustering is one of the most fundamental tasks in data\nmining and machine learning, with an endless list of appli-\ncations. It is also a notoriously hard task, whose outcome\nis affected by a number of factors – including data acqui-\nsition and representation, use of preprocessing such as di-\nmensionality reduction (DR), the choice of clustering cri-\nterion and optimization algorithm, and initialization (Ertoz\net al., 2003; Banerjee et al., 2005). Since its introduction\nin 1957 by Lloyd (published much later in 1982 (Lloyd,\n1982)), K-means has been extensively used either alone or\ntogether with suitable preprocessing, due to its simplicity\nand effectiveness. K-means is suitable for clustering data\nsamples that are evenly spread around some centroids (cf.\nthe ﬁrst subﬁgure in Fig. 1), but many real-life datasets do\nnot exhibit this ‘K-means-friendly’ structure. Much effort\nhas been spent on mapping high-dimensional data to a cer-\ntain space that is suitable for performing K-means. Various\ntechniques, including principal component analysis (PCA),\ncanonical correlation analysis (CCA), nonnegative matrix\nfactorization (NMF) and sparse coding (dictionary learn-\ning), were adopted for this purpose. In addition to these\nlinear DR operators (e.g., a projection matrix), nonlinear\nDR techniques such as those used in spectral clustering (Ng\net al., 2002) and sparse subspace clustering (Elhamifar &\nVidal, 2013; You et al., 2016) have also been considered.\nIn recent years, motivated by the success of deep neu-\nral networks (DNNs) in supervised learning, unsupervised\ndeep learning approaches are now widely used for DR prior\nto clustering. For example, the stacked autoencoder (SAE)\n(Vincent et al., 2010), deep CCA (DCCA) (Andrew et al.,\n2013), and sparse autoencoder (Ng, 2011) take insights\nfrom PCA, CCA, and sparse coding, respectively, and make\nuse of DNNs to learn nonlinear mappings from the data do-\nmain to low-dimensional latent spaces. These approaches\ntreat their DNNs as a preprocessing stage that is separately\ndesigned from the subsequent clustering stage. The hope is\nthat the latent representations of the data learned by these\nDNNs will be naturally suitable for clustering. However,\nsince no clustering-promoting objective is explicitly incor-\nporated in the learning process, the learned DNNs do not\nnecessarily output reduced-dimension data that are suitable\narXiv:1610.04794v2  [cs.LG]  13 Jun 2017Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nfor clustering – as will be seen in our experiments.\nIn (De Soete & Carroll, 1994; Patel et al., 2013; Yang et al.,\n2017), joint DR and clustering was considered. The ratio-\nnale behind this line of work is that if there exists some\nlatent space where the entities nicely fall into clusters, then\nit is natural to seek a DR transformation that reveals such\nstructure, i.e., which yields a low K-means clustering cost.\nThis motivates using the K-means cost in latent space as\na prior that helps choose the right DR, and pushes DR\ntowards producing K-means-friendly representations. By\nperforming joint DR and K-means clustering, impressive\nclustering results have been observed in (Yang et al., 2017).\nThe limitation of these works is that the observable data is\nassumed to be generated from the latent clustering-friendly\nspace via simple linear transformation. While simple linear\ntransformation works well in many cases, there are other\ncases where the generative process is more complex, in-\nvolving a nonlinear mapping.\nContributions In this work, we propose a joint DR and K-\nmeans clustering framework, where the DR part is imple-\nmented through learning a DNN, rather than a linear model.\nUnlike previous attempts that utilize this joint DNN and\nclustering idea, we made customized design for this unsu-\npervised task. Although implementing this idea is highly\nnon-trivial (much more challenging than (De Soete & Car-\nroll, 1994; Patel et al., 2013; Yang et al., 2017) where the\nDR part only needs to learn a linear model), our objective is\nwell-motivated: by better modeling the data transformation\nprocess with a more general model, a much more K-means-\nfriendly latent space can be learned – as we will demon-\nstrate. A sneak peek of the kind of performance that can be\nexpected using our proposed method can be seen in Fig. 1,\nwhere we generate four clusters of 2-D data which are well\nseparated in the 2-D Euclidean space and then transform\nthem to a 100-D space using a complex non-linear map-\nping [cf. (9)] which destroys the cluster structure. One can\nsee that the proposed algorithm outputs reduced-dimension\ndata that are most suitable for applying K-means. Our spe-\nciﬁc contributions are as follows:\n•Optimization Criterion Design: We propose an opti-\nmization criterion for joint DNN-based DR and K-means\nclustering. The criterion is a combination of three parts,\nnamely, dimensionality reduction, data reconstruction, and\ncluster structure-promoting regularization. We deliberately\ninclude the reconstruction part and implement it using a\ndecoding network, which is crucial for avoiding trivial so-\nlutions. The criterion is also ﬂexible – it can be extended to\nincorporate different DNN structures (e.g. convolutional\nneural networks (LeCun et al., 1998; Krizhevsky et al.,\n2012)) and clustering criteria, e.g., subspace clustering.\n•Effective and Scalable Optimization Procedure: The\nformulated optimization problem is very challenging to\nFigure 1.The learned 2-D reduced-dimension data by different\nmethods. The observable data is in the 100-D space and is gener-\nated from 2-D data (cf. the ﬁrst subﬁgure) through the nonlinear\ntransformation in (9). The true cluster labels are indicated using\ndifferent colors.\nhandle, since it involves layers of nonlinear activation func-\ntions and integer constraints that are induced by the K-\nmeans part. We propose a judiciously designed solution\npackage, including empirically effective initialization and\na novel alternating stochastic gradient algorithm. The algo-\nrithmic structure is simple, enables online implementation,\nand is very scalable.\n•Comprehensive Experiments and Validation: We pro-\nvide a set of synthetic-data experiments and validate the\nmethod on different real datasets including various docu-\nment and image copora. Evidently visible improvement\nfrom the respective state-of-art is observed for all the\ndatasets that we experimented with.\n•Reproducibility: The code for the experiments is avail-\nable at https://github.com/boyangumn/DCN.\n2. Background and Related Works\nGiven a set of data samples {xi}i=1,...,N where xi ∈RM ,\nthe task of clustering is to group theN data samples into K\ncategories. Arguably, K-means (Lloyd, 1982) is the most\nwidely adopted algorithm. K-means approaches this task\nby optimizing the following cost function:\nmin\nM∈RM×K,{si∈RK}\nN∑\ni=1\n∥xi −Msi∥2\n2 (1)\ns.t. sj,i ∈{0,1}, 1T si = 1 ∀i,j,Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nwhere si is the assignment vector of data point iwhich has\nonly one non-zero element, sj,i denotes the jth element of\nsi, and thekth column ofM, i.e., mk, denotes the centroid\nof the kth cluster.\nK-means works well when the data samples are evenly scat-\ntered around their centroids in the feature space; we con-\nsider datasets which have this structure as being ‘K-means-\nfriendly’ (cf. top-left subﬁgure of Fig. 1). However, high-\ndimensional data are in general not very K-means-friendly.\nIn practice, using a DR pre-processing, e.g., PCA or NMF\n(Xu et al., 2003; Cai et al., 2011), to reduce the dimension\nof xi to a much lower dimensional space and then apply K-\nmeans usually gives better results. In addition to the above\nclassic DR methods that essentially learn a linear genera-\ntive model from the latent space to the data domain, nonlin-\near DR approaches such as those used in spectral clustering\n(Ng et al., 2002; V on Luxburg, 2007) and DNN-based DR\n(Hinton & Salakhutdinov, 2006; Schroff et al., 2015; Her-\nshey et al., 2016) are also widely used as pre-processing be-\nfore K-means or other clustering algorithms, see also (Vin-\ncent et al., 2010; Bruna & Mallat, 2013).\nInstead of using DR as a pre-processing, joint DR and clus-\ntering was also considered in the literature (De Soete &\nCarroll, 1994; Patel et al., 2013; Yang et al., 2017). This\nline of work can be summarized as follows. Consider\nthe generative model where a data sample is generated by\nxi = Whi, where W ∈RM×R and hi ∈RR, where\nR ≪M. Assume that the data clusters are well-separated\nin latent domain (i.e., where hi lives) but distorted by the\ntransformation introduced by W. Reference (Yang et al.,\n2017) formulated the joint optimization problem as fol-\nlows:\nmin\nM,{si},W,H\n∥X −WH ∥2\nF + λ\nN∑\ni=1\n∥hi −Msi∥2\n2\n+ r1(H) + r2(W) (2)\ns.t. sj,i ∈{0,1}, 1T si = 1 ∀i,j,\nwhere X = [ x1,..., xN ], H = [ h1,..., hN ], and λ ≥\n0 is a parameter for balancing data ﬁdelity and the latent\ncluster structure. In (2), the ﬁrst term performs DR and the\nsecond term performs latent clustering. The termsr1(·) and\nr2(·) are regularizations (e.g., nonnegativity or sparsity) to\nprevent trivial solutions, e.g.,H →0 ∈RR×N ; see details\nin (Yang et al., 2017).\nThe data model X ≈ WH in the above line of work\nmay be oversimpliﬁed: The data generating process can be\nmuch more complex than this linear transform. Therefore,\nit is well justiﬁed to seek powerful non-linear transforms,\ne.g. DNNs, to model this data generating process, while\nat the same time make use of the joint DR and clustering\nidea. Two recent works, (Xie et al., 2016) and (Yang et al.,\nInput\nLatent \nfeatures\nClustering \nmodule\nFigure 2.A problematic joint deep clustering structure. To avoid\nclutter, some links are omitted.\n2016), made such attempts.\nThe idea of (Xie et al., 2016) and (Yang et al., 2016) is to\nconnect a clustering module to the output layer of a DNN,\nand jointly learn DNN parameters and clusters. Speciﬁ-\ncally, the approaches look into an optimization problem of\nthe following form\nmin\nW,Θ\nˆL=\nN∑\ni=1\nq(f(xi; W); Θ), (3)\nwhere f(xi; W) is the network output given data sample\nxi, Wcollects the network parameters, and Θ denotes pa-\nrameters of some clustering model. For instance, Θ stands\nfor the centroids M and assignments {si}if the K-means\nclustering formulation (1) is adopted. The q(·) in (3) de-\nnotes some clustering loss, e.g., the Kullback-Leibler (KL)\ndivergence loss in (Xie et al., 2016) and agglomerative\nclustering loss in (Yang et al., 2016). An illustration of\nthis kind of approaches is shown in Fig. 2. This idea seems\nreasonable, but is problematic. A global optimal solution\nto Problem (3) is f(xi; W) = 0 and the optimal objec-\ntive value ˆL = 0 can always be achieved. Another type\nof trivial solutions are simply mapping arbitrary data sam-\nples to tight clusters, which will lead to a small value of\nˆL– but this could be far from being desired since there is\nno provision for respecting the data samples xi’s; see the\nbottom-middle subﬁgure in Fig. 1 [Deep Clustering Net-\nwork (DCN) w/o reconstruction] and the bottom-left sub-\nﬁgure in Fig. 1 [DEC]. This issue also exists in (Yang et al.,\n2016).\n3. Proposed Formulation\nWe are motivated to model the relationship between the ob-\nservable data xi and its clustering-friendly latent represen-\ntation hi using a nonlinear mapping, i.e.,\nhi = f(xi; W), f(·; W) : RM →RR,\nwhere f(·; W) denotes the mapping function and Wde-\nnote the set of parameters. In this work, we propose to\nemploy a DNN as our mapping function, since DNNs have\nthe ability of approximating any continuous mapping using\na reasonable number of parameters (Hornik et al., 1989).\nWe want to learn the DNN and perform clustering simul-\ntaneously. The critical question here is how to avoid triv-\nial solutions in this unsupervised task. In fact, this can beTowards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nresolved by taking insights from (2). The key to prevent\ntrivial solution in the linear DR case lies in the reconstruc-\ntion part, i.e., the term ∥X −WH ∥2\nF in (2). This term\nensures that the learned hi’s can (approximately) recon-\nstruct the xi’s using the basis W. This motivates incor-\nporating a reconstruction term in the joint DNN-based DR\nand K-means. In the realm of unsupervised DNN, there\nare several well-developed approaches for reconstruction –\ne.g., the stacked autoencoder (SAE) is a popular choice for\nserving this purpose. To prevent trivial low-dimensional\nrepresentations such as all-zero vectors, SAE uses a decod-\ning network g(·; Z) to map the hi’s back to the data do-\nmain and requires that g(hi; Z) and xi match each other\nwell under some metric, e.g., mutual information or least\nsquares-based measures.\nBy the above reasoning, we come up with the following\ncost function:\nmin\nW,Z,\nM,{si}\nN∑\ni=1\n(\nℓ(g(f(xi)),xi) + λ\n2 ∥f(xi) −Msi∥2\n2\n)\n(4)\ns.t. sj,i ∈{0,1}, 1T si = 1 ∀i,j,\nwhere we have simpliﬁed the notation f(xi; W) and\ng(hi; Z) to f(xi) and g(hi), respectively, for conciseness.\nThe function ℓ(·) : RM →R is a certain loss function that\nmeasures the reconstruction error. In this work, we adopt\nthe least-squares loss ℓ(x,y) = ∥x −y∥2\n2; other choices\nsuch as ℓ1-norm based ﬁtting and the KL divergence can\nalso be considered. λ ≥0 is a regularization parameter\nwhich balances the reconstruction error versus ﬁnding K-\nmeans-friendly latent representations.\nFig. 3 presents the network structure corresponding to the\nformulation in (4). Compare to the network in Fig. 2, our\nlatent features are also responsible for reconstructing the\ninput, preventing all the aforementioned trivial solutions.\nOn the left-hand side of the ‘bottleneck’ layer are the so-\ncalled encoding or forward layers that transform raw data\nto a low-dimensional space. On the right-hand side are the\n‘decoding’ layers that try to reconstruct the data from the\nlatent space. The K-means task is performed at the bottle-\nneck layer. The forward network, the decoding network,\nand the K-means cost are optimized simultaneously. In our\nexperiments, the structure of the decoding networks is a\n‘mirrored version’ of the encoding network, and for both\nthe encoding and decoding networks, we use the rectiﬁed\nlinear unit (ReLU) activation-based neurons (Nair & Hin-\nton, 2010). Since our objective is to perform DNN-driven\nK-means clustering, we will refer to the network in Fig. 3\nas the Deep Clustering Network (DCN) in the sequel.\nWe should remark that the proposed optimization criterion\nin (4) and the network in Fig. 3 are very ﬂexible: Other\ntypes of networks, e.g., deep convolutional neural networks\nInput Reconstruction\nLatent \nfeatures\nClustering \nmodule\nFigure 3.Proposed deep clustering network (DCN).\n(LeCun et al., 1998; Krizhevsky et al., 2012), can be used.\nFor the clustering part, other clustering criteria, e.g., K-\nsubspace and soft K-means (Law et al., 2005; Banerjee\net al., 2005), are also viable options. Nevertheless, we will\nconcentrate on the proposed DCN in the sequel, as our in-\nterest is to provide a proof-of-concept rather than exhaust-\ning the possibilities of combinations.\n4. Optimization Procedure\nOptimizing (4) is highly non-trivial since both the cost\nfunction and the constraints are non-convex. In addition,\nthere are scalability issues that need to be taken into ac-\ncount. In this section, we propose a pragmatic optimization\nprocedure including an empirically effective initialization\nmethod and an alternating optimization based algorithm for\nhandling (4).\n4.1. Initialization via Layer-wise Pre-Training\nFor dealing with hard non-convex optimization problems\nlike that in (4), initialization is usually crucial. To initial-\nize the parameters of the network, i.e., (W,Z), we use the\nlayer-wise pre-training method as in (Bengio et al., 2007)\nfor training autoencoders. This pre-training technique may\nbe avoided in large-scale supervised learning tasks. For\nthe proposed DCN which is completely unsupervised, how-\never, we ﬁnd that the layer-wise pre-training procedure is\nimportant no matter the size of the dataset. We refer the\nreaders to (Bengio et al., 2007) for an introduction of layer-\nwise pre-training. After pre-training, we perform K-means\nto the outputs of the bottleneck layer to obtain initial values\nof M and {si}.\n4.2. Alternating Stochastic Optimization\nEven with a good initialization, handling Problem (4) is\nstill very challenging. The commonly used stochastic gra-\ndient descent (SGD) algorithm cannot be directly applied\nto jointly optimize W,Z,M and {si}because the block\nvariable {si}is constrained on a discrete set. Our idea is to\ncombine the insights of alternating optimization and SGD.\nSpeciﬁcally, we propose to optimize the subproblems with\nrespect to (w.r.t.) one of M, {si}and (W,Z) while keep-\ning the other two sets of variables ﬁxed.Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\n4.2.1. U PDATE NETWORK PARAMETERS\nFor ﬁxed (M,{si}), the subproblem w.r.t.(W,Z) is simi-\nlar to training an SAE – but with an additional penalty term\non the clustering performance. We can take advantage of\nthe mature tools for training DNNs, e.g., back-propagation\nbased SGD and its variants. To implement SGD for updat-\ning the network parameters, we look at the problem w.r.t.\nthe incoming data xi:\nmin\nW,Z\nLi = ℓ(g(f(xi)),xi) + λ\n2 ∥f(xi) −Msi∥2\n2 . (5)\nThe gradient of the above function over the network param-\neters is easily computable, i.e., ▽XLi = ∂ℓ(g(f(xi)),xi)\n∂X +\nλ∂f(xi)\n∂X (f(xi) −Msi), where X = (W,Z) is a collec-\ntion of the network parameters and the gradients ∂ℓ\n∂X and\n∂f(xi)\n∂X can be calculated by back-propagation (Rumelhart\net al., 1988) (strictly speaking, what we calculate here is\nthe subgradient w.r.t. Xsince the ReLU function is non-\ndifferentible at zero). Then, the network parameters are\nupdated by\nX←X− α▽XLi, (6)\nwhere α> 0 is a diminishing learning rate.\n4.2.2. U PDATE CLUSTERING PARAMETERS\nFor ﬁxed network parameters and M, the assignment vec-\ntor of the current sample, i.e., si, can be naturally updated\nin an online fashion. Speciﬁcally, we update si as follows:\nsj,i ←\n\n\n\n1, if j = arg min\nk={1,...,K}\n∥f(xi) −mk∥2 ,\n0, otherwise.\n(7)\nWhen ﬁxing {si}and X, the update of M is simple and\nmay be done in a variety of ways. For example, one can\nsimply use mk = (1/|Ci\nk|) ∑\ni∈Ci\nk\nf(xi), where Ci\nk is the\nrecorded index set of samples assigned to clusterkfrom the\nﬁrst sample to the current sample i. Although the above\nupdate is intuitive, it could be problematic for online al-\ngorithms, since the already appeared historical data (i.e.,\nx1,..., xi) might not be representative enough to model\nthe global cluster structure and the initial si’s might be far\naway from being correct. Therefore, simply averaging the\ncurrent assigned samples may cause numerical problems.\nInstead of doing the above, we employ the idea in (Scul-\nley, 2010) to adaptively change the learning rate of updat-\ning m1,..., mK. The intuition is simple: assume that the\nclusters are roughly balanced in terms of the number of\ndata samples they contain. Then, after updating M for a\nnumber of samples, one should update the centroids of the\nclusters that already have many assigned members more\ngracefully while updating others more aggressively, to keep\nbalance. To implement this, let ci\nk be the count of the num-\nber of times the algorithm assigned a sample to cluster k\nbefore handling the incoming sample xi, and update mk\nby a simple gradient step:\nmk ←mk −(1/ci\nk) (mk −f(xi)) sk,i, (8)\nwhere the gradient step size1/ci\nk controls the learning rate.\nThe above update of M can also be viewed as an SGD\nstep, thereby resulting in an overall alternating block SGD\nprocedure that is summarized in Algorithm 1. Note that an\nepoch corresponds to a pass of all data samples through the\nnetwork.\nAlgorithm 1Alternating SGD\n1: Initialization {Perform T epochs over the data}\n2: for t= 1 : T do\n3: Update network parameters by (6)\n4: Update assignment by (7)\n5: Update centroids by (8)\n6: end for\nAlgorithm 1 has many favorable properties. First, it can\nbe implemented in a completely online fashion, and thus\nis very scalable. Second, many known tricks for enhanc-\ning performance of DNN training can be directly used. In\nfact, we have used a mini-batch version of SGD and batch-\nnormalization (Ioffe & Szegedy, 2015) in our experiments,\nwhich indeed help improve performance.\n5. Experiments\nIn this section, we use synthetic and real-world data to\nshowcase the effectiveness of DCN. We implement DCN\nusing the deep learning toolbox Theano (Theano Develop-\nment Team, 2016).\n5.1. Synthetic-Data Demonstration\nOur settings are as follows: Assume that the data points\nhave K-means-friendly structure in a two-dimensional do-\nmain (cf. the ﬁrst subﬁgure of Fig. 1). This two-\ndimensional domain is a latent domain which we do not\nobserve and we denote the latent representations of the\ndata points as hi’s in this domain. What we observe is\nxi ∈R100 that is obtained via the following transforma-\ntion:\nxi = σ(Uσ(Whi)) , (9)\nwhere W ∈R10×2 and U ∈R100×10 are matrices whose\nentries follow the zero-mean unit-variance i.i.d. Gaussian\ndistribution, σ(·) is a sigmod function to introduce nonlin-\nearity. Under the above generative model, recovering the\nK-means-friendly domain where hi’s live seems very chal-\nlenging.Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nWe generate four clusters, each of which has 2,500 samples\nand their geometric distribution on the 2-D plane is shown\nin the ﬁrst subﬁgure of Fig. 1 that we have seen before. The\nother subﬁgures show the recovered 2-D data fromxi’s us-\ning a number of DR methods, namely, NMF (Lee & Se-\nung, 1999), local linear embedding (LLE) (Saul & Roweis,\n2003), Laplacian eigenmap (LapEig) (Ng et al., 2002) – the\nﬁrst step of spectral clustering, and DEC (Xie et al., 2016).\nWe also present the result of using the formulation in (3)\n(DCN w/o reconstruction) which is a similar idea as in (Xie\net al., 2016). For the three DNN-based methods (DCN,\nDEC, and SAE + KM), we use a four-layer forward net-\nwork for dimensionality reduction, where the layers have\n100, 50, 10 and 2 neurons, respectively; the reconstruction\nnetwork used in DCN and SAE (and also in the per-training\nstage of DEC) is a mirrored version of the forward net-\nwork. As one can see in Fig. 1, all the DR methods except\nthe proposed DCN fail to map xi’s to a 2-D domain that\nis suitable for applying K-means. In particular, DEC and\nDCN w/o reconstruction indeed give trivial solutions: the\nreduced-dimension data are separated to four clusters, and\nthus ˆLis small. But this solution is meaningless since the\ndata partitioning is arbitrary.\nIn the supplementary materials, two additional simulations\nwith different generative model than (9) are presented, and\nsimilar results are observed. This further illustrates the\nDCN’s ability of recovering clustering-friendly structure\nunder different nonlinear generative models.\n5.2. Real-Data Validation\nIn this section, we validate the proposed approach on sev-\neral real-data sets which are all publicly available.\n5.2.1. B ASELINE METHODS\nWe compare the proposed DCN with a variety of baseline\nmethods:\n1) K-means (KM): The classic K-means (Lloyd, 1982).\n2) Spectral Clustering (SC): The classic SC algorithm\n(Ng et al., 2002).\n3) Sparse Subspace Clustering with Orthogonal Match-\ning Pursuit (SSC-OMP) (You et al., 2016): SSC is\nconsidered very competitive for clustering images; we use\nthe newly proposed greedy version here for scalability.\n4) Locally Consistent Concept Factorization (LCCF)\n(Cai et al., 2011): LCCF is based on NMF with a graph\nLaplacian regularization and is considered state-of-the-art\nfor document clustering.\n5) XRAY (Kumar et al., 2013): XRAY is an NMF-based\ndocument clustering algorithm that scales very well.\n6) NMF followed by K-means (NMF+KM): This ap-\nproach applies NMF for DR, and then applies K-means to\nthe reduced-dimension data.\n7) Stacked Autoencoder followed by K-means\n(SAE+KM): This is also a two-stage approach. We\nuse SAE for DR ﬁrst and then apply K-means.\n8) Joint NMF and K-means (JNKM)(Yang et al., 2017):\nJNKM performs joint DR and K-means clustering as the\nproposed DCN does – but the DR part is based on NMF.\n9) Deep Embedded Clustering (DEC)(Xie et al., 2016):\nDEC performs joint DNN and clustering, where the loss\nfunction contains only clustering loss, without penalty on\nreconstruction as in our method. We use the code 1 pro-\nvided by the authors. For each experiment, we select the\nbaselines that are considered most competitive and suitable\nfor that application from the above pool.\n5.2.2. E VALUATION METRICS\nWe adopt standard metrics for evaluating clustering perfor-\nmance. Speciﬁcally, we employ the following three met-\nrics: normalized mutual information (NMI) (Cai et al.,\n2011), adjusted Rand index (ARI) (Yeung & Ruzzo, 2001),\nand clustering accuracy (ACC) (Cai et al., 2011). In a nut-\nshell, all the above three measuring metrics are commonly\nused in the clustering literature, and all have pros and cons.\nBut using them together sufﬁces to demonstrate the effec-\ntiveness of the clustering algorithms. Note that NMI and\nACC lie in the range of zero to one with one being the per-\nfect clustering result and zero the worst. ARI is a value\nwithin −1 to 1, with one being the best clustering perfor-\nmance and minus one the opposite.\n5.2.3. RCV1\nWe ﬁrst test the algorithms on a large-scale text corpus,\nnamely, the Reuters Corpus V olume 1 Version 2 (RCV1-\nv2). The RCV1-v2 corpus (Lewis et al., 2004) contains\n804,414 documents, which were manually categorized into\n103 different topics. We use a subset of the documents\nfrom the whole corpus. This subset contains 20 topics and\n365,968 documents and each document has a single topic\nlabel. As in (Nitish et al., 2014), we pick the 2,000 most\nfrequently used words (in the tf-idf form) as the features of\nthe documents.\nWe conduct experiments using different number of clus-\nters. Towards this end, we ﬁrst sort the clusters according\nto the number of documents that they have in a descending\norder, and then apply the algorithms to the ﬁrst 4, 8, 12, 16,\n20 clusters, respectively. Note that the ﬁrst several clusters\nhave many more documents compared to the other clusters\n(cf. Fig. 4). This way, we gradually increase the number of\ndocuments in our experiments and create cases with much\nmore unbalanced cluster sizes for testing the algorithms –\nwhich means we gradually increase the difﬁculty of the ex-\nperiments. To avoid unrealistic tuning, for all the experi-\nments, we use a DCN whose forward network has ﬁve hid-\nden layers which have 2000,1000,1000,1000,50 neurons,\nrespectively. The reconstruction network has a mirrored\nstructure. We set λ= 0.1 for balancing the reconstruction\n1https://github.com/piiswrong/decTowards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\n0 5 10 15 20\nindex of clusters\n0\n2\n4\n6\n8size of clusters\n× 104\nFigure 4.The sizes of 20 clusters in the experiment.\nTable 1.Evaluation on the RCV1-v2 dataset\nMethods DCN SAE+KM KM DEC XRAY\n4 Clust.NMI 0.76 0.73 0.62 0.11 0.12ARI 0.67 0.65 0.50 0.07 -0.01ACC 0.80 0.79 0.70 0.38 0.34\n8 Clust.NMI 0.63 0.60 0.57 0.10 0.24ARI 0.46 0.42 0.38 0.05 0.09ACC 0.63 0.62 0.59 0.24 0.39\n12 Clust.NMI 0.67 0.65 0.6 0.09 0.22ARI 0.52 0.51 0.37 0.02 0.05ACC 0.60 0.56 0.54 0.18 0.29\n16 Clust.NMI 0.62 0.60 0.56 0.09 0.23ARI 0.36 0.35 0.30 0.02 0.04ACC 0.51 0.50 0.48 0.17 0.29\n20 Clust.NMI 0.61 0.59 0.58 0.08 0.25ARI 0.33 0.33 0.29 0.01 0.04ACC 0.47 0.46 0.47 0.14 0.28\nerror and the clustering regularization.\nTable 1 shows the results given by the proposed DCN,\nSAE+KM, KM, and XRAY; other baselines are not scal-\nable enough to handle the RCV1-v2 dataset and thus are\ndropped. One can see that for each case that we have tried,\nthe proposed method gives clear improvement relative to\nthe other methods. Particularly, the DCN approach outper-\nforms the two-stage approach, i.e., SAE+KM, in almost all\nthe cases and for all the evaluation metrics – this clearly\ndemonstrates the advantage of using the joint optimization\ncriterion. We notice that the performance of DEC in this ex-\nperiment is unsatisfactory, possibly because 1) this dataset\nis highly unbalanced (cf. Fig. 4), while DEC is designed\nto produce balanced clusters; 2) DEC gets trapped in trivial\nsolutions, as we discussed in Sec 2.\nFig. 5a shows how NMI, ARI, and ACC change when the\nproposed algorithm runs from epoch to epoch. One can see\na clear ascending trend of every evaluation metric. This\nresult shows that both the network structure and the opti-\nmization algorithm work towards a desired direction. In\nthe future, it would be intriguing to derive (sufﬁcient) con-\nditions for guaranteeing such improvement using the pro-\nposed algorithm. Nevertheless, such empirical observation\nin Fig. 5a is already very interesting and encouraging.\nWe visualize the 50-D learned embeddings of our net-\nwork on the RCV1 4-clusters dataset, using t-SNE (Van der\nMaaten & Hinton, 2008), as shown in Fig. 5b. We can see\nthat the proposed DCN method learns much improved re-\nsults compared to the initialization. Also, the DEC method\ndoes not get a desiable clustering result, possibly due to the\nTable 2.Evaluation on the 20Newsgroup dataset.\nMethodsDCN SAE+KM LCCF NMF+KM KM SC XARY JNKMNMI0.480.47 0.46 0.39 0.41 0.40 0.19 0.40ARI0.340.28 0.17 0.17 0.15 0.17 0.02 0.10ACC0.440.42 0.32 0.33 0.3 0.34 0.18 0.24\nimbalance clusters.\n0 10 20 30 40 50\nEpoches\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\nNMI\nARI\nACC\n(a) Clustering performance\nmetrics v.s. training epochs.\n(b) Visualization using t-SNE.\nFrom top-left to bottom-right:\nOriginal data, DEC result, DCN\ninitialization, DCN result\nFigure 5.Visualization on the 4-clusters subset of RCV1-v2\n5.2.4. 20N EWSGROUP\nThe 20Newsgroup corpus is a collection of 18,846 text\ndocuments which are partitioned into 20 different news-\ngroups. Using this corpus, we can observe how the pro-\nposed method works with a relatively small amount of\nsamples. As the previous experiment, we use the tf-idf\nrepresentation of the documents and pick the 2,000 most\nfrequently used words as the features. Since this dataset\nis small, we include more baselines that are not scalable\nenough for RCV1-v2. Among them, both JNKM and\nLCCF are considered state-of-art for document clustering.\nIn this experiment, we use a DNN with three forward lay-\ners which have 250, 100, and 20 neurons, respectively. This\nis a relatively ‘small network’ since the 20Newsgroup cor-\npus may not have sufﬁcient samples to ﬁt a large network.\nAs before, the decoding network for reconstruction has a\nmirrored structure of the encoding part, and the baseline\nSAE+KM uses the same network for the autoencoder part.\nTable 2 summarizes the results of this experiment. As one\ncan see, LCCF indeed gives the best performance among\nthe algorithms that do not use DNNs. SAE+KM improves\nARI and ACC quite substantially by involving DNN – this\nsuggests that the generative model may indeed be non-\nlinear. DCN performs even better by using the proposed\njoint DR and clustering criterion, which supports our mo-\ntivation that a K-means regularization can help discover a\nclustering-friendly space.\n5.2.5. R AW MNIST\nIn this and next subsections, we present two experiments\nusing two versions of the MNIST dataset. We ﬁrst employTowards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nTable 3.Evaluation on the raw MNIST dataset.\nMethodsDCN SAE+KM DEC KM SSC-OMPNMI0.810.73 0.80 0.50 0.31ARI0.750.670.750.37 0.13ACC0.83 0.800.840.53 0.30\nthe raw MNIST dataset that has 70,000 data samples. Each\nsample is a 28 ×28 gray-scale image containing a hand-\nwritten digit, i.e., one of {0,1,..., 9}. Same as (Xie et al.,\n2016), we use a 4-layers forward network and set the num-\nber of neurons to be 500, 500, 2000, and 10, respectively.\nThe reconstruction network is still a ‘mirrored’ version of\nthe forward network. The hyperparameter λis set to 1. We\nuse SSC-OMP, which is a scalable version of SSC, and KM\nas a baseline for this experiment.\nTable 3 shows results of applying DCN, SAE+KM, DEC,\nKM and SSC-OMP to the raw MNIST data – the other\nbaselines are not efﬁcient enough to handle 70,000 sam-\nples and thus are left out. One can see that our result is on\npar with the result of DEC reported in (Xie et al., 2016),\nand both methods outperform other methods by a large\nmargin. The DEC method performs very competitively on\nthis dataset, possibly because it is designed to favor bal-\nanced clusters, which is the case for MNIST dataset. On\nthe dataset RCV1-v2 with unbalanced clusters, the result\nof DEC is not as satisfactory, see Fig. 5b. It is also inter-\nesting to note that our method yields approximately same\nresults as DEC in this balanced case, but DCN also works\nwell in unbalanced cases, as we have seen.\n5.2.6. P RE-PROCESSED MNIST\nBesides the above experiment using the raw MNIST data,\nwe also provide another interesting experiment using pre-\nprocessed MNIST data. The pre-processing is done by a\nrecently introduced technique, namely, the scattering net-\nwork (ScatNet) (Bruna & Mallat, 2013). ScatNet is a\ncascade of multiple layers of wavelet transform, which is\nable to learn a good feature space for clustering / classi-\nﬁcation of images. Utilizing ScatNet, the work in (You\net al., 2016) reported very promising clustering results on\nMNIST using SSC-OMP. Our objective here is to see if\nthe proposed DCN can further improve the performance\nfrom SSC-OMP. Our idea is simple: SSC-OMP is essen-\ntially a procedure of constructing a similarity matrix of the\ndata; after obtaining this matrix, it performs K-means on\nthe rows of a matrix comprising several selected eigenvec-\ntors of the similarity matrix (Ng et al., 2002). Therefore, it\nmakes sense to treat the whole ScatNet + SSC-OMP pro-\ncedure as pre-processing for performing K-means, and one\ncan replace K-means by DCN to improve performance.\nThe results are shown in Table 4. One can see that the\nproposed method exhibits the best performance among the\nalgorithms. We note that the result of using KM on the data\nprocessed by ScatNet and SSC-OMP is worse than that was\nTable 4.Evaluation on pre-processed MNIST\nMethodsDCN SAE+KM KM (SSC-OMP)NMI0.880.86 0.85ARI0.890.86 0.82ACC0.950.93 0.86\nFigure 6.Clustering performance on MNIST with different λ.\nreported in (You et al., 2016). This is possibly because we\nuse all the 70,000 samples, while only a subset was selected\nfor conducting the experiments in (You et al., 2016).\nThis experiment is particularly interesting since it suggests\nthat for any clustering algorithm that employs K-means as\na key component, e.g., spectral clustering and sparse sub-\nspace clustering, one can use the proposed DCN to re-\nplace K-means and a better result can be expected. This\nis meaningful since many datasets are originally not suit-\nable for K-means due to the nature of the data – but af-\nter pre-processing (e.g., kernelization and eigendecompo-\nsition), the pre-processed data is already more K-means-\nfriendly, and using the proposed DCN at this point can fur-\nther strengthen the result.\n5.2.7. P ARAMETER SELECTION\nThe parameter λ is important, since it trades off between\nthe reconstruction objective and the clustering objective.\nAs we see from the experiments, the proposed DCN works\nwell with an appropriately chosen λ. Moreover, our ex-\nperience suggests that the performance of our approach is\ninsensitive to the exact value of λ. Fig. 6 shows how the\nproposed method performs with different λon the MNIST\ndataset. As we can see, although there is degradation of\nperformance as λ gets inappropriately large, the degrada-\ntion is mild. The proposed method gives satisfactory result\nfor a range of λ.\n6. Conclusion\nIn this work, we proposed a joint DR and K-means clus-\ntering approach where the DR part is accomplished via\nlearning a deep neural network. Our goal is to automat-\nically map high-dimensional data to a latent space where\nK-means is a suitable tool for clustering. We carefully de-\nsigned the network structure to avoid trivial and meaning-\nless solutions and proposed an effective and scalable op-\ntimization procedure to handle the formulated challenging\nproblem. Synthetic and real data experiments showed that\nthe algorithm is very effective on a variety of datasets.Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nAcknowledgements\nThis work is supported by National Science Foundation un-\nder Projects NSF IIS-1447788, NSF ECCS-1608961, and\nNSF CCF-1526078. The GPU used in this work was kindly\ndonated by NVIDIA.\nReferences\nAndrew, G., Arora, R., Bilmes, J., and Livescu, K. Deep\ncanonical correlation analysis. In Proceedings of the\n30th International Conference on International Confer-\nence on Machine Learning, pp. 1247–1255, 2013.\nBanerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J.\nClustering with bregman divergences. Journal of Ma-\nchine Learning Research, 6:1705–1749, Oct 2005.\nBengio, Y ., Lamblin, P., Popovici, D., and Larochelle,\nH. Greedy layer-wise training of deep networks. In\nAdvances in Neural Information Processing Systems 19\n(NIPS 2006), volume 19, pp. 153–160, 2007.\nBruna, J. and Mallat, S. Invariant scattering convolution\nnetworks. IEEE Transaction on Pattern Analysis Ma-\nchine Intelligence, 35(8):1872–1886, 2013.\nCai, D., He, X., and Han, J. Locally consistent concept\nfactorization for document clustering. IEEE Transac-\ntion on Knowledge and Data Engineering , 23(6):902–\n913, 2011.\nDe Soete, G. and Carroll, J. D. K-means clustering in a\nlow-dimensional euclidean space. In New Approaches in\nClassiﬁcation and Data Analysis, pp. 212–219. Springer,\n1994.\nElhamifar, E. and Vidal, R. Sparse subspace clustering:\nAlgorithm, theory, and applications. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 35(11):\n2765–2781, 2013.\nErtoz, L., Steinbach, M., and Kumar, V . Finding clusters\nof different sizes, shapes, and densities in noisy, high\ndimensional data. In Proceedings of Second SIAM Inter-\nnational Conference on Data Mining, pp. 47–58, 2003.\nHershey, J. R., Chen, Z., Le Roux, J., and Watanabe, S.\nDeep clustering: Discriminative embeddings for seg-\nmentation and separation. In Proceedings of 2016 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing, pp. 31–35. IEEE, 2016.\nHinton, G. E. and Salakhutdinov, R. Reducing the dimen-\nsionality of data with neural networks. Science, 313\n(5786):504–507, 2006.\nHornik, K., Stinchcombe, M., and White, H. Multilayer\nfeedforward networks are universal approximators.Neu-\nral Networks, 2(5):359–366, 1989.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerat-\ning deep network training by reducing internal covariate\nshift. In Proceedings of the 32nd International Confer-\nence on Machine Learning, pp. 448–456, 2015.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn Advances in Neural Infomation Processing Systems\n25 (NIPS 2012), pp. 1097–1105, 2012.\nKumar, A., Sindhwani, V ., and Kambadur, P. Fast conical\nhull algorithms for near-separable non-negative matrix\nfactorization. In Proceedings of 30th International Con-\nference on Machine Learning, pp. 231–239, 2013.\nLaw, M. H. C., Topchy, A., and Jain, A. K. Model-based\nclustering with probabilistic constraints. In Proceed-\nings of the 2005 SIAM International Conference on Data\nMining, pp. 641–645. SIAM, 2005.\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-\nbased learning applied to document recognition. Pro-\nceedings of the IEEE, 86(11):2278–2324, 1998.\nLee, D. D. and Seung, S. H. Learning the parts of objects by\nnon-negative matrix factorization. Nature, 401(6755):\n788–791, 1999.\nLewis, D. D., Yang, Y ., Rose, T. G., and Li, F. RCV1:\nA new benchmark collection for text categorization re-\nsearch. Journal of Machine Learning Research, 5:361–\n397, Apr 2004.\nLloyd, S. Least squares quantization in PCM. IEEE Trans-\naction on Information Theory, 28(2):129–137, 1982.\nNair, V . and Hinton, G. E. Rectiﬁed linear units improve\nrestricted boltzmann machines. In Proceedings of the\n27th International Conference on Machine Learning, pp.\n807–814, 2010.\nNg, A. Y . Sparse autoencoder. CS294A Lecture notes, 72:\n1–19, 2011.\nNg, A. Y ., Jordan, M., and Weiss, Y . On spectral cluster-\ning: Analysis and an algorithm. In Advances in Neu-\nral Information Processing Systems 15 (NIPS 2002), pp.\n849–856, 2002.\nNitish, S., Hinton, G. E., Krizhevsky, A., Sutskever, I., and\nSalakhutdinov, R. Dropout: a simple way to prevent\nneural networks from overﬁtting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nPatel, V . M., Van Nguyen, H., and Vidal, R. Latent space\nsparse subspace clustering. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, pp. 225–232, 2013.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. Learn-\ning representations by back-propagating errors. Neuro-\ncomputing: foundations of research, pp. 696–699, 1988.\nSaul, L. K. and Roweis, S. T. Think globally, ﬁt locally: un-\nsupervised learning of low dimensional manifolds.Jour-\nnal of Machine Learning Research, 4:119–155, 2003.\nSchroff, F., Kalenichenko, D., and Philbin, J. Facenet: A\nuniﬁed embedding for face recognition and clustering.\nIn Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pp. 815–823, 2015.\nSculley, D. Web-scale k-means clustering. In Proceedings\nof the 19th International Conference on World Wide Web\n(WWW), pp. 1177–1178. ACM, 2010.\nTheano Development Team. Theano: A Python framework\nfor fast computation of mathematical expressions. arXiv\ne-prints, abs/1605.02688, May 2016. URL http://\narxiv.org/abs/1605.02688.\nVan der Maaten, L. and Hinton, G. E. Visualizing data\nusing t-sne. Journal of Machine Learning Research , 9:\n2579–2605, Nov 2008.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y ., and Man-\nzagol, P. A. Stacked denoising autoencoders: Learning\nuseful representations in a deep network with a local\ndenoising criterion. Journal of Machine Learning Re-\nsearch, 11:3371–3408, Dec 2010.\nV on Luxburg, U. A tutorial on spectral clustering.Statistics\nand Computing, 17(4):395–416, 2007.\nXie, J., Girshick, R., and Farhadi, A. Unsupervised deep\nembedding for clustering analysis. In Proceedings of\nthe 33rd International Conference on Machine Learn-\ning, 2016.\nXu, W., Liu, X., and Gong, Y . Document clustering based\non non-negative matrix factorization. In Proceedings of\nthe 26th annual international ACM SIGIR conference on\nResearch and development in informaion retrieval , pp.\n267–273. ACM, 2003.\nYang, B., Fu, X., and Sidiropoulos, N. D. Learning from\nhidden traits: Joint factor analysis and latent clustering.\nIEEE Transaction on Signal Processing , pp. 256–269,\nJan. 2017.\nYang, J., Parikh, D., and Batra, D. Joint unsupervised learn-\ning of deep representations and image clusters. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 5147–5156, 2016.\nYeung, K. Y . and Ruzzo, W. L. Details of the adjusted rand\nindex and clustering algorithms, supplement to the paper\nan empirical study on principal component analysis for\nclustering gene expression data. Bioinformatics, 17(9):\n763–774, 2001.\nYou, C., Robinson, D., and Vidal, R. Scalable sparse sub-\nspace clustering by orthogonal matching pursuit. InPro-\nceedings of Conference on Computer Vision and Pattern\nRecognition (CVPR), volume 1, 2016.Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nSupplementary material for “Towards K-\nmeans-friendly Spaces: Simultaneous\nDeep Learning and Clustering”\n1. Additional Synthetic-Data Experiments\n1.1. Additional Generative Models\nIn this section, we provide two more examples to illustrate\nthe ability of DCN in recovering K-means-friendly spaces\nunder different generative models. We ﬁrst consider the\ntransformation as follows:\nxi = (σ(Whi))2 , (10)\nwhere σ(·) is the sigmoid function as before and W ∈\nR100×2 is similarly generated as in the paper. We perform\nelementwise squaring on the result features to further com-\nplicate the generating process. The corresponding results\ncan be seen in Fig. 1 of this supplementary document. One\ncan see that a similar pattern as we have observed in the\nmain text is also presented here: The proposed DCN recov-\ners a 2-D K-means-friendly space very well and the other\nmethods all fail.\nIn Fig. 2, we test the algorithms under the generative model\nxi = tanh (σ(Whi)) , (11)\nwhere W ∈R100×2. Same as before, the proposed DCN\ngives very clear clusters in the recovered 2-D space.\nThe results in this section and the synthetic-data experi-\nment presented in main text are encouraging: Under a vari-\nety of complicated nonlinear generative models, DCN can\noutput clustering-friendly latent representations.\n2. Additional Real-Data Experiments\n2.1. Pendigits\nBeside the real datasets in the paper, we also conduct ex-\nperiment on the Pendigits dataset. The Pendigits dataset\nconsists of 10,992 data samples. Each sample records 8\ncoordinates on a tablet, on which a subject is instructed to\nwrite the digits from 0 to 9. So each sample corresponds to\na vector of length 16, and represents one of the digits. Note\nthat this dataset is quite different from MNIST – each digit\nin MNIST is represented by an image (pixel values) while\ndigits in Pendigits are represented by 8 coordinates of the\nstylus when a person was writing a certain digit. Since each\ndigit is represented by a very small-size vector of length 16,\nwe use a small network who has three forward layers which\nare with 16, 16, and 10 neurons. Table 1 shows the results:\nThe proposed methods give the best clustering performance\ncompared to the competing methods, and the methods us-\ning DNNs outperform the ‘shallow’ ones that do not use\nFigure 1.The generated latent representations {hi} in the 2-D\nspace and the recovered 2-D representations from xi ∈ R100,\nwhere xi = (σ(W hi))2.\nTable 1.Evaluation on the Pendigits dataset\nMethods DCN SAE+KM SC KM\nNMI 0.69 0.65 0.67 0.67\nARI 0.56 0.53 0.55 0.55\nACC 0.72 0.70 0.71 0.69\nneural networks for DR.\n2.2. DCN as Feature Learner\nWe motivate and develop DCN as a clustering method that\ndirectly works on unlabeled data. In practice, DCN can also\nbe utilized as a feature-learning method when training sam-\nples are available – i.e., one can feed labeled training data\nto DCN, tune the parameters of the network to learn well\nclustered latent representations of the training samples, and\nthen use the trained DCN (to be speciﬁc, the forward net-\nwork) to reduce dimension of unseen testing data.\nHere, we provide some additional results to showcase the\nfeature-learning ability of DCN. We perform a 5-fold cross-\nvalidation experiment on the raw MNIST dataset, where\neach fold is a 80/20 training/testing random split. The per-\nformance of SAE+KM on the training sets is presented as\na baseline.\nThe obtained NMI, ARI, and ACC (mean and standard de-\nviation) are listed in Table. 2. One can see that the training\nand testing stages of DCN output similar results, which is\nrather encouraging. This experiment suggests that DCN is\nvery promising as a representation learner.Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nFigure 2.The generated latent representations {hi} in the 2-D\nspace of the recovered 2-D representations from xi ∈ R100,\nwhere xi = tanh (σ(W hi)).\nTable 2.The mean (stand deviation) of the evaluation results of\nthe 5-fold cross-validation on MNIST.\nNMI ARI ACC\nDCN-Training 0.80 (0.001) 0.74 (0.002) 0.83 (0.002)\nDCN-Testing 0.81 (0.003) 0.75 (0.005) 0.83 (0.004)\nSAE+KM 0.73 (0.001) 0.67 (0.002) 0.80 (0.002)\n3. Detailed Settings of Real-Data Experiments\n3.1. Algorithm Parameters\nThere is a set of parameters in the proposed algorithm\nwhich need to be pre-deﬁned. Speciﬁcally, the learning rate\nα, the number of epochs T (recall that one epoch responds\nto a pass of all the data samples through the network), and\nthe balancing regularization parameter λ. These parame-\nters vary from case to case since they are related to a num-\nber of factors, e.g., dimension of the data samples, total\nnumber of samples, scale (or energy) of the samples, etc.\nIn practice, a reasonable way to tune these parameters is\nthrough observing the performance of the algorithm under\nvarious parameters on a small validation subset whose la-\nbels are known.\nNote that the proposed algorithm has two stages, i.e., pre-\ntraining and the main algorithm and they usually use two\ndifferent sets of parameters since the algorithmic structure\nof the two stages are quite different (to be more precise, the\npre-training state does not work with the whole network but\nTable 3.List of parameters used in DCN.\nNotations Meaning\nλ regularization parameter\nαp base pre-training stepsize\nαl base learning stepsize\nTp pre-traing epochs\nTl learning epochs\nonly deals with a pair of encoding-decoding layers greed-\nily). Therefore, we distinguish the parameters of the two\nstages as listed in Table 3, to better describe the settings.\nWe implement SGD for solving the subproblem w.r.t. X\nusing the Nesterov-type acceleration ( ?), the mini-batch\nversion, and the momentum method. Batch normalization\n(Ioffe & Szegedy, 2015) that is recently proven to be very\neffective for training supervised deep networks is also em-\nployed. Through out the experiments, the momentum pa-\nrameter is set to be 0.9, the mini-batch size is selected to\nbe ≈0.01 ×N, and the other parameters are adjusted ac-\ncordingly in each experiments – which will be described in\ndetail in the next section.\n3.2. Network Parameters\nThe considered network has two parts, namely, the forward\nencoding network that reduces the dimensionality of the\ndata and the decoding network that reconstructs the data.\nWe let two networks to have a mirrored structure of each\nother. There are also two parameters of a forward network,\ni.e., the width of each layer (number of neurons) and the\ndepth of the network (number of layers). There is no strict\nrule for setting up these two parameters, but the rule of\nthumb is to adjust them according the amounts of data sam-\nples of the datasets and the dimension of each sample. Us-\ning a deeper and wider network may be able to better cap-\nture the underlying nonlinear transformation of the data, as\nthe network has more degrees of freedom. However, ﬁnd-\ning a large number of parameters accurately requires a large\namount of data since the procedure can be essentially con-\nsidered as solving a large system of nonlinear equations –\nand ﬁnding more unknowns needs more equalities in the\nsystem, or, data samples in this case. Therefore, there is a\nclear trade-off between network depth/width and the over-\nall performance.\n3.3. Detailed Parameter Settings\nThe detailed parameter settings for experiments on RCV1-\nv2 are shown in Tables 4. Parameter settings for 20News-\ngroup, raw MNIST, pre-processed MNIST, and Pendigits\nare shown in Tables 5, 6, 7, and 8, respectively.Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\n4. More Discussions\nWe have the following several more points as further dis-\ncussion:\n1. We have observed that runing SAE for epochs may\neven worsen the clustering performance in the two-\nstage approach. In Fig. 3, we show how the cluster-\ning performance indexes change with the epochs when\nwe run SAE without K-means regularization.One can\nsee that the performance in fact becomes worse com-\npared to merely using pre-training (i.e., initialization).\nThis means that using the SAE does not necessarily\nhelp clustering – and this supports our motivation for\nadding a K-means-friendly structure-enhancing regu-\nlarization.\n2. To alleviate the effect brought by the intrinsic random-\nness of the algorithms, e.g., random initialization of\npre-training, the reported results are all obtained via\nrunning the experiments several times and taking aver-\nage (speciﬁcally, we run the experiments with smaller\nsize, i.e., 20Newsgroup, raw and processed MNIST,\nand Pendigits for ten times and the results of the much\nlarger dataset RCV-v2 are average of ﬁve runs; the re-\nsults for DEC in Table 1 is from a single run.). There-\nfore, the presented results reﬂect the performance of\nthe algorithms in an average sense.\n3. We treat this work as a proof-of-concept: Joint DNN\nlearning and clustering is a highly viable task accord-\ning to our design and experiments. In the future, many\npractical issues will be investigated – e.g., designing\ntheory-backed ways of setting up network and algo-\nrithm parameters. Another very intriguing direction\nis of course to design convergence-guaranteed algo-\nrithms for optimizing the proposed criterion and its\nvariants. We leave these interesting considerations for\nfuture work.\nTable 4.Parameter settings for RCV1-v2\nparameters description\nf(xi; W): RM →RR M = 2,000 and R= 50\nSample size N 178,603 or 267,466\nforward net. depth 5 layers\nlayer width 2000/1000/1000/1000/50\nλ 0.1\nαp 0.01\nαl 0.05\nTp 50\nTl 50\nFigure 3.Clustering performance degrades when training with\nonly reconstruction error term. This is in sharp contrast with\nFigure 5(a) in the paper, where clustering performance improves\nwhen training the proposed DCN model.\nTable 5.Parameter settings for 20Newsgroup\nparameters description\nf(xi; W): RM →RR M = 2,000 and R= 20\nSample size N 18,846\nforward net. depth 3 layers\nlayer width 250/100/20\nλ 10\nαp 0.01\nαl 0.001\nTp 10\nTl 50\nTable 6.Parameter settings for raw MNIST\nparameters description\nf(xi; W): RM →RR M = 784 and R= 50\nSample size N 70,000\nforward net. depth 4 layers\nlayer width 500/ 500/ 2000/10\nλ 0.05\nαp 0.01\nαl 0.05\nTp 50\nTl 50Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\nTable 7.Parameter settings for Pre-Processed MNIST\nparameters description\nf(xi; W): RM →RR M = 10 and R= 5\nSample size N 70,000\nforward net. depth 3 layers\nlayer width 50/ 20/ 5\nλ 0.1\nαp 0.01\nαl 0.01\nTp 10\nTl 50\nTable 8.Parameter settings for Pendigits\nparameters description\nf(xi; W): RM →RR M = 16 and R= 10\nSample size N 10,992\nforward net. depth 3 layers\nlayer width 50/ 16/ 10\nλ 0.5\nαp 0.01\nαl 0.01\nTp 50\nTl 50"
    }
}