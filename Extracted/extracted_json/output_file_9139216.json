{
    "title": "Hierarchical Text-Conditional",
    "content": {
        "page_content": "Hierarchical Text-Conditional\nImage Generation with CLIP Latents\nAditya Ramesh∗\nOpenAI\naramesh@openai.com\nPrafulla Dhariwal∗\nOpenAI\nprafulla@openai.com\nAlex Nichol∗\nOpenAI\nalex@openai.com\nCasey Chu∗\nOpenAI\ncasey@openai.com\nMark Chen\nOpenAI\nmark@openai.com\nAbstract\nContrastive models like CLIP have been shown to learn robust representations of\nimages that capture both semantics and style. To leverage these representations for\nimage generation, we propose a two-stage model: a prior that generates a CLIP\nimage embedding given a text caption, and a decoder that generates an image\nconditioned on the image embedding. We show that explicitly generating image\nrepresentations improves image diversity with minimal loss in photorealism and\ncaption similarity. Our decoders conditioned on image representations can also\nproduce variations of an image that preserve both its semantics and style, while\nvarying the non-essential details absent from the image representation. Moreover,\nthe joint embedding space of CLIP enables language-guided image manipulations\nin a zero-shot fashion. We use diffusion models for the decoder and experiment\nwith both autoregressive and diffusion models for the prior, ﬁnding that the latter\nare computationally more efﬁcient and produce higher-quality samples.\n1 Introduction\nRecent progress in computer vision has been driven by scaling models on large datasets of captioned\nimages collected from the internet [10, 44, 60, 39, 31, 16]. Within this framework, CLIP [39] has\nemerged as a successful representation learner for images. CLIP embeddings have a number of\ndesirable properties: they are robust to image distribution shift, have impressive zero-shot capabilities,\nand have been ﬁne-tuned to achieve state-of-the-art results on a wide variety of vision and language\ntasks [45]. Concurrently, diffusion models [ 46, 48, 25] have emerged as a promising generative\nmodeling framework, pushing the state-of-the-art on image and video generation tasks [11, 26, 24].\nTo achieve best results, diffusion models leverage a guidance technique [ 11, 24] which improves\nsample ﬁdelity (for images, photorealism) at the cost of sample diversity.\nIn this work, we combine these two approaches for the problem of text-conditional image generation.\nWe ﬁrst train a diffusion decoder to invert the CLIP image encoder. Our inverter is non-deterministic,\nand can produce multiple images corresponding to a given image embedding. The presence of\nan encoder and its approximate inverse (the decoder) allows for capabilities beyond text-to-image\ntranslation. As in GAN inversion [62, 55], encoding and decoding an input image produces semanti-\ncally similar output images (Figure 3). We can also interpolate between input images by inverting\ninterpolations of their image embeddings (Figure 4). However, one notable advantage of using the\nCLIP latent space is the ability to semantically modify images by moving in the direction of any\nencoded text vector (Figure 5), whereas discovering these directions in GAN latent space involves\n∗Equal contribution\narXiv:2204.06125v1  [cs.CV]  13 Apr 2022vibrant portrait painting of Salvador Dalí with a robotic half face a shiba inu wearing a beret and black turtleneck a close up of a handpalm with leaves growing from it\nan espresso machine that makes coffee from human souls, artstation panda mad scientist mixing sparkling chemicals, artstation a corgi’s head depicted as an explosion of a nebula\na dolphin in an astronaut suit on saturn, artstation a propaganda poster depicting a cat dressed as french emperor\nnapoleon holding a piece of cheese a teddy bear on a skateboard in times square\nFigure 1: Selected 1024 ×1024 samples from a production version of our model.\n2Figure 2: A high-level overview of unCLIP. Above the dotted line, we depict the CLIP training process,\nthrough which we learn a joint representation space for text and images. Below the dotted line, we depict our\ntext-to-image generation process: a CLIP text embedding is ﬁrst fed to an autoregressive or diffusion prior\nto produce an image embedding, and then this embedding is used to condition a diffusion decoder which\nproduces a ﬁnal image. Note that the CLIP model is frozen during training of the prior and decoder.\nluck and diligent manual examination. Furthermore, encoding and decoding images also provides us with a\ntool for observing which features of the image are recognized or disregarded by CLIP.\nTo obtain a full generative model of images, we combine the CLIP image embeddingdecoder with a prior\nmodel, which generates possible CLIP image embeddings from a given text caption. We compare our\ntext-to-image system with other systems such as DALL-E [40] and GLIDE [35], ﬁnding that our samples are\ncomparable in quality to GLIDE, but with greater diversity in our generations. We also develop methods for\ntraining diffusion priors in latent space, and show that they achieve comparable performance to autoregressive\npriors, while being more compute-efﬁcient. We refer to our full text-conditional image generation stack as\nunCLIP, since it generates images by inverting the CLIP image encoder.\n2 Method\nOur training dataset consists of pairs (x,y) of images xand their corresponding captions y. Given an image x,\nlet zi and zt be its CLIP image and text embeddings, respectively. We design our generative stack to produce\nimages from captions using two components:\n• A prior P(zi|y) that produces CLIP image embeddings zi conditioned on captions y.\n• A decoder P(x|zi,y) that produces images x conditioned on CLIP image embeddings zi (and\noptionally text captions y).\nThe decoder allows us to invert images given their CLIP image embeddings, while the prior allows us to learn\na generative model of the image embeddings themselves. Stacking these two components yields a generative\nmodel P(x|y) of images xgiven captions y:\nP(x|y) =P(x,zi|y) =P(x|zi,y)P(zi|y).\nThe ﬁrst equality holds because zi is a deterministic function of x. The second equality holds because of the\nchain rule. Thus, we can sample from the true conditional distribution P(x|y) by ﬁrst sampling zi using the\n3prior, and then sampling xusing the decoder. In the following sections, we describe our decoder and prior\nstacks. For training details and hyperparameters, refer to Appendix C.\n2.1 Decoder\nWe use diffusion models [25, 48] to produce images conditioned on CLIP image embeddings (and optionally\ntext captions). Speciﬁcally, we modify the architecture described in Nichol et al. (2021) by projecting and\nadding CLIP embeddings to the existing timestep embedding, and by projecting CLIP embeddings into four\nextra tokens of context that are concatenated to the sequence of outputs from the GLIDE text encoder. We\nretained the text conditioning pathway present in the original GLIDE model, hypothesizing that it could allow\nthe diffusion model to learn aspects of natural language that CLIP fails to capture (e.g. variable binding), but\nﬁnd that it offers little help in this regard (Section 7).\nWhile we can sample from the conditional distribution of the decoder directly, past work using diffusion\nmodels shows using guidance on the conditioning information [11, 24, 35] improves sample quality a lot.\nWe enable classiﬁer-free guidance [ 24] by randomly setting the CLIP embeddings to zero (or a learned\nembedding) 10% of the time, and randomly dropping the text caption 50% of the time during training.\nTo generate high resolution images, we train two diffusion upsampler models [ 34, 43]: one to upsample\nimages from 64×64 to 256×256 resolution, and another to further upsample those to1024×1024 resolution.\nTo improve the robustness of our upsamplers, we slightly corrupt the conditioning images during training.\nFor the ﬁrst upsampling stage, we use gaussian blur [ 43], and for the second, we use a more diverse BSR\ndegradation [42, 59]. To reduce training compute and improve numerical stability, we follow Rombach et al.\n[42] and train on random crops of images that are one-fourth the target size. We use only spatial convolutions\nin the model (i.e., no attention layers) and at inference time directly apply the model at the target resolution,\nobserving that it readily generalizes to the higher resolution. We found no beneﬁt from conditioning the\nupsamplers on the caption, and use unconditional ADMNets [11] with no guidance.\n2.2 Prior\nWhile a decoder can invert CLIP image embeddings zi to produce images x, we need a prior model that\nproduces zi from captions yto enable image generations from text captions. We explore two different model\nclasses for the prior model:\n• Autoregressive (AR)prior: the CLIP image embedding zi is converted into a sequence of discrete\ncodes and predicted autoregressively conditioned on the caption y.\n• Diffusion prior: The continuous vector zi is directly modelled using a Gaussian diffusion model\nconditioned on the caption y.\nIn addition to the caption, we can condition the prior on the CLIP text embedding zt since it is a deterministic\nfunction of the caption. To improve sample quality we also enable sampling using classiﬁer-free guidance for\nboth the AR and diffusion prior, by randomly dropping this text conditioning information 10% of the time\nduring training.\nTo train and sample from the AR prior more efﬁciently, we ﬁrst reduce the dimensionality of the CLIP image\nembeddings zi by applying Principal Component Analysis (PCA) [37]. In particular, we ﬁnd that the rank\nof the CLIP representation space is drastically reduced when training CLIP with SAM [15] while slightly\nimproving evaluation metrics. We are able to preserve nearly all of the information2 by retaining only 319\nprincipal components out of the original 1,024. After applying PCA, we order the principal components\nby decreasing eigenvalue magnitude, quantize each of the 319 dimensions into 1,024 discrete buckets, and\n2I.e., less than 1% average mean-squared error in reconstructing the image representations.\n4Figure 3: Variations of an input image by encoding with CLIP and then decoding with a diffusion model. The\nvariations preserve both semantic information like presence of a clock in the painting and the overlapping\nstrokes in the logo, as well as stylistic elements like the surrealism in the painting and the color gradients in\nthe logo, while varying the non-essential details.\npredict the resulting sequence using a Transformer [53] model with a causal attention mask. This results in a\nthreefold reduction in the number of tokens predicted during inference, and improves training stability.\nWe condition the AR prior on the text caption and the CLIP text embedding by encoding them as a preﬁx\nto the sequence. Additionally, we prepend a token indicating the (quantized) dot product between the text\nembedding and image embedding, zi·zt. This allows us to condition the model on a higher dot product, since\nhigher text-image dot products correspond to captions which better describe the image. In practice, we ﬁnd it\nbeneﬁcial to sample the dot product from the top half of the distribution.3\nFor the diffusion prior, we train a decoder-only Transformer with a causal attention mask on a sequence\nconsisting of, in order: the encoded text, the CLIP text embedding, an embedding for the diffusion timestep,\nthe noised CLIP image embedding, and a ﬁnal embedding whose output from the Transformer is used to\npredict the unnoised CLIP image embedding. We choose not to condition the diffusion prior on zi ·zt like in\nthe AR prior; instead, we improve quality during sampling time by generating two samples ofzi and selecting\nthe one with a higher dot product with zt. Instead of using the ϵ-prediction formulation from Ho et al. [25],\nwe ﬁnd it better to train our model to predict the unnoised zi directly, and use a mean-squared error loss on\nthis prediction:\nLprior = Et∼[1,T],z(t)\ni ∼qt\n[\n∥fθ(z(t)\ni ,t,y ) −zi∥2]\n3We swept over percentiles 50%, 70%, 85%, 95% and found 50% to be optimal in all experiments.\n5Figure 4: Variations between two images by interpolating their CLIP image embedding and then decoding\nwith a diffusion model. We ﬁx the decoder seed across each row. The intermediate variations naturally blend\nthe content and style from both input images.\n3 Image Manipulations\nOur approach allows us to encode any given image xinto a bipartite latent representation (zi,xT) that is\nsufﬁcient for the decoder to produce an accurate reconstruction. The latent zi describes the aspects of the\nimage that are recognized by CLIP, while the latent xT encodes all of the residual information necessary for\nthe decoder to reconstruct x. The former is obtained by simply encoding the image with the CLIP image\nencoder. The latter is obtained by applying DDIM inversion (Appendix F in [ 11]) to xusing the decoder,\nwhile conditioning on zi. We describe three different kinds of manipulations that are enabled by this bipartite\nrepresentation.\n3.1 Variations\nGiven an image x, we can produce related images that share the same essential content but vary in other\napects, such as shape and orientation (Figure 3). To do this, we apply the decoder to the bipartite represen-\ntation (zi,xT) using DDIM with η >0 for sampling. With η= 0, the decoder becomes deterministic and\nwill reconstruct the given image x. Larger values of ηintroduce stochasticity into successive sampling steps,\nresulting in variations that are perceptually “centered” around the original image x. As ηincreases, these\nvariations tell us what information was captured in the CLIP image embedding (and thus is preserved across\nsamples), and what was lost (and thus changes across the samples).\n6a photo of a cat →an anime drawing of a super saiyan cat, artstation\na photo of a victorian house →a photo of a modern house\na photo of an adult lion →a photo of lion cub\na photo of a landscape in winter →a photo of a landscape in fall\nFigure 5: Text diffs applied to images by interpolating between their CLIP image embeddings and a normalised\ndifference of the CLIP text embeddings produced from the two descriptions. We also perform DDIM inversion\nto perfectly reconstruct the input image in the ﬁrst column, and ﬁx the decoder DDIM noise across each row.\n3.2 Interpolations\nIt is also possible to blend two images x1 and x2 for variations (Figure 4), traversing all of the concepts in\nCLIP’s embedding space that occur between them. To do this, we rotate between their CLIP embeddingszi1\nand zi2 using spherical interpolation, yielding intermediate CLIP representations ziθ = slerp(zi1 ,zi2 ,θ)\nas θis varied from 0 to 1. There are two options for producing the intermediate DDIM latents along the\ntrajectory. The ﬁrst option involves interpolating between their DDIM inverted latents xT1 and xT2 (by\nsetting xTθ = slerp(xT1 ,xT2 ,θ)), which yields a single trajectory whose endpoints reconstruct x1 and x2.\nThe second option involves ﬁxing the DDIM latent to a randomly-sampled value for all interpolates in the\ntrajectory. This results in an inﬁnite number of trajectories between x1 and x2, though the endpoints of these\ntrajectories will generally no longer coincide with the original images. We use this approach in Figure 4.\n3.3 Text Diffs\nA key advantage of using CLIP compared to other models for image representations is that it embeds images\nand text to the same latent space, thus allowing us to apply language-guided image manipulations (i.e., text\ndiffs), which we show in Figure 5. To modify the image to reﬂect a new text description y, we ﬁrst obtain\nits CLIP text embedding zt, as well as the CLIP text embedding zt0 of a caption describing the current\nimage4. We then compute a text diffvector zd = norm(zt −zt0 ) from these by taking their difference and\n4Instead of a description of the current image, we also experimented with using a dummy caption like “a photo” for\nthe baseline, or removing it altogether. These also worked well.\n7Granny Smith: 100%\niPod: 0%\nPizza: 0%\nGranny Smith: 0.02%\niPod: 99.98%\nPizza: 0%\nGranny Smith: 94.33%\niPod: 0%\nPizza: 5.66%\nFigure 6: Variations of images featuring typographic attacks [ 20] paired with the CLIP model’s predicted\nprobabilities across three labels. Surprisingly, the decoder still recovers Granny Smith apples even when the\npredicted probability for this label is near 0%. We also ﬁnd that our CLIP model is slightly less susceptible to\nthe “pizza” attack than the models investigated in [20].\nnormalizing. Now, we can rotate between the image CLIP embedding zi and the text diff vector zd using\nspherical interpolation, yielding intermediate CLIP representations zθ = slerp(zi,zd,θ), where θis increased\nlinearly from 0 to a maximum value that is typically in [0.25,0.50]. We produce the ﬁnal outputs by decoding\nthe interpolates zθ, ﬁxing the base DDIM noise to xT throughout the entire trajectory.\n4 Probing the CLIP Latent Space\nOur decoder model provides a unique opportunity to explore CLIP latent space by allowing us to directly\nvisualize what the CLIP image encoder is seeing. As an example use case, we can revisit cases where CLIP\nmakes incorrect predictions, such as typographic attacks [20]. In these adversarial images, a piece of text\nis overlayed on top of an object, which causes CLIP to predict the object described by the text rather than\nthe object depicted in the image. This piece of text essentially hides the original object in terms of output\nprobabilities. In Figure 6, we show an example of this attack from [20], wherein an apple can be misclassiﬁed\nas an iPod. Surprisingly, we ﬁnd that our decoder still generates pictures of apples with high probability\neven though the predicted probability of “Granny Smith” is near zero. Even more notable, the model never\nproduces pictures of iPods, despite the very high relative predicted probability of this caption.\n8Figure 7: Visualization of reconstructions of CLIP latents from progressively more PCA dimensions (20, 30,\n40, 80, 120, 160, 200, 320 dimensions), with the original source image on the far right. The lower dimensions\npreserve coarse-grained semantic information, whereas the higher dimensions encode ﬁner-grained details\nabout the exact form of the objects in the scene.\nPCA reconstructions offer another tool for probing the structure of the CLIP latent space. In Figure 7, we take\nthe CLIP image embeddings of a handful of source images and reconstruct them with progressively more\nPCA dimensions, and then visualize the reconstructed image embeddings using our decoder with DDIM on a\nﬁxed seed. This allows us to see what semantic information the different dimensions encode. We observe that\nthe early PCA dimensions preserve coarse-grained semantic information such as what types of objects are in\nthe scene, whereas the later PCA dimensions encode ﬁner-grained detail such as the shapes and exact form\nof the objects. For example, in the ﬁrst scene, the earlier dimensions seem to encode that there is food and\nperhaps a container present, whereas the later dimensions encode tomatoes and a bottle speciﬁcally. Figure 7\nalso serves as a visualization of what the AR prior is modeling, since the AR prior is trained to explicitly\npredict these principal components in this order.\n5 Text-to-Image Generation\n5.1 Importance of the Prior\nAlthough we train a prior to generate CLIP image embeddings from captions, the prior is not strictly necessary\nfor caption-to-image generation. For instance, our decoder can condition on both CLIP image embeddings\nand captions, but the CLIP image embedding is dropped 5% of the time during training in order to enable\nclassiﬁer-free guidance. Therefore, at sampling time, we can condition on only the caption, although this\nunderperforms a model trained fully in this way (this model is GLIDE, and we do a thorough comparison\nwith GLIDE in Sections 5.2 and 5.3). Another possibility is to feed the decoder the CLIP text embedding as if\nit were an image embedding, as previously observed [61, 54]. The ﬁrst two rows of Figure 8 depicts samples\nobtained in these two ways; the third row depicts samples obtained with a prior. Conditioning the decoder\non just the caption is clearly worst, but conditioning on text embeddings zero-shot does produce reasonable\nresults. Building on this observation, another approach would be to train the decoder to condition on CLIP\ntext embeddings [9] instead of CLIP image embeddings (although we would lose the capabilities mentioned\nin Section 4).\nTo quantify the effectiveness of these alternate approaches, we train two models: a small decoder conditioned\non CLIP text embeddings, and a small unCLIP stack (diffusion prior and decoder). We then compare samples\nfrom the text-embedding decoder, samples from the unCLIP stack, and samples obtained from feeding text\n9Caption\nText embedding\nImage embedding\n“A group of baseball\nplayers is crowded at\nthe mound.”\n“an oil painting of a\ncorgi wearing a\nparty hat”\n“a hedgehog using a\ncalculator”\n“A motorcycle parked in a\nparking space next to\nanother motorcycle.”\n“This wire metal rack\nholds several pairs of\nshoes and sandals”\nFigure 8: Samples using different conditioning signals for the same decoder. In the ﬁrst row, we pass the text\ncaption to the decoder, and pass a zero vector for the CLIP embedding. In the second row, we pass both the\ntext caption and the CLIP text embedding of the caption. In the third row, we pass the text and a CLIP image\nembedding generated by an autoregressive prior for the given caption. Note that this decoder is only trained\nto do the text-to-image generation task (without the CLIP image representation) 5% of the time.\nembeddings to the unCLIP decoder zero-shot, sweeping across guidance scales for all models. We ﬁnd\nthat these approaches respectively score FIDs of 9.16, 7.99, and 16.55 on a test set, suggesting the unCLIP\napproach is best. We also run human evaluations comparing the ﬁrst two settings, sweeping over sampling\nhyperparameters for each using our human evaluation proxy model (Appendix A). We ﬁnd that humans prefer\nthe full unCLIP stack 57.0% ±3.1% of the time for photorealism and 53.1% ±3.1% of the time for caption\nsimilarity.\nGiven the importance of the prior, it is worth evaluating different approaches for training it. We compare both\nthe AR and diffusion priors throughout our experiments. In all cases (Sections 5.2, 5.4, and 5.5), we ﬁnd that\nthe diffusion prior outperforms the AR prior for comparable model size and reduced training compute.\n5.2 Human Evaluations\nWe observe in Figure 1 that unCLIP is capable of synthesizing complex, realistic images. While we can\ncompare sample quality to past models using FID, it is not always aligned with human judgment. To better\ngauge the generation capabilities of our system, we conduct systematic human evaluations comparing unCLIP\nto GLIDE for photorealism, caption similarity, and sample diversity.\nWe follow the protocol of Ramesh et al., Nichol et al. [40, 35] for the ﬁrst two evaluations: for photorealism,\nusers are presented with pairs of images and must choose which looks more photorealistic; for caption\n101.0\n2.0\n3.0\n4.0\nunCLIP GLIDE\nFigure 9: Samples when increasing guidance scale for both unCLIP and GLIDE, using the prompt, “A green\nvase ﬁlled with red roses sitting on top of table.” For unCLIP, we ﬁx the latent vectors sampled from the prior,\nand only vary the guidance scale of the decoder. For both models, we ﬁx the diffusion noise seed for each\ncolumn. Samples from unCLIP improve in quality (more realistic lighting and shadows) but do not change in\ncontent as we increase guidance scale, preserving semantic diversity even at high decoder guidance scales.\nunCLIP Prior Photorealism Caption Similarity Diversity\nAR 47.1% ±3.1% 41.1% ±3.0% 62.6% ±3.0%\nDiffusion 48.9% ±3.1% 45.3% ±3.0% 70.5% ±2.8%\nTable 1: Human evaluations comparing unCLIP to GLIDE. We compare to both the AR and diffusion prior\nfor unCLIP. Reported ﬁgures are 95% conﬁdence intervals of the probability that the unCLIP model speciﬁed\nby the row beats GLIDE. Sampling hyperparameters for all models were swept to optimize an automated\nproxy for human photorealism evaluations.\nsimilarity, users are additionally prompted with a caption, and must choose which image better matches the\ncaption. In both evaluations, there is a third “Not sure” option. For diversity, we propose a new evaluation\nprotocol in which humans are presented with two 4 ×4 grids of samples and must choose which is more\ndiverse (with a third option, “Not sure”). For this evaluation, we produce sample grids using 1,000 captions\nfrom the MS-COCO validation set, and always compare sample grids for the same caption. Before running\nhuman comparisons, we swept over sampling hyperparameters for each model using a CLIP linear probe\ntrained to be a proxy for human photorealism evaluations (Appendix A). These hyperparameters are ﬁxed\nacross all three types of evaluation.\nWe present our results in Table 1. In general, the diffusion prior performs better than the AR prior in\npairwise comparisons against GLIDE. We ﬁnd that humans still slightly prefer GLIDE to unCLIP in terms of\nphotorealism, but the gap is very small. Even with similar photorealism, unCLIP is strongly preferred over\nGLIDE in terms of diversity, highlighting one of its beneﬁts.\n111.0 1.5 2.0 2.5 3.0\nGLIDE guidance scale\n20%\n30%\n40%\n50%\n60%\n70%\n80%\nF requency unCLIP was preferred over GLIDE\nunCLIP is better\nGLIDE is better\nin terms of photorealism\nin terms of caption similarity\nin terms of diversity\nFigure 10: When comparing unCLIP (with our best sampling settings) to various settings of guidance scale\nfor GLIDE, unCLIP was preferred by human evaluators on at least one axis among photorealism, caption\nsimilarity, and diversity for each comparison. At the higher guidance scales used to generate photorealistic\nimages, unCLIP yields greater diversity for comparable photorealism and caption similarity.\n1.01.52.02.53.03.54.0\nGuidance Scale\n10\n12\n14\n16\n18MS-COCO FID\nGLIDE\nunCLIP (AR)\nunCLIP (Diffusion)\nFigure 11: FID versus guidance scale for unCLIP and GLIDE. For the unCLIP priors, we swept over sampling\nhyperparameters and ﬁxed to the settings with the best minimum FID.\n5.3 Improved Diversity-Fidelity Trade-off with Guidance\nCompared to GLIDE, we qualitatively observe that unCLIP is able to generate more diverse images while\nleveraging the guidance technique to improve sample quality. To understand why, consider Figure 9 where\nwe increase guidance scale for both GLIDE and unCLIP. For GLIDE, the semantics (camera angle, color,\nsize) converge as we increase guidance scale, whereas for unCLIP the semantic information of the scene is\nfrozen in the CLIP image embedding and therefore does not collapse when guiding the decoder.\nIn Section 5.2, we observed that unCLIP achieves similar photorealism as GLIDE while maintaining more\ndiversity, but that its caption matching capabilities were slightly worse. It is natural to ask whether GLIDE’s\nguidance scale can be lowered to obtain the same diversity level as unCLIP while maintaining better caption\n12Model FID Zero-shot FID Zero-shot FID (ﬁlt)\nAttnGAN (Xu et al., 2017) 35.49\nDM-GAN (Zhu et al., 2019) 32.64\nDF-GAN (Tao et al., 2020) 21.42\nDM-GAN + CL (Ye et al., 2021) 20.79\nXMC-GAN (Zhang et al., 2021) 9.33\nLAFITE (Zhou et al., 2021) 8.12\nMake-A-Scene (Gafni et al., 2022) 7.55\nDALL-E (Ramesh et al., 2021) ∼28\nLAFITE (Zhou et al., 2021) 26.94\nGLIDE (Nichol et al., 2021) 12.24 12.89\nMake-A-Scene (Gafni et al., 2022) 11.84\nunCLIP (AR prior) 10.63 11.08\nunCLIP (Diffusion prior) 10.39 10.87\nTable 2: Comparison of FID on MS-COCO 256 ×256. We use guidance scale 1.25 for the decoder for both\nthe AR and diffusion prior, and achieve the best results using the diffusion prior.\nmatching. In Figure 10, we conduct a more careful study of this question by performing human evaluations\nacross several GLIDE guidance scales. We ﬁnd that GLIDE at guidance scale 2.0 is very close to the\nphotorealism and caption similarity of unCLIP, while still producing less diverse samples.\nFinally, in Figure 11 we compute MS-COCO zero-shot FID [23] while sweeping over guidance scale for both\nunCLIP and GLIDE, ﬁnding that guidance hurts the FID of unCLIP much less so than for GLIDE. In this\nevaluation, we ﬁx the guidance scale of the unCLIP prior and only vary the guidance scale of the decoder.\nThis is another indication that guidance hurts the diversity of GLIDE much more than unCLIP, since FID\nheavily penalizes non-diverse generations.\n5.4 Comparison on MS-COCO\nIn the text-conditional image generation literature, it has become standard practice to evaluate FID on the\nMS-COCO [28] validation set. We present results on this benchmark in Table 2. Like GLIDE and DALL-E,\nunCLIP is not directly trained on the MS-COCO training set, but can still generalize to the validation set\nzero-shot. We ﬁnd that, compared to these other zero-shot models, unCLIP achieves a new state-of-the-art\nFID of 10.39 when sampling with the diffusion prior. In Figure 12, we visually compare unCLIP to various\nrecent text-conditional image generation models on several captions from MS-COCO. We ﬁnd that, like the\nother methods, unCLIP produces realistic scenes that capture the text prompts.\n5.5 Aesthetic Quality Comparison\nWe additionally perform automated aesthetic quality evaluations comparing unCLIP to GLIDE. Our goal with\nthis evaluation is to assess how well each model produces artistic illustrations and photographs. To this end,\nwe generated 512 “artistic” captions using GPT-3 [4] by prompting it with captions for existing artwork (both\nreal and AI generated). Next, we trained a CLIP linear probe to predict human aesthetic judgments using\nthe A V A dataset [33] (Appendix A). For each model and set of sampling hyperparameters, we produce four\nimages for each prompt, and report the mean predicted aesthetic judgment over the full batch of 2048 images.\nIn Figure 13, we present results on our aesthetic quality evaluation. We ﬁnd that guidance improves aesthetic\nquality for both GLIDE and unCLIP. For unCLIP, we only guide the decoder (we found that guiding the prior\nhurt results). We also plot the aesthetic quality against Recall5, since guidance typically induces a trade-off\n5Recall is computed with respect to the training dataset.\n13Real Image\nDALL-E\nGLIDE\nMake-A-Scene\nunCLIP\nunCLIP (prod.)\n“a green train is coming\ndown the tracks”\n“a group of skiers are\npreparing to ski down\na mountain.”\n“a small kitchen with\na low ceiling”\n“a group of elephants\nwalking in muddy\nwater.”\n“a living area with a\ntelevision and a table”\nFigure 12: Random image samples on MS-COCO prompts.\n141.0 1.5 2.0 2.5 3.0 3.5 4.0\nguidance scale\n4.60\n4.65\n4.70\n4.75\n4.80\n4.85mean AVA prediction\nGLIDE\nunCLIP (AR)\nunCLIP (diffusion)\n4.60 4.65 4.70 4.75 4.80 4.85\nmean AVA prediction\n0.450\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600recall\nGLIDE\nunCLIP (AR)\nunCLIP (diffusion)\nFigure 13: Aesthetic quality evaluations comparing GLIDE and unCLIP using 512 auto-generated artistic\nprompts. We ﬁnd that both models beneﬁt from guidance, but unCLIP does not sacriﬁce recall for aesthetic\nquality.\nbetween ﬁdelity and diversity. Interestingly, we ﬁnd that guiding unCLIP does not decrease Recall while still\nimproving aesthetic quality according to this metric.\n6 Related Work\nSynthetic image generation is a well studied problem, and most popular techniques for unconditional image\ngeneration have also been applied to the text-conditional setting. Many previous works have trained GANs\n[21] on publicly available image captioning datasets to produce text-conditional image samples [ 56, 63,\n49, 58, 57]. Other works have adapted the VQ-V AE approach [52] to text-conditional image generation by\ntraining autoregressive transformers on sequences of text tokens followed by image tokens [40, 12, 1]. Finally,\nsome works have applied diffusion models to the problem, training either continuous [35] or discrete [22]\ndiffusion models with auxiliary text encoders to handle textual input.\nPrevious works have leveraged hierarchical generative processes to create high-quality synthetic images.\nRazavi et al. [ 41] trains a multi-layer discrete autoencoder, allowing them to ﬁrst sample coarse-grained\nlatent codes and then use this as conditioning information when sampling higher-resolution latent codes.\nChild, Vahdat and Kautz [5, 50] generate images using V AEs with a hierarchy of latent codes that increase\nprogressively with resolution. Concurrently with our work, Gafni et al. [17] conditions a generative image\nmodel on segmentation masks, allowing for a generative process that ﬁrst samples a semantic map of an\nimage and then conditions the generated image on this information.\nThe computational beneﬁts of using diffusion to model a latent space has been noted by previous works.\nPreechakul et al. [38] propose an autoencoder framework where diffusion models are used to render latent\nvariables as images, and a second diffusion model is used to generate these latents (similar to our diffusion\nprior). Vahdat et al. [51] use a score-based model for the latent space of a V AE, while Rombach et al. [42]\nuse diffusion models on the latents obtained from a VQGAN [14] like autoencoder.\nSince its release, CLIP [39] has been used extensively to steer generative image models towards text prompts.\nGalatolo et al., Patashnik et al., Murdock, Gal et al. [ 19, 36, 32, 18] guide GANs using gradients from a\nCLIP model. For diffusion models, Dhariwal and Nichol [11] introduced classiﬁer guidance as a way to use\ngradients from a classiﬁer trained on noised images to steer the model towards higher quality generations.\nNichol et al. [35] train a CLIP model on noised images and guide a text-conditional diffusion model, while\nCrowson, Crowson [7, 8] use an unnoised CLIP model to guide unconditional or class-conditional diffusion\nmodels. Ho and Salimans [24] introduced classiﬁer-free guidance and showed that one can perform guidance\n15(a) unCLIP\n (b) GLIDE\nFigure 14: Samples from unCLIP and GLIDE for the prompt “a red cube on top of a blue cube”.\nimplictly from the predictions of the model with and without the conditioning information, thus removing\nthe need for a classiﬁer. Nichol et al. [35] showed classiﬁer-free guidance works more favorably than CLIP\nguidance for text conditional image generation.\nSeveral previous works have trained generative image models that are directly conditioned on CLIP embed-\ndings. Zhou et al. [61] condition GAN models on randomly perturbed CLIP image embeddings, ﬁnding that\nthese models can generalize to CLIP text embeddings to produce text-conditional images. Crowson [9] trained\ndiffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional image generation.\nWang et al. [54] train an autoregressive generative model conditioned on CLIP image embeddings, ﬁnding\nthat it generalizes to CLIP text embeddings well enough to allow for text-conditional image synthesis.\nBordes et al. [3] train diffusion models conditioned on image representations from contrastive models. While\nthe diffusion models themselves cannot generate images unconditionally, the authors experimented with a\nsimple approach for two-stage image generation by employing Kernel Density Estimation to sample image\nrepresentations. By feeding these generated representations to the diffusion model, they can generate images\nend-to-end in a way similar to our proposed technique. However, our work differs from this in two ways: ﬁrst,\nwe use multimodal contrastive representations rather than image-only representations; second, we employ\nmuch more powerful generative models for the ﬁrst stage of the generation hierarchy, and these generative\nmodels are conditioned on text.\n7 Limitations and Risks\nAlthough conditioning image generation on CLIP embeddings improves diversity, this choice does come with\ncertain limitations. In particular, unCLIP is worse at binding attributes to objects than a corresponding GLIDE\nmodel. In Figure 14, we ﬁnd that unCLIP struggles more than GLIDE with a prompt where it must bind two\nseparate objects (cubes) to two separate attributes (colors). We hypothesize that this occurs because the CLIP\nembedding itself does not explicitly bind attributes to objects, and ﬁnd that reconstructions from the decoder\noften mix up attributes and objects, as shown in Figure 15. A similar and likely related issue is that unCLIP\n16Figure 15: Reconstructions from the decoder for difﬁcult binding problems. We ﬁnd that the reconstructions\nmix up objects and attributes. In the ﬁrst two examples, the model mixes up the color of two objects. In the\nrightmost example, the model does not reliably reconstruct the relative size of two objects.\nFigure 16: Samples from unCLIP for the prompt, “A sign that says deep learning.”\nstruggles at producing coherent text, as illustrated in Figure 16; it is possible that the CLIP embedding does\nnot precisely encode spelling information of rendered text. This issue is likely made worse because the BPE\nencoding we use obscures the spelling of the words in a caption from the model, so the model needs to have\nindependently seen each token written out in the training images in order to learn to render it.\nWe also note that our stack still has a hard time producing details in complex scenes (Figure 17). We\nhypothesize that this is a limitation of our decoder hierarchy producing an image at a base resolution of\n64 ×64 and then upsampling it. Training our unCLIP decoder at a higher base resolution should be able to\nalleviate this, at the cost of additional training and inference compute.\nAs discussed in the GLIDE paper, image generation models carry risks related to deceptive and otherwise\nharmful content. unCLIP’s performance improvements also raise the risk proﬁle over GLIDE. As the\ntechnology matures, it leaves fewer traces and indicators that outputs are AI-generated, making it easier to\nmistake generated images for authentic ones and vice versa. More research is also needed on how the change\nin architecture changes how the model learns biases in training data.\n17(a) A high quality photo of a dog playing in a green ﬁeld next to a lake.\n(b) A high quality photo of Times Square.\nFigure 17: unCLIP samples show low levels of detail for some complex scenes.\nThe risks of these models should be assessed in relation to the particular deployment context, which includes\ntraining data, guardrails in place, the deployment space, and who will have access. A preliminary analysis of\nthese issues in the context of the DALL·E 2 Preview platform (the ﬁrst deployment of an unCLIP model), can\nbe found in Mishkin et al. [30].\n8 Acknowledgements\nWe’d like to thank Jong Wook Kim, Hyeonwoo Noh, Alec Radford, Pranav Shyam, and Ilya Sutskever for\nhelpful discussions and contributions to our work. We’d also like to thank Yunxin Jiao for creating several\nﬁgures used in the paper. We are grateful to the Acceleration and Supercomputing teams at OpenAI for their\nwork on software and hardware infrastructure this project used.\n18References\n[1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A Causal Masked\nMultimodal Model of the Internet. arXiv:2201.07520, 2022.\n[2] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-DPM: an Analytic Estimate of the Optimal\nReverse Variance in Diffusion Probabilistic Models. CoRR, abs/2201.06503, 2022. URL https:\n//arxiv.org/abs/2201.06503.\n[3] Florian Bordes, Randall Balestriero, and Pascal Vincent. High Fidelity Visualization of What Your\nSelf-Supervised Representation Knows About. arXiv:2112.09164, 2021.\n[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language Models are Few-Shot Learners. arXiv:2005.14165, 2020.\n[5] Rewon Child. Very Deep V AEs Generalize Autoregressive Models and Can Outperform Them on\nImages. arXiv:2011.10650, 2021.\n[6] Katherine Crowson. A V A Linear Probe. https://twitter.com/RiversHaveWings/status/\n1472346186728173568?s=20&t=T-HRr3Gw5HRGjQaMDtRe3A, 2021.\n[7] Katherine Crowson. CLIP guided diffusion HQ 256x256. https://colab.research.google.com/\ndrive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj, 2021.\n[8] Katherine Crowson. CLIP Guided Diffusion 512x512, Secondary Model Method. https://twitter.\ncom/RiversHaveWings/status/1462859669454536711, 2021.\n[9] Katherine Crowson. v-diffusion. https://github.com/crowsonkb/v-diffusion-pytorch, 2021.\n[10] Karan Desai and Justin Johnson. VirTex: Learning Visual Representations from Textual Annotations.\narXiv:2006.06666, 2020.\n[11] Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis.\narXiv:2105.05233, 2021.\n[12] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, and Jie Tang. CogView: Mastering Text-to-Image Generation via Transformers.\narXiv:2105.13290, 2021.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\narXiv:2010.11929, 2020.\n[14] Patrick Esser, Robin Rombach, and Björn Ommer. Taming Transformers for High-Resolution Image\nSynthesis. arXiv:2012.09841, 2020.\n[15] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-Aware Minimization\nfor Efﬁciently Improving Generalization. arXiv:2010.01412, 2020.\n19[16] Andreas Fürst, Elisabeth Rumetshofer, Viet Thuong Tran, Hubert Ramsauer, Fei Tang, Johannes Lehner,\nD P Kreil, Michael K Kopp, Günter Klambauer, Angela Bitto-Nemling, and Sepp Hochreiter. CLOOB:\nModern Hopﬁeld Networks with InfoLOOB Outperform CLIP, 2022. URL https://openreview.\nnet/forum?id=qw674L9PfQE.\n[17] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-A-\nScene: Scene-Based Text-to-Image Generation with Human Priors. arXiv:2203.13131, 2022.\n[18] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. StyleGAN-NADA:\nCLIP-Guided Domain Adaptation of Image Generators. arXiv:2108.00946, 2021.\n[19] Federico A. Galatolo, Mario G. C. A. Cimino, and Gigliola Vaglini. Generating images from caption\nand vice versa via CLIP-Guided Generative Latent Space Search. arXiv:2102.01645, 2021.\n[20] Gabriel Goh, Nick Cammarata †, Chelsea V oss†, Shan Carter, Michael Petrov, Ludwig Schubert, Alec\nRadford, and Chris Olah. Multimodal Neurons in Artiﬁcial Neural Networks. Distill, 2021. doi:\n10.23915/distill.00030. https://distill.pub/2021/multimodal-neurons.\n[21] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative Adversarial Networks. arXiv:1406.2661, 2014.\n[22] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining\nGuo. Vector Quantized Diffusion Model for Text-to-Image Synthesis. arXiv:2111.14822, 2021.\n[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs\nTrained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. Advances in Neural\nInformation Processing Systems 30 (NIPS 2017), 2017.\n[24] Jonathan Ho and Tim Salimans. Classiﬁer-Free Diffusion Guidance. In NeurIPS 2021 Workshop on\nDeep Generative Models and Downstream Applications, 2021. URL https://openreview.net/\nforum?id=qw8AKxfYbI.\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models.arXiv:2006.11239,\n2020.\n[26] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans.\nCascaded Diffusion Models for High Fidelity Image Generation. arXiv:2106.15282, 2021.\n[27] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980,\n2014.\n[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro\nPerona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft COCO: Common Objects in\nContext. arXiv:1405.0312, 2014.\n[29] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv:1711.05101, 2017.\n[30] Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen Krueger, and Girish Sastry. DALL ·E 2\nPreview - Risks and Limitations. 2022. URL https://github.com/openai/dalle-2-preview/\nblob/main/system-card.md.\n[31] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. SLIP: Self-supervision meets\nLanguage-Image Pre-training. arXiv:2112.12750, 2021.\n[32] Ryan Murdock. The Big Sleep. https://twitter.com/advadnoun/status/\n1351038053033406468, 2021.\n20[33] Naila Murray, Luca Marchesotti, and Florent Perronnin. A V A: A large-scale database for aesthetic visual\nanalysis. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2408–2415,\n2012. doi: 10.1109/CVPR.2012.6247954.\n[34] Alex Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models.\narXiv:2102.09672, 2021.\n[35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with\nText-Guided Diffusion Models. arXiv:2112.10741, 2021.\n[36] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. StyleCLIP: Text-\nDriven Manipulation of StyleGAN Imagery. arXiv:2103.17249, 2021.\n[37] Karl Pearson. LIII. On lines and planes of closest ﬁt to systems of points in space, November 1901.\nURL https://doi.org/10.1080/14786440109462720.\n[38] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion\nAutoencoders: Toward a Meaningful and Decodable Representation. arXiv:2111.15640, 2021.\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\nTransferable Visual Models From Natural Language Supervision. arXiv:2103.00020, 2021.\n[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-Shot Text-to-Image Generation. arXiv:2102.12092, 2021.\n[41] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating Diverse High-Fidelity Images with\nVQ-V AE-2.arXiv:1906.00446, 2019.\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nResolution Image Synthesis with Latent Diffusion Models. arXiv:2112.10752, 2021.\n[43] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi.\nImage Super-Resolution via Iterative Reﬁnement. arXiv:arXiv:2104.07636, 2021.\n[44] Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning Visual Representations with Caption\nAnnotations. arXiv:2008.01392, 2020.\n[45] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao,\nand Kurt Keutzer. How Much Can CLIP Beneﬁt Vision-and-Language Tasks? arXiv:2107.06383, 2021.\n[46] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised\nLearning using Nonequilibrium Thermodynamics. arXiv:1503.03585, 2015.\n[47] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models.\narXiv:2010.02502, 2020.\n[48] Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models.\narXiv:2006.09011, 2020.\n[49] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. DF-GAN:\nDeep Fusion Generative Adversarial Networks for Text-to-Image Synthesis. arXiv:2008.05865, 2020.\n[50] Arash Vahdat and Jan Kautz. NV AE: A Deep Hierarchical Variational Autoencoder.arXiv:2007.03898,\n2020.\n21[51] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based Generative Modeling in Latent Space. In\nNeural Information Processing Systems (NeurIPS), 2021.\n[52] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural Discrete Representation Learning.\narXiv:1711.00937, 2017.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762, 2017.\n[54] Zihao Wang, Wei Liu, Qian He, Xinglong Wu, and Zili Yi. CLIP-GEN: Language-Free Training of a\nText-to-Image Generator with CLIP. arXiv:2203.00386, 2022.\n[55] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. GAN\nInversion: A Survey. arXiv:2101.05278, 2021.\n[56] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.\nAttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks.\narXiv:1711.10485, 2017.\n[57] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving Text-to-Image\nSynthesis Using Contrastive Learning. arXiv:2107.02423, 2021.\n[58] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-Modal Contrastive\nLearning for Text-to-Image Generation. arXiv:2101.04702, 2021.\n[59] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a Practical Degradation Model\nfor Deep Blind Image Super-Resolution. 2021 IEEE/CVF International Conference on Computer\nVision (ICCV), Oct 2021. doi: 10.1109/iccv48922.2021.00475. URL http://dx.doi.org/10.1109/\nICCV48922.2021.00475.\n[60] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. Contrastive\nLearning of Medical Visual Representations from Paired Images and Text. arXiv:2010.00747, 2020.\n[61] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu,\nJinhui Xu, and Tong Sun. LAFITE: Towards Language-Free Training for Text-to-Image Generation.\narXiv:2111.13792, 2021.\n[62] Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A. Efros. Generative Visual Manipulation\non the Natural Image Manifold. arXiv:1609.03552, 2016.\n[63] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-GAN: Dynamic Memory Generative Adversarial\nNetworks for Text-to-Image Synthesis. arXiv:1904.01310, 2019.\n22A Linear Probes for Evaluations\nFor our evaluations, we leverage two new linear probes on top of a CLIP ViT-L/14 [13] model. To automate\naesthetic quality evaluations, we follow the procedure used by Crowson [6], training a linear regression model\non images and mean ratings from the A V A dataset [33]. To reduce the cost of hyperparameter sweeps before\nconducting human evaluations, we train a logistic regression model to predict win probabilities between pairs\nof images. To train this model, we used 15,000 pairwise image comparisons gathered from all of our previous\nhuman evaluations. For each comparison i, we computed CLIP image embeddings xi and yi for the two\nimages in the pair. We then trained a linear model f(x) such that 1/(1 + exp (f(xi) −f(yi))) approximates\nthe probability that a human prefers the image for yi. This can be reduced to a logistic regression problem\nwith inputs equal to yi −xi.\nB Error Bars for Human Evaluation\nWhen computing error bars for human evaluations, we use the normal approximation interval with p= 0.95.\nWe expect the normal approximation to be accurate for such a large sample size of n= 1000.\nC Training Details\nThe unCLIP models used for the experiments in this paper were trained with the hyperparameters described\nbelow, unless otherwise noted. We additionally trained a production version of unCLIP using similarly\nsized models but with modiﬁed architectures and trained for longer; we include changes to accommodate\nproduct and safety requirements (e.g. inpainting, preventing unwanted memorization), and train on a larger\ndataset that is ﬁltered for aesthetic quality and safety. We report model and training hyperparameters for the\npaper models in Table 3. All models were trained using Adam [ 27] with corrected weight decay [29] and\nmomentum β1 = 0.9.\nOur CLIP model uses a ViT-H/16 [ 13] image encoder that consumes 256 ×256 resolution images, and\nhas width 1280 with 32 Transformer [53] blocks. The text encoder also follows the architecture described\nin Radford et al. [39]: it is a Transformer [53] with a causal attention mask, with width 1024 and 24 Trans-\nformer blocks. Both models are trained with learning rate 3 ×10−4 and SAM [15] with ρ= 0.1, where the\nperturbations are applied independently by the replicas, each of which uses batch size 64. The remaining\nhyperparameters are the same as those reported in Radford et al. [39].\nWhen training the encoder, we sample from the CLIP [ 39] and DALL-E [ 40] datasets (approximately\n650M images in total) with equal probability. When training the decoder, upsamplers, and prior, we use\nonly the DALL-E dataset [40] (approximately 250M images). Incorporating the noisier CLIP dataset while\ntraining the generative stack negatively impacted sample quality in our initial evaluations.\nOur decoder architecture is the 3.5 billion parameter GLIDE model, with the same architecture and diffusion\nhyperparameters as in Nichol et al. [35]. We train with learned sigma and sample with 250 strided sampling\nsteps as in Nichol and Dhariwal [34].\nWe use the ADMNet architecture [11] for the upsamplers. In the ﬁrst upsampling stage, we use a cosine\nnoising schedule, 320 channels and a depth of 3 resblocks per resolution inside the ADMNet. We also apply\ngaussian blur (kernel size 3, sigma 0.6) as described in Saharia et al. [43]. In the second upsampling stage,\nwe use a linear noising schedule, 192 channels, a depth of 2 resblocks per resolution, and train with the BSR\ndegradation from Rombach et al. [42]. Neither upsampler uses attention. To reduce inference time, we use\nDDIM [47] and manually tune the number of steps, with 27 steps for 256 ×256 model, and 15 steps for the\n1024 ×1024 model.\n23For the AR prior, we use a Transformer text encoder with width 2048 and 24 blocks and a decoder with\na causal attention mask, width 1664, and 24 blocks. For the diffusion prior, we use a Transformer with\nwidth 2048 and 24 blocks, and sample with Analytic DPM [ 2] with 64 strided sampling steps. To reuse\nhyperparameters tuned for diffusion noise schedules on images from Dhariwal and Nichol [11], we scale the\nCLIP embedding inputs by 17.2 to match the empirical variance of RGB pixel values of ImageNet images\nscaled to [−1,1].\nAR prior Diffusion prior 64 64 →256 256 →1024\nDiffusion steps - 1000 1000 1000 1000\nNoise schedule - cosine cosine cosine linear\nSampling steps - 64 250 27 15\nSampling variance method - analytic [2] learned [34] DDIM [47] DDIM [47]\nCrop fraction - - - 0.25 0.25\nModel size 1B 1B 3.5B 700M 300M\nChannels - - 512 320 192\nDepth - - 3 3 2\nChannels multiple - - 1,2,3,4 1,2,3,4 1,1,2,2,4,4\nHeads channels - - 64 - -\nAttention resolution - - 32,16,8 - -\nText encoder context 256 256 256 - -\nText encoder width 2048 2048 2048 - -\nText encoder depth 24 24 24 - -\nText encoder heads 32 32 32 - -\nLatent decoder context 384 - - - -\nLatent decoder width 1664 - - - -\nLatent decoder depth 24 - - - -\nLatent decoder heads 26 - - - -\nDropout - - 0.1 0.1 -\nWeight decay 4.0e-2 6.0e-2 - - -\nBatch size 4096 4096 2048 1024 512\nIterations 1M 600K 800K 1M 1M\nLearning rate 1.6e-4 1.1e-4 1.2e-4 1.2e-4 1.0e-4\nAdam β2 0.91 0.96 0.999 0.999 0.999\nAdam ϵ 1.0e-10 1.0e-6 1.0e-8 1.0e-8 1.0e-8\nEMA decay 0.999 0.9999 0.9999 0.9999 0.9999\nTable 3: Hyperparameters for the models\n24D Random samples\nIn Figures 18, 19 and 20 we show random samples from our production model for some of the prompts from\nFigure 1.\nFigure 18: Random samples from unCLIP for prompt “Vibrant portrait painting of Salvador Dali with a\nrobotic half face”\n25Figure 19: Random samples from unCLIP for prompt “A close up of a handpalm with leaves growing from\nit.”\n26Figure 20: Random samples from unCLIP for prompt “A teddybear on a skateboard in Times Square.”\n27"
    }
}