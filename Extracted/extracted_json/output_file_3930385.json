{
    "title": "GPT Understands, Too",
    "content": {
        "page_content": "GPT Understands, Too\nXiao Liu1∗, Yanan Zheng1∗, Zhengxiao Du1, Ming Ding1, Yujie Qian2,\nZhilin Yang1†, Jie Tang1†\n1Tsinghua University 2Massachusetts Institute of Technology\nAbstract\nPrompting a pretrained language model with\nnatural language patterns has been proved effec-\ntive for natural language understanding (NLU).\nHowever, our preliminary study reveals that\nmanual discrete prompts often lead to unsta-\nble performance—e.g., changing a single word\nin the prompt might result in substantial per-\nformance drop. We propose a novel method\nP-Tuning that employs trainable continuous\nprompt embeddings in concatenation with dis-\ncrete prompts. Empirically, P-Tuning not only\nstabilizes training by minimizing the gap be-\ntween various discrete prompts, but also im-\nproves performance by a sizeable margin on\na wide range of NLU tasks including LAMA\nand SuperGLUE. P-Tuning is generally effec-\ntive for both frozen and tuned language models,\nunder both the fully-supervised and few-shot\nsettings.\n1 Introduction\nPretrained language models (PLMs; Brown et al.,\n2020) have significantly advanced the performance\nof natural language understanding (NLU). PLMs\nare trained with different pretraining objectives,\nsuch as masked language modeling (Devlin et al.,\n2018), autoregressive language modeling (Radford\net al., 2019), seq2seq (Raffel et al., 2019), and per-\nmutation language modeling (Yang et al., 2019).\nPLMs can be further enhanced with prompting\n(Brown et al., 2020; Schick and Schütze, 2020),\nwhich employs manually written prompt patterns as\nadditional input to a language model. With prompt-\ning while PLMs are either finetuned on a small la-\nbeled dataset or frozen for direct inference on down-\nstream tasks. Prompting has significantly improved\nthe performance of many NLU tasks (Brown et al.,\n2020; Schick and Schütze, 2020).\n† corresponding to: Zhilin Yang (zhiliny@tsinghua.edu.cn)\nand Jie Tang (jietang@tsinghua.edu.cn)\n∗ indicates equal contribution.\nFigure 1: Average scores on 7 dev datasets of Super-\nGLUE using P-Tuning.\nPrompt P@1\nw/o PT\nP@1\nw/ PT\n[X] is located in [Y]. (original) 31.3 57.8\n[X] is located in which country or state? [Y]. 19.8 57.8\n[X] is located in which country? [Y]. 31.4 58.1\n[X] is located in which country? In [Y]. 51.1 58.1\nTable 1: Discrete prompts suffer from instability (high\nvariance), while P-Tuning stabilizes and improves per-\nformance. Results are precision@1 on LAMA-TREx\nP17 with BERT-base-cased. “PT” refers to P-Tuning,\nwhich trains additional continuous prompts in concate-\nnation with discrete prompts.\nHowever, we observe that manual discrete\nprompts suffer from a large degree of instability.\nAs shown in Table 1, with a frozen language model,\nchanging a single word in the prompt might result\nin substantial performance drop. As we will show\nin Section 3, when the language model is tuned,\nthe instability problem is alleviated but the perfor-\nmance difference between different prompts is still\nsizeable, especially in the few-shot setting. Such\nan instability issue of discrete prompts poses a crit-\nical challenge in practice. Recent approaches of\nautomatic prompting have attempted to search for a\nbetter-performing prompt given a task (Shin et al.,\n2020; Gao et al., 2020; Jiang et al., 2020b), but\nthese methods do not change the unstable nature of\ndiscrete prompts.\nTo reduce the instability of discrete prompts,\nwe propose a novel method P-Tuning that em-\nploys trainable continuous prompt embeddings in\nconcatenation with discrete prompts. Specifically,\narXiv:2103.10385v2  [cs.CL]  25 Oct 2023given a discrete prompt as the input, P-Tuning con-\ncatenates continuous prompt embeddings with the\ndiscrete prompt tokens and feeds them as the input\nto the language model. The continuous prompts are\nupdated by backpropagation to optimize the task\nobjective. The intuition is that continuous prompts\nincorporate a certain degree of learnability into the\ninput, which may learn to offset the effects of mi-\nnor changes in discrete prompts to improve training\nstability. To further improve performance, we em-\nploy a prompt encoder using LSTMs or MLPs to\nmodel the dependency between continuous prompt\nembeddings.\nWe experiment with two NLU benchmarks: the\nLAMA (Petroni et al., 2019) knowledge probing\nand SuperGLUE (Wang et al., 2019a). On LAMA,\nwith the language model frozen, P-Tuning out-\nperforms manual discrete prompts and searched\nprompts by 20+ points and 9 points respectively\nwith the same pretrained models. On SuperGLUE,\nwith the language model finetuned, P-Tuning out-\nperforms PET (Schick and Schütze, 2020) with\nthe best discrete prompts under both the fully-\nsupervised and few-shot settings. In addition to im-\nproving performance, our results show that across\na wide range of tasks and settings, P-Tuning sub-\nstantially reduces the performance gap between dif-\nferent discrete prompts, which results in improved\nstability for language model adaptation.\n2 Method\n2.1 Issues with Discrete Prompts\nPrompting employs natural language patterns as\nadditional inputs to pretrained language models for\nadaptation to downstream tasks (Brown et al., 2020;\nSchick and Schütze, 2020). Prior work (Zheng\net al., 2021) has pointed out that prompting has\nachieved consistent and substantial improvements\non a number of NLP tasks. However, it still re-\nmains a challenging problem of how to write high-\nperforming discrete prompts.\nWe performed preliminary experiments using\ndifferent manual prompts on the LAMA knowledge\nprobing task (Petroni et al., 2019), which aims to\nextract triplet knowledge from a language model\nby predicting the tail entities. Results in Table 1\nshow that manual discrete prompts lead to unstable\nperformance. For example, if we compare the last\ntwo prompts in the table, changing a single word\nin prompt causes a drastic decrease of 20 points in\nperformance.\nIn light of the challenge, recent works propose to\nautomate the search procedure of discrete prompts\nby mining the training corpus (Jiang et al., 2020b),\ngradient-based searching (Shin et al., 2020), and us-\ning pretrained generative models (Gao et al., 2020).\nHowever, these works aim at searching for better-\nperforming prompts but do not change the nature\nof instability for discrete prompts. In addition to\nthe instability issue, searching in the discrete space\nmight not be able to fully leverage the gradients\nfrom backpropagation, which will potentially result\nin suboptimal solutions. To this end, we explore\nthe possibility of training continuous prompts to\nstabilize and improve the performance of language\nmodel adaptation.\n2.2 P-Tuning\nFormally, let M be a pretrained language model\nwith a hidden size of h and a vocabulary size of\n|V|. Let {(xi, yi))}i be a labeled dataset for an\nNLU task, where x0:n = {x0, x1, ..., xn} is an\ninput consisting of a sequence of discrete tokens,\nand y ∈ Yis a label. Our goal is to estimate the\nconditional probability for classification fM(x) =\nˆp(y|x) with parameters of M either finetuned or\nfrozen.\nPrompting was proposed in the format of\ndiscrete tokens (Schick and Schütze, 2020).\nLet [Di] be a discrete prompt token. Each\nprompt can be described as a template T =\n{[D0:i], x, [D(i+1):j], y, [D(j+1):k]}, which could\norganize the labeled data (including the inputs x\nand the label y) into a sequence of text tokens, such\nthat the task could be reformulated as filling in the\nblanks of the input text. For example, for the task of\npredicting a country’s capital (LAMA-TREx P36),\na prompt could be “The capital of [INPUT] is [LA-\nBEL].” With a piece of labeled data “(Britain, Lon-\ndon)”, the reformulated text would be “The capital\nof Britain is [MASK].”, where “[MASK]\" should\npredict the given label “London”. Both discrete\nprompts and discrete data are together mapped into\ninput embeddings:\n{e(D0)...e(Di), e(x0), ...,e(xn), ...,e(Dk)}\nthrough the pretrained embedding layer, where e ∈\nR|V|×d.\nHowever, as is discussed in Section 2.1, such\ndiscrete prompts tend to be extremely unstable\nand might not be optimal with back-propagation.\nTherefore, we propose P-Tuning that uses contin-\nuous prompt embeddings to improve and stabilizePre-trained Language Model\n(GPT, BERT, … \u0001\nPrompt Encoder\n[P 0 ]\n<latexit sha1_base64=\"0DXRfnpo546JxXEWz0BMIZpwZ/g=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtOu1ekWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+EDUj6U+w==</latexit><latexit sha1_base64=\"0DXRfnpo546JxXEWz0BMIZpwZ/g=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtOu1ekWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+EDUj6U+w==</latexit><latexit sha1_base64=\"0DXRfnpo546JxXEWz0BMIZpwZ/g=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtOu1ekWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+EDUj6U+w==</latexit><latexit sha1_base64=\"0DXRfnpo546JxXEWz0BMIZpwZ/g=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtOu1ekWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+EDUj6U+w==</latexit>\n[P i ]\n<latexit sha1_base64=\"04Hd84/XZ1Ucnoy1lracOFTzCn0=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtMu73SLJats6WXOAzsDJWSrGhVf0EYPETykCMAQQhL24SChpwUbFmLiOpgQJwhxHWeYokDalLIYZTjEjug7oF0rY0PaK89Eqz06xadXkNLEAWkiyhOE1WmmjqfaWbG/eU+0p7rbmP5u5hUQKzEk9i/dLPO/OlWLRB+nugZONcWaUdV5mUuqu6Jubn6pSpJDTJzCPYoLwp5Wzvpsak2ia1e9dXT8TWcqVu29LDfFu7olDdj+Oc55UD8q21bZvjwuVc6yUeexh30c0jxPUMEFqqiRt8AjnvBsXBlj4864/0w1cplmF9+W8fAB2deVNA==</latexit><latexit sha1_base64=\"04Hd84/XZ1Ucnoy1lracOFTzCn0=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtMu73SLJats6WXOAzsDJWSrGhVf0EYPETykCMAQQhL24SChpwUbFmLiOpgQJwhxHWeYokDalLIYZTjEjug7oF0rY0PaK89Eqz06xadXkNLEAWkiyhOE1WmmjqfaWbG/eU+0p7rbmP5u5hUQKzEk9i/dLPO/OlWLRB+nugZONcWaUdV5mUuqu6Jubn6pSpJDTJzCPYoLwp5Wzvpsak2ia1e9dXT8TWcqVu29LDfFu7olDdj+Oc55UD8q21bZvjwuVc6yUeexh30c0jxPUMEFqqiRt8AjnvBsXBlj4864/0w1cplmF9+W8fAB2deVNA==</latexit><latexit sha1_base64=\"04Hd84/XZ1Ucnoy1lracOFTzCn0=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtMu73SLJats6WXOAzsDJWSrGhVf0EYPETykCMAQQhL24SChpwUbFmLiOpgQJwhxHWeYokDalLIYZTjEjug7oF0rY0PaK89Eqz06xadXkNLEAWkiyhOE1WmmjqfaWbG/eU+0p7rbmP5u5hUQKzEk9i/dLPO/OlWLRB+nugZONcWaUdV5mUuqu6Jubn6pSpJDTJzCPYoLwp5Wzvpsak2ia1e9dXT8TWcqVu29LDfFu7olDdj+Oc55UD8q21bZvjwuVc6yUeexh30c0jxPUMEFqqiRt8AjnvBsXBlj4864/0w1cplmF9+W8fAB2deVNA==</latexit><latexit sha1_base64=\"04Hd84/XZ1Ucnoy1lracOFTzCn0=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtMu73SLJats6WXOAzsDJWSrGhVf0EYPETykCMAQQhL24SChpwUbFmLiOpgQJwhxHWeYokDalLIYZTjEjug7oF0rY0PaK89Eqz06xadXkNLEAWkiyhOE1WmmjqfaWbG/eU+0p7rbmP5u5hUQKzEk9i/dLPO/OlWLRB+nugZONcWaUdV5mUuqu6Jubn6pSpJDTJzCPYoLwp5Wzvpsak2ia1e9dXT8TWcqVu29LDfFu7olDdj+Oc55UD8q21bZvjwuVc6yUeexh30c0jxPUMEFqqiRt8AjnvBsXBlj4864/0w1cplmF9+W8fAB2deVNA==</latexit>\n[P i +1 ]\n<latexit sha1_base64=\"T4HkTbSj3PFQ1GfAlSrlXjyhL+Y=\">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIsgCCURQZdFNy4r2IfUUpI41cG8mEzEUoo7cesPuNU/Ev9A/8I7YwpqEZ2Q5My595yZe6+XBDyVtv1aMKamZ2bnivOlhcWl5RVztdxM40z4rOHHQSzanpuygEesIbkMWDsRzA29gLW8q0MVb10zkfI4OpGDhHVD9yLife67kqieWe6cha68FOGwPuoN+bYz6vbMil219bImgZODCvJVj80XnOEcMXxkCMEQQRIO4CKlpwMHNhLiuhgSJwhxHWcYoUTajLIYZbjEXtH3gnadnI1orzxTrfbplIBeQUoLm6SJKU8QVqdZOp5pZ8X+5j3UnupuA/p7uVdIrMQlsX/pxpn/1alaJPrY1zVwqinRjKrOz10y3RV1c+tLVZIcEuIUPqe4IOxr5bjPltakunbVW1fH33SmYtXez3MzvKtb0oCdn+OcBM2dqmNXnePdSu0gH3UR69jAFs1zDzUcoY4Ged/gEU94Nk6NW+POuP9MNQq5Zg3flvHwAURqluE=</latexit><latexit sha1_base64=\"T4HkTbSj3PFQ1GfAlSrlXjyhL+Y=\">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIsgCCURQZdFNy4r2IfUUpI41cG8mEzEUoo7cesPuNU/Ev9A/8I7YwpqEZ2Q5My595yZe6+XBDyVtv1aMKamZ2bnivOlhcWl5RVztdxM40z4rOHHQSzanpuygEesIbkMWDsRzA29gLW8q0MVb10zkfI4OpGDhHVD9yLife67kqieWe6cha68FOGwPuoN+bYz6vbMil219bImgZODCvJVj80XnOEcMXxkCMEQQRIO4CKlpwMHNhLiuhgSJwhxHWcYoUTajLIYZbjEXtH3gnadnI1orzxTrfbplIBeQUoLm6SJKU8QVqdZOp5pZ8X+5j3UnupuA/p7uVdIrMQlsX/pxpn/1alaJPrY1zVwqinRjKrOz10y3RV1c+tLVZIcEuIUPqe4IOxr5bjPltakunbVW1fH33SmYtXez3MzvKtb0oCdn+OcBM2dqmNXnePdSu0gH3UR69jAFs1zDzUcoY4Ged/gEU94Nk6NW+POuP9MNQq5Zg3flvHwAURqluE=</latexit><latexit sha1_base64=\"T4HkTbSj3PFQ1GfAlSrlXjyhL+Y=\">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIsgCCURQZdFNy4r2IfUUpI41cG8mEzEUoo7cesPuNU/Ev9A/8I7YwpqEZ2Q5My595yZe6+XBDyVtv1aMKamZ2bnivOlhcWl5RVztdxM40z4rOHHQSzanpuygEesIbkMWDsRzA29gLW8q0MVb10zkfI4OpGDhHVD9yLife67kqieWe6cha68FOGwPuoN+bYz6vbMil219bImgZODCvJVj80XnOEcMXxkCMEQQRIO4CKlpwMHNhLiuhgSJwhxHWcYoUTajLIYZbjEXtH3gnadnI1orzxTrfbplIBeQUoLm6SJKU8QVqdZOp5pZ8X+5j3UnupuA/p7uVdIrMQlsX/pxpn/1alaJPrY1zVwqinRjKrOz10y3RV1c+tLVZIcEuIUPqe4IOxr5bjPltakunbVW1fH33SmYtXez3MzvKtb0oCdn+OcBM2dqmNXnePdSu0gH3UR69jAFs1zDzUcoY4Ged/gEU94Nk6NW+POuP9MNQq5Zg3flvHwAURqluE=</latexit><latexit sha1_base64=\"T4HkTbSj3PFQ1GfAlSrlXjyhL+Y=\">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIsgCCURQZdFNy4r2IfUUpI41cG8mEzEUoo7cesPuNU/Ev9A/8I7YwpqEZ2Q5My595yZe6+XBDyVtv1aMKamZ2bnivOlhcWl5RVztdxM40z4rOHHQSzanpuygEesIbkMWDsRzA29gLW8q0MVb10zkfI4OpGDhHVD9yLife67kqieWe6cha68FOGwPuoN+bYz6vbMil219bImgZODCvJVj80XnOEcMXxkCMEQQRIO4CKlpwMHNhLiuhgSJwhxHWcYoUTajLIYZbjEXtH3gnadnI1orzxTrfbplIBeQUoLm6SJKU8QVqdZOp5pZ8X+5j3UnupuA/p7uVdIrMQlsX/pxpn/1alaJPrY1zVwqinRjKrOz10y3RV1c+tLVZIcEuIUPqe4IOxr5bjPltakunbVW1fH33SmYtXez3MzvKtb0oCdn+OcBM2dqmNXnePdSu0gH3UR69jAFs1zDzUcoY4Ged/gEU94Nk6NW+POuP9MNQq5Zg3flvHwAURqluE=</latexit>\n[P m ]\n<latexit sha1_base64=\"tjiKpTOvdf5v8j22gLqr1mbMF+o=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtNu0OkWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+ED41uVOA==</latexit><latexit sha1_base64=\"tjiKpTOvdf5v8j22gLqr1mbMF+o=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtNu0OkWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+ED41uVOA==</latexit><latexit sha1_base64=\"tjiKpTOvdf5v8j22gLqr1mbMF+o=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtNu0OkWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+ED41uVOA==</latexit><latexit sha1_base64=\"tjiKpTOvdf5v8j22gLqr1mbMF+o=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtNu0OkWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+ED41uVOA==</latexit>\n… …\nh 0\n<latexit sha1_base64=\"XIhpn8OxNUvhZRM6e2EnUbWV7tU=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1HP6ZUrTtXRy54HrgEVmNVIyi+4QR8JAuSIwBBDEg7hIaOnAxcOUuK6mBInCHEdZ7hHibQ5ZTHK8Igd03dIu45hY9orz0yrAzolpFeQ0sYBaRLKE4TVabaO59pZsb95T7WnutuE/r7xioiVGBH7l26W+V+dqkVigFNdA6eaUs2o6gLjkuuuqJvbX6qS5JASp3Cf4oJwoJWzPttak+naVW89HX/TmYpV+8Dk5nhXt6QBuz/HOQ9aR1XXqboXx5XamRl1EXvYxyHN8wQ11NFAk7yHeMQTnq26FVu5dfeZahWMZhfflvXwAeV8kBA=</latexit><latexit sha1_base64=\"XIhpn8OxNUvhZRM6e2EnUbWV7tU=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1HP6ZUrTtXRy54HrgEVmNVIyi+4QR8JAuSIwBBDEg7hIaOnAxcOUuK6mBInCHEdZ7hHibQ5ZTHK8Igd03dIu45hY9orz0yrAzolpFeQ0sYBaRLKE4TVabaO59pZsb95T7WnutuE/r7xioiVGBH7l26W+V+dqkVigFNdA6eaUs2o6gLjkuuuqJvbX6qS5JASp3Cf4oJwoJWzPttak+naVW89HX/TmYpV+8Dk5nhXt6QBuz/HOQ9aR1XXqboXx5XamRl1EXvYxyHN8wQ11NFAk7yHeMQTnq26FVu5dfeZahWMZhfflvXwAeV8kBA=</latexit><latexit sha1_base64=\"XIhpn8OxNUvhZRM6e2EnUbWV7tU=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1HP6ZUrTtXRy54HrgEVmNVIyi+4QR8JAuSIwBBDEg7hIaOnAxcOUuK6mBInCHEdZ7hHibQ5ZTHK8Igd03dIu45hY9orz0yrAzolpFeQ0sYBaRLKE4TVabaO59pZsb95T7WnutuE/r7xioiVGBH7l26W+V+dqkVigFNdA6eaUs2o6gLjkuuuqJvbX6qS5JASp3Cf4oJwoJWzPttak+naVW89HX/TmYpV+8Dk5nhXt6QBuz/HOQ9aR1XXqboXx5XamRl1EXvYxyHN8wQ11NFAk7yHeMQTnq26FVu5dfeZahWMZhfflvXwAeV8kBA=</latexit><latexit sha1_base64=\"XIhpn8OxNUvhZRM6e2EnUbWV7tU=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1HP6ZUrTtXRy54HrgEVmNVIyi+4QR8JAuSIwBBDEg7hIaOnAxcOUuK6mBInCHEdZ7hHibQ5ZTHK8Igd03dIu45hY9orz0yrAzolpFeQ0sYBaRLKE4TVabaO59pZsb95T7WnutuE/r7xioiVGBH7l26W+V+dqkVigFNdA6eaUs2o6gLjkuuuqJvbX6qS5JASp3Cf4oJwoJWzPttak+naVW89HX/TmYpV+8Dk5nhXt6QBuz/HOQ9aR1XXqboXx5XamRl1EXvYxyHN8wQ11NFAk7yHeMQTnq26FVu5dfeZahWMZhfflvXwAeV8kBA=</latexit>\nh i\n<latexit sha1_base64=\"nWnmfxj2c+WFY4VamC6wC03+Ux4=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1GP98oVp+roZc8D14AKzGok5RfcoI8EAXJEYIghCYfwkNHTgQsHKXFdTIkThLiOM9yjRNqcshhleMSO6TukXcewMe2VZ6bVAZ0S0itIaeOANAnlCcLqNFvHc+2s2N+8p9pT3W1Cf994RcRKjIj9SzfL/K9O1SIxwKmugVNNqWZUdYFxyXVX1M3tL1VJckiJU7hPcUE40MpZn22tyXTtqreejr/pTMWqfWByc7yrW9KA3Z/jnAeto6rrVN2L40rtzIy6iD3s45DmeYIa6migSd5DPOIJz1bdiq3cuvtMtQpGs4tvy3r4AGzrkEk=</latexit><latexit sha1_base64=\"nWnmfxj2c+WFY4VamC6wC03+Ux4=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1GP98oVp+roZc8D14AKzGok5RfcoI8EAXJEYIghCYfwkNHTgQsHKXFdTIkThLiOM9yjRNqcshhleMSO6TukXcewMe2VZ6bVAZ0S0itIaeOANAnlCcLqNFvHc+2s2N+8p9pT3W1Cf994RcRKjIj9SzfL/K9O1SIxwKmugVNNqWZUdYFxyXVX1M3tL1VJckiJU7hPcUE40MpZn22tyXTtqreejr/pTMWqfWByc7yrW9KA3Z/jnAeto6rrVN2L40rtzIy6iD3s45DmeYIa6migSd5DPOIJz1bdiq3cuvtMtQpGs4tvy3r4AGzrkEk=</latexit><latexit sha1_base64=\"nWnmfxj2c+WFY4VamC6wC03+Ux4=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1GP98oVp+roZc8D14AKzGok5RfcoI8EAXJEYIghCYfwkNHTgQsHKXFdTIkThLiOM9yjRNqcshhleMSO6TukXcewMe2VZ6bVAZ0S0itIaeOANAnlCcLqNFvHc+2s2N+8p9pT3W1Cf994RcRKjIj9SzfL/K9O1SIxwKmugVNNqWZUdYFxyXVX1M3tL1VJckiJU7hPcUE40MpZn22tyXTtqreejr/pTMWqfWByc7yrW9KA3Z/jnAeto6rrVN2L40rtzIy6iD3s45DmeYIa6migSd5DPOIJz1bdiq3cuvtMtQpGs4tvy3r4AGzrkEk=</latexit><latexit sha1_base64=\"nWnmfxj2c+WFY4VamC6wC03+Ux4=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1GP98oVp+roZc8D14AKzGok5RfcoI8EAXJEYIghCYfwkNHTgQsHKXFdTIkThLiOM9yjRNqcshhleMSO6TukXcewMe2VZ6bVAZ0S0itIaeOANAnlCcLqNFvHc+2s2N+8p9pT3W1Cf994RcRKjIj9SzfL/K9O1SIxwKmugVNNqWZUdYFxyXVX1M3tL1VJckiJU7hPcUE40MpZn22tyXTtqreejr/pTMWqfWByc7yrW9KA3Z/jnAeto6rrVN2L40rtzIy6iD3s45DmeYIa6migSd5DPOIJz1bdiq3cuvtMtQpGs4tvy3r4AGzrkEk=</latexit>\n… h i +1\n<latexit sha1_base64=\"uv7sHjM75Zjq7Y01g85optkEEAs=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkUQhJKIoMuiGxcuKtgH1FKS6bQdTJMwmQgldOcPuNUPE/9A/8I7YwpqEZ2Q5My559yZe68fByJRjvNasBYWl5ZXiqultfWNza3y9k4ziVLJeINFQSTbvpfwQIS8oYQKeDuW3Bv7AW/5dxc63rrnMhFReKMmMe+OvWEoBoJ5iqjWqJeJI3faK1ecqmOWPQ/cHFSQr3pUfsEt+ojAkGIMjhCKcAAPCT0duHAQE9dFRpwkJEycY4oSeVNScVJ4xN7Rd0i7Ts6GtNc5E+NmdEpArySnjQPyRKSThPVptomnJrNmf8udmZz6bhP6+3muMbEKI2L/8s2U//XpWhQGODM1CKopNoyujuVZUtMVfXP7S1WKMsTEadynuCTMjHPWZ9t4ElO77q1n4m9GqVm9Z7k2xbu+JQ3Y/TnOedA8rrpO1b0+qdTO81EXsYd9HNI8T1HDJepomCof8YRn68qS1sTKPqVWIffs4tuyHj4AMo2RxQ==</latexit><latexit sha1_base64=\"uv7sHjM75Zjq7Y01g85optkEEAs=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkUQhJKIoMuiGxcuKtgH1FKS6bQdTJMwmQgldOcPuNUPE/9A/8I7YwpqEZ2Q5My559yZe68fByJRjvNasBYWl5ZXiqultfWNza3y9k4ziVLJeINFQSTbvpfwQIS8oYQKeDuW3Bv7AW/5dxc63rrnMhFReKMmMe+OvWEoBoJ5iqjWqJeJI3faK1ecqmOWPQ/cHFSQr3pUfsEt+ojAkGIMjhCKcAAPCT0duHAQE9dFRpwkJEycY4oSeVNScVJ4xN7Rd0i7Ts6GtNc5E+NmdEpArySnjQPyRKSThPVptomnJrNmf8udmZz6bhP6+3muMbEKI2L/8s2U//XpWhQGODM1CKopNoyujuVZUtMVfXP7S1WKMsTEadynuCTMjHPWZ9t4ElO77q1n4m9GqVm9Z7k2xbu+JQ3Y/TnOedA8rrpO1b0+qdTO81EXsYd9HNI8T1HDJepomCof8YRn68qS1sTKPqVWIffs4tuyHj4AMo2RxQ==</latexit><latexit sha1_base64=\"uv7sHjM75Zjq7Y01g85optkEEAs=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkUQhJKIoMuiGxcuKtgH1FKS6bQdTJMwmQgldOcPuNUPE/9A/8I7YwpqEZ2Q5My559yZe68fByJRjvNasBYWl5ZXiqultfWNza3y9k4ziVLJeINFQSTbvpfwQIS8oYQKeDuW3Bv7AW/5dxc63rrnMhFReKMmMe+OvWEoBoJ5iqjWqJeJI3faK1ecqmOWPQ/cHFSQr3pUfsEt+ojAkGIMjhCKcAAPCT0duHAQE9dFRpwkJEycY4oSeVNScVJ4xN7Rd0i7Ts6GtNc5E+NmdEpArySnjQPyRKSThPVptomnJrNmf8udmZz6bhP6+3muMbEKI2L/8s2U//XpWhQGODM1CKopNoyujuVZUtMVfXP7S1WKMsTEadynuCTMjHPWZ9t4ElO77q1n4m9GqVm9Z7k2xbu+JQ3Y/TnOedA8rrpO1b0+qdTO81EXsYd9HNI8T1HDJepomCof8YRn68qS1sTKPqVWIffs4tuyHj4AMo2RxQ==</latexit><latexit sha1_base64=\"uv7sHjM75Zjq7Y01g85optkEEAs=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkUQhJKIoMuiGxcuKtgH1FKS6bQdTJMwmQgldOcPuNUPE/9A/8I7YwpqEZ2Q5My559yZe68fByJRjvNasBYWl5ZXiqultfWNza3y9k4ziVLJeINFQSTbvpfwQIS8oYQKeDuW3Bv7AW/5dxc63rrnMhFReKMmMe+OvWEoBoJ5iqjWqJeJI3faK1ecqmOWPQ/cHFSQr3pUfsEt+ojAkGIMjhCKcAAPCT0duHAQE9dFRpwkJEycY4oSeVNScVJ4xN7Rd0i7Ts6GtNc5E+NmdEpArySnjQPyRKSThPVptomnJrNmf8udmZz6bhP6+3muMbEKI2L/8s2U//XpWhQGODM1CKopNoyujuVZUtMVfXP7S1WKMsTEadynuCTMjHPWZ9t4ElO77q1n4m9GqVm9Z7k2xbu+JQ3Y/TnOedA8rrpO1b0+qdTO81EXsYd9HNI8T1HDJepomCof8YRn68qS1sTKPqVWIffs4tuyHj4AMo2RxQ==</latexit>\nh m\n<latexit sha1_base64=\"MV7vt71uo3HwFtGIdDyhAdpOcgw=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lGQ6bYfmxWSilCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rJ4FIleO8FqyFxaXlleJqaW19Y3OrvL3TSuNMMt5kcRDLa99LeSAi3lRCBfw6kdwL/YC3/fG5jrdvuUxFHF2pScK7oTeMxEAwTxF1OeqFvXLFqTpm2fPAzUEF+WrE5RfcoI8YDBlCcERQhAN4SOnpwIWDhLgupsRJQsLEOe5RIm1GWZwyPGLH9B3SrpOzEe21Z2rUjE4J6JWktHFAmpjyJGF9mm3imXHW7G/eU+Op7zahv597hcQqjIj9SzfL/K9O16IwwKmpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hPcUmYGeWsz7bRpKZ23VvPxN9Mpmb1nuW5Gd71LWnA7s9xzoPWUdV1qu7FcaV2lo+6iD3s45DmeYIa6migSd5DPOIJz1bdiqzMuvtMtQq5ZhfflvXwAXZrkE0=</latexit><latexit sha1_base64=\"MV7vt71uo3HwFtGIdDyhAdpOcgw=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lGQ6bYfmxWSilCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rJ4FIleO8FqyFxaXlleJqaW19Y3OrvL3TSuNMMt5kcRDLa99LeSAi3lRCBfw6kdwL/YC3/fG5jrdvuUxFHF2pScK7oTeMxEAwTxF1OeqFvXLFqTpm2fPAzUEF+WrE5RfcoI8YDBlCcERQhAN4SOnpwIWDhLgupsRJQsLEOe5RIm1GWZwyPGLH9B3SrpOzEe21Z2rUjE4J6JWktHFAmpjyJGF9mm3imXHW7G/eU+Op7zahv597hcQqjIj9SzfL/K9O16IwwKmpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hPcUmYGeWsz7bRpKZ23VvPxN9Mpmb1nuW5Gd71LWnA7s9xzoPWUdV1qu7FcaV2lo+6iD3s45DmeYIa6migSd5DPOIJz1bdiqzMuvtMtQq5ZhfflvXwAXZrkE0=</latexit><latexit sha1_base64=\"MV7vt71uo3HwFtGIdDyhAdpOcgw=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lGQ6bYfmxWSilCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rJ4FIleO8FqyFxaXlleJqaW19Y3OrvL3TSuNMMt5kcRDLa99LeSAi3lRCBfw6kdwL/YC3/fG5jrdvuUxFHF2pScK7oTeMxEAwTxF1OeqFvXLFqTpm2fPAzUEF+WrE5RfcoI8YDBlCcERQhAN4SOnpwIWDhLgupsRJQsLEOe5RIm1GWZwyPGLH9B3SrpOzEe21Z2rUjE4J6JWktHFAmpjyJGF9mm3imXHW7G/eU+Op7zahv597hcQqjIj9SzfL/K9O16IwwKmpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hPcUmYGeWsz7bRpKZ23VvPxN9Mpmb1nuW5Gd71LWnA7s9xzoPWUdV1qu7FcaV2lo+6iD3s45DmeYIa6migSd5DPOIJz1bdiqzMuvtMtQq5ZhfflvXwAXZrkE0=</latexit><latexit sha1_base64=\"MV7vt71uo3HwFtGIdDyhAdpOcgw=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lGQ6bYfmxWSilCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rJ4FIleO8FqyFxaXlleJqaW19Y3OrvL3TSuNMMt5kcRDLa99LeSAi3lRCBfw6kdwL/YC3/fG5jrdvuUxFHF2pScK7oTeMxEAwTxF1OeqFvXLFqTpm2fPAzUEF+WrE5RfcoI8YDBlCcERQhAN4SOnpwIWDhLgupsRJQsLEOe5RIm1GWZwyPGLH9B3SrpOzEe21Z2rUjE4J6JWktHFAmpjyJGF9mm3imXHW7G/eU+Op7zahv597hcQqjIj9SzfL/K9O16IwwKmpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hPcUmYGeWsz7bRpKZ23VvPxN9Mpmb1nuW5Gd71LWnA7s9xzoPWUdV1qu7FcaV2lo+6iD3s45DmeYIa6migSd5DPOIJz1bdiqzMuvtMtQq5ZhfflvXwAXZrkE0=</latexit>\n…\nPseudo Prompts\nInput embedding\nBack \nPropagation\nPre-trained Language Model\n(GPT, BERT, … \u0001\nPrompt Generator\nInput embedding\nDiscrete rewards\nBritain [MASK]The capital of is\ne (Britain)\n<latexit sha1_base64=\"apa0nk0gEVHoA/TQZedTMI5FqmY=\">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64=\"apa0nk0gEVHoA/TQZedTMI5FqmY=\">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64=\"apa0nk0gEVHoA/TQZedTMI5FqmY=\">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64=\"apa0nk0gEVHoA/TQZedTMI5FqmY=\">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit>\ne (The)\n<latexit sha1_base64=\"X/YIDIxg/Hse8WN1EcNWk2xxhY8=\">AAAC23icjVHLSsNAFD2Nr1pfUcGNm2AR6qYkIuiy6MZlhb6gLSVJp20wL5KJWGJX7sStP+BW/0f8A/0L74wpqEV0QpIz595zZu69Vug6Mdf115wyN7+wuJRfLqysrq1vqJtbjThIIpvV7cANopZlxsx1fFbnDndZK4yY6Vkua1qXZyLevGJR7AR+jY9D1vXMoe8MHNvkRPXUnY5n8pE1SNmk1OHsmqe1EZsc9NSiXtbl0maBkYEislUN1Bd00EcAGwk8MPjghF2YiOlpw4COkLguUuIiQo6MM0xQIG1CWYwyTGIv6TukXTtjfdoLz1iqbTrFpTcipYZ90gSUFxEWp2kynkhnwf7mnUpPcbcx/a3MyyOWY0TsX7pp5n91ohaOAU5kDQ7VFEpGVGdnLonsiri59qUqTg4hcQL3KR4RtqVy2mdNamJZu+itKeNvMlOwYm9nuQnexS1pwMbPcc6CxmHZ0MvGxVGxcpqNOo9d7KFE8zxGBeeook7eN3jEE56VrnKr3Cn3n6lKLtNs49tSHj4ACkCYvg==</latexit><latexit sha1_base64=\"X/YIDIxg/Hse8WN1EcNWk2xxhY8=\">AAAC23icjVHLSsNAFD2Nr1pfUcGNm2AR6qYkIuiy6MZlhb6gLSVJp20wL5KJWGJX7sStP+BW/0f8A/0L74wpqEV0QpIz595zZu69Vug6Mdf115wyN7+wuJRfLqysrq1vqJtbjThIIpvV7cANopZlxsx1fFbnDndZK4yY6Vkua1qXZyLevGJR7AR+jY9D1vXMoe8MHNvkRPXUnY5n8pE1SNmk1OHsmqe1EZsc9NSiXtbl0maBkYEislUN1Bd00EcAGwk8MPjghF2YiOlpw4COkLguUuIiQo6MM0xQIG1CWYwyTGIv6TukXTtjfdoLz1iqbTrFpTcipYZ90gSUFxEWp2kynkhnwf7mnUpPcbcx/a3MyyOWY0TsX7pp5n91ohaOAU5kDQ7VFEpGVGdnLonsiri59qUqTg4hcQL3KR4RtqVy2mdNamJZu+itKeNvMlOwYm9nuQnexS1pwMbPcc6CxmHZ0MvGxVGxcpqNOo9d7KFE8zxGBeeook7eN3jEE56VrnKr3Cn3n6lKLtNs49tSHj4ACkCYvg==</latexit><latexit sha1_base64=\"X/YIDIxg/Hse8WN1EcNWk2xxhY8=\">AAAC23icjVHLSsNAFD2Nr1pfUcGNm2AR6qYkIuiy6MZlhb6gLSVJp20wL5KJWGJX7sStP+BW/0f8A/0L74wpqEV0QpIz595zZu69Vug6Mdf115wyN7+wuJRfLqysrq1vqJtbjThIIpvV7cANopZlxsx1fFbnDndZK4yY6Vkua1qXZyLevGJR7AR+jY9D1vXMoe8MHNvkRPXUnY5n8pE1SNmk1OHsmqe1EZsc9NSiXtbl0maBkYEislUN1Bd00EcAGwk8MPjghF2YiOlpw4COkLguUuIiQo6MM0xQIG1CWYwyTGIv6TukXTtjfdoLz1iqbTrFpTcipYZ90gSUFxEWp2kynkhnwf7mnUpPcbcx/a3MyyOWY0TsX7pp5n91ohaOAU5kDQ7VFEpGVGdnLonsiri59qUqTg4hcQL3KR4RtqVy2mdNamJZu+itKeNvMlOwYm9nuQnexS1pwMbPcc6CxmHZ0MvGxVGxcpqNOo9d7KFE8zxGBeeook7eN3jEE56VrnKr3Cn3n6lKLtNs49tSHj4ACkCYvg==</latexit><latexit sha1_base64=\"X/YIDIxg/Hse8WN1EcNWk2xxhY8=\">AAAC23icjVHLSsNAFD2Nr1pfUcGNm2AR6qYkIuiy6MZlhb6gLSVJp20wL5KJWGJX7sStP+BW/0f8A/0L74wpqEV0QpIz595zZu69Vug6Mdf115wyN7+wuJRfLqysrq1vqJtbjThIIpvV7cANopZlxsx1fFbnDndZK4yY6Vkua1qXZyLevGJR7AR+jY9D1vXMoe8MHNvkRPXUnY5n8pE1SNmk1OHsmqe1EZsc9NSiXtbl0maBkYEislUN1Bd00EcAGwk8MPjghF2YiOlpw4COkLguUuIiQo6MM0xQIG1CWYwyTGIv6TukXTtjfdoLz1iqbTrFpTcipYZ90gSUFxEWp2kynkhnwf7mnUpPcbcx/a3MyyOWY0TsX7pp5n91ohaOAU5kDQ7VFEpGVGdnLonsiri59qUqTg4hcQL3KR4RtqVy2mdNamJZu+itKeNvMlOwYm9nuQnexS1pwMbPcc6CxmHZ0MvGxVGxcpqNOo9d7KFE8zxGBeeook7eN3jEE56VrnKr3Cn3n6lKLtNs49tSHj4ACkCYvg==</latexit>\ne (capital)\n<latexit sha1_base64=\"rwoN1CZfwDAAgs823DmssBjrTSU=\">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64=\"rwoN1CZfwDAAgs823DmssBjrTSU=\">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64=\"rwoN1CZfwDAAgs823DmssBjrTSU=\">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64=\"rwoN1CZfwDAAgs823DmssBjrTSU=\">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit>\ne (of)\n<latexit sha1_base64=\"flebG6e5LB7rimfipQ3vJzIFXBg=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gKSVJJ20wL5KJWEI37sStP+BWP0j8A/0L74wpqEV0QpIz595zZu69VuS5Cde014IyN7+wuFRcLq2srq1vlDe3WkmYxjZr2qEXxh3LTJjnBqzJXe6xThQz07c81rauzkS8fc3ixA2DSz6OWM83h4HruLbJieqXdwzf5CPLydikanB2w7PQmRz0yxWtpsmlzgI9BxXkqxGWX2BggBA2UvhgCMAJezCR0NOFDg0RcT1kxMWEXBlnmKBE2pSyGGWYxF7Rd0i7bs4GtBeeiVTbdIpHb0xKFfukCSkvJixOU2U8lc6C/c07k57ibmP6W7mXTyzHiNi/dNPM/+pELRwOTmQNLtUUSUZUZ+cuqeyKuLn6pSpODhFxAg8oHhO2pXLaZ1VqElm76K0p428yU7Bib+e5Kd7FLWnA+s9xzoLWYU3XavrFUaV+mo+6iF3soUrzPEYd52igSd4ZHvGEZ8VQbpU75f4zVSnkmm18W8rDByeCmGg=</latexit><latexit sha1_base64=\"flebG6e5LB7rimfipQ3vJzIFXBg=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gKSVJJ20wL5KJWEI37sStP+BWP0j8A/0L74wpqEV0QpIz595zZu69VuS5Cde014IyN7+wuFRcLq2srq1vlDe3WkmYxjZr2qEXxh3LTJjnBqzJXe6xThQz07c81rauzkS8fc3ixA2DSz6OWM83h4HruLbJieqXdwzf5CPLydikanB2w7PQmRz0yxWtpsmlzgI9BxXkqxGWX2BggBA2UvhgCMAJezCR0NOFDg0RcT1kxMWEXBlnmKBE2pSyGGWYxF7Rd0i7bs4GtBeeiVTbdIpHb0xKFfukCSkvJixOU2U8lc6C/c07k57ibmP6W7mXTyzHiNi/dNPM/+pELRwOTmQNLtUUSUZUZ+cuqeyKuLn6pSpODhFxAg8oHhO2pXLaZ1VqElm76K0p428yU7Bib+e5Kd7FLWnA+s9xzoLWYU3XavrFUaV+mo+6iF3soUrzPEYd52igSd4ZHvGEZ8VQbpU75f4zVSnkmm18W8rDByeCmGg=</latexit><latexit sha1_base64=\"flebG6e5LB7rimfipQ3vJzIFXBg=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gKSVJJ20wL5KJWEI37sStP+BWP0j8A/0L74wpqEV0QpIz595zZu69VuS5Cde014IyN7+wuFRcLq2srq1vlDe3WkmYxjZr2qEXxh3LTJjnBqzJXe6xThQz07c81rauzkS8fc3ixA2DSz6OWM83h4HruLbJieqXdwzf5CPLydikanB2w7PQmRz0yxWtpsmlzgI9BxXkqxGWX2BggBA2UvhgCMAJezCR0NOFDg0RcT1kxMWEXBlnmKBE2pSyGGWYxF7Rd0i7bs4GtBeeiVTbdIpHb0xKFfukCSkvJixOU2U8lc6C/c07k57ibmP6W7mXTyzHiNi/dNPM/+pELRwOTmQNLtUUSUZUZ+cuqeyKuLn6pSpODhFxAg8oHhO2pXLaZ1VqElm76K0p428yU7Bib+e5Kd7FLWnA+s9xzoLWYU3XavrFUaV+mo+6iF3soUrzPEYd52igSd4ZHvGEZ8VQbpU75f4zVSnkmm18W8rDByeCmGg=</latexit><latexit sha1_base64=\"flebG6e5LB7rimfipQ3vJzIFXBg=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gKSVJJ20wL5KJWEI37sStP+BWP0j8A/0L74wpqEV0QpIz595zZu69VuS5Cde014IyN7+wuFRcLq2srq1vlDe3WkmYxjZr2qEXxh3LTJjnBqzJXe6xThQz07c81rauzkS8fc3ixA2DSz6OWM83h4HruLbJieqXdwzf5CPLydikanB2w7PQmRz0yxWtpsmlzgI9BxXkqxGWX2BggBA2UvhgCMAJezCR0NOFDg0RcT1kxMWEXBlnmKBE2pSyGGWYxF7Rd0i7bs4GtBeeiVTbdIpHb0xKFfukCSkvJixOU2U8lc6C/c07k57ibmP6W7mXTyzHiNi/dNPM/+pELRwOTmQNLtUUSUZUZ+cuqeyKuLn6pSpODhFxAg8oHhO2pXLaZ1VqElm76K0p428yU7Bib+e5Kd7FLWnA+s9xzoLWYU3XavrFUaV+mo+6iF3soUrzPEYd52igSd4ZHvGEZ8VQbpU75f4zVSnkmm18W8rDByeCmGg=</latexit>\ne (is)\n<latexit sha1_base64=\"u3KTSyivQRk08+u91kKYRwDoHq0=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gLSVJp+1gXiQTsYRu3Ilbf8CtfpD4B/oX3hlTUIvohCRnzr3nzNx77dDlsTCM15w2N7+wuJRfLqysrq1vFDe3GnGQRA6rO4EbRC3bipnLfVYXXLisFUbM8myXNe2rMxlvXrMo5oF/KcYh63rW0OcD7liCqF5xp+NZYmQPUjYpdwS7ESmPJwe9YsmoGGrps8DMQAnZqgXFF3TQRwAHCTww+BCEXViI6WnDhIGQuC5S4iJCXMUZJiiQNqEsRhkWsVf0HdKunbE+7aVnrNQOneLSG5FSxz5pAsqLCMvTdBVPlLNkf/NOlae825j+dublESswIvYv3TTzvzpZi8AAJ6oGTjWFipHVOZlLoroib65/qUqQQ0icxH2KR4QdpZz2WVeaWNUue2up+JvKlKzcO1lugnd5Sxqw+XOcs6BxWDGNinlxVKqeZqPOYxd7KNM8j1HFOWqok3eKRzzhWetot9qddv+ZquUyzTa+Le3hAzgqmG8=</latexit><latexit sha1_base64=\"u3KTSyivQRk08+u91kKYRwDoHq0=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gLSVJp+1gXiQTsYRu3Ilbf8CtfpD4B/oX3hlTUIvohCRnzr3nzNx77dDlsTCM15w2N7+wuJRfLqysrq1vFDe3GnGQRA6rO4EbRC3bipnLfVYXXLisFUbM8myXNe2rMxlvXrMo5oF/KcYh63rW0OcD7liCqF5xp+NZYmQPUjYpdwS7ESmPJwe9YsmoGGrps8DMQAnZqgXFF3TQRwAHCTww+BCEXViI6WnDhIGQuC5S4iJCXMUZJiiQNqEsRhkWsVf0HdKunbE+7aVnrNQOneLSG5FSxz5pAsqLCMvTdBVPlLNkf/NOlae825j+dublESswIvYv3TTzvzpZi8AAJ6oGTjWFipHVOZlLoroib65/qUqQQ0icxH2KR4QdpZz2WVeaWNUue2up+JvKlKzcO1lugnd5Sxqw+XOcs6BxWDGNinlxVKqeZqPOYxd7KNM8j1HFOWqok3eKRzzhWetot9qddv+ZquUyzTa+Le3hAzgqmG8=</latexit><latexit sha1_base64=\"u3KTSyivQRk08+u91kKYRwDoHq0=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gLSVJp+1gXiQTsYRu3Ilbf8CtfpD4B/oX3hlTUIvohCRnzr3nzNx77dDlsTCM15w2N7+wuJRfLqysrq1vFDe3GnGQRA6rO4EbRC3bipnLfVYXXLisFUbM8myXNe2rMxlvXrMo5oF/KcYh63rW0OcD7liCqF5xp+NZYmQPUjYpdwS7ESmPJwe9YsmoGGrps8DMQAnZqgXFF3TQRwAHCTww+BCEXViI6WnDhIGQuC5S4iJCXMUZJiiQNqEsRhkWsVf0HdKunbE+7aVnrNQOneLSG5FSxz5pAsqLCMvTdBVPlLNkf/NOlae825j+dublESswIvYv3TTzvzpZi8AAJ6oGTjWFipHVOZlLoroib65/qUqQQ0icxH2KR4QdpZz2WVeaWNUue2up+JvKlKzcO1lugnd5Sxqw+XOcs6BxWDGNinlxVKqeZqPOYxd7KNM8j1HFOWqok3eKRzzhWetot9qddv+ZquUyzTa+Le3hAzgqmG8=</latexit><latexit sha1_base64=\"u3KTSyivQRk08+u91kKYRwDoHq0=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gLSVJp+1gXiQTsYRu3Ilbf8CtfpD4B/oX3hlTUIvohCRnzr3nzNx77dDlsTCM15w2N7+wuJRfLqysrq1vFDe3GnGQRA6rO4EbRC3bipnLfVYXXLisFUbM8myXNe2rMxlvXrMo5oF/KcYh63rW0OcD7liCqF5xp+NZYmQPUjYpdwS7ESmPJwe9YsmoGGrps8DMQAnZqgXFF3TQRwAHCTww+BCEXViI6WnDhIGQuC5S4iJCXMUZJiiQNqEsRhkWsVf0HdKunbE+7aVnrNQOneLSG5FSxz5pAsqLCMvTdBVPlLNkf/NOlae825j+dublESswIvYv3TTzvzpZi8AAJ6oGTjWFipHVOZlLoroib65/qUqQQ0icxH2KR4QdpZz2WVeaWNUue2up+JvKlKzcO1lugnd5Sxqw+XOcs6BxWDGNinlxVKqeZqPOYxd7KNM8j1HFOWqok3eKRzzhWetot9qddv+ZquUyzTa+Le3hAzgqmG8=</latexit>\ne ([MASK])\n<latexit sha1_base64=\"lmVmOIHtBwXAqq1jPqBsI6SCI4s=\">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64=\"lmVmOIHtBwXAqq1jPqBsI6SCI4s=\">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64=\"lmVmOIHtBwXAqq1jPqBsI6SCI4s=\">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64=\"lmVmOIHtBwXAqq1jPqBsI6SCI4s=\">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit>\nBritain [MASK]\ne (Britain)\n<latexit sha1_base64=\"apa0nk0gEVHoA/TQZedTMI5FqmY=\">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64=\"apa0nk0gEVHoA/TQZedTMI5FqmY=\">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64=\"apa0nk0gEVHoA/TQZedTMI5FqmY=\">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64=\"apa0nk0gEVHoA/TQZedTMI5FqmY=\">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit>\ne ([MASK])\n<latexit sha1_base64=\"lmVmOIHtBwXAqq1jPqBsI6SCI4s=\">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64=\"lmVmOIHtBwXAqq1jPqBsI6SCI4s=\">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64=\"lmVmOIHtBwXAqq1jPqBsI6SCI4s=\">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64=\"lmVmOIHtBwXAqq1jPqBsI6SCI4s=\">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit>\n(a) Discrete Prompt Search (b) P-tuning\ncapital\ne (capital)\n<latexit sha1_base64=\"rwoN1CZfwDAAgs823DmssBjrTSU=\">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64=\"rwoN1CZfwDAAgs823DmssBjrTSU=\">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64=\"rwoN1CZfwDAAgs823DmssBjrTSU=\">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64=\"rwoN1CZfwDAAgs823DmssBjrTSU=\">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit>\nFigure 2: An example of prompt search for “The capital of Britain is [MASK]”. Given the context (blue zone,\n“Britain”) and target (red zone, “[MASK]”), the orange zone refer to the prompt. In (a), the prompt generator only\nreceives discrete rewards; on the contrary, in (b) the continuous prompt embeddings and prompt encoder can be\noptimized in a differentiable way.\nprompting. Let [Pi] be the ith continuous prompt\nembedding. The prompt template for P-Tuning is\nas follows:\nT = {[P0:i], x, [P(i+1):j], y, [P(j+1):k]}\nP-Tuning leverages an extra embedding function\nf : [Pi] → hi to map the template to\n{h0, ..., hi, e(x), hi+1, ..., hj, e(y), hj+1, ..., hk}\nFinally, we update the embeddings {Pi}k\ni=1 to op-\ntimize a task loss function.\nIt is noteworthy that we can also concatenate\ndiscrete prompts with continuous prompts, which\nperforms better and is adopted throughout our ex-\nperiments. P-Tuning is applicable to both frozen\nand finetuned language models.\n2.3 Prompt Encoder\nIn the aforementioned framework, we employ a\nmapping function f to map trainable embeddings\n{Pi} to model inputs {hi}. The intuition is that\nby using a mapping function, it is more conve-\nnient to model the dependency between different\nprompt embeddings, compared to using indepen-\ndent learnable embeddings. In our implementation,\nwe use a lightweight neural network to formulate\nthe function f. Specifically, we experiment with\nusing long short-term memory (LSTM) networks,\nmulti-layer perceptrons (MLPs), and the identity\nmapping function in Section 3.\n3 Experiments\nWe include two NLU benchmarks: LAMA (Petroni\net al., 2019) for knowledge probing (§ 3.1) and Su-\nperGLUE (Wang et al., 2019a) for general natural\nlanguage understanding. On SuperGLUE, we con-\nsider both the fully-supervised learning (§ 3.2) and\nfew-shot learning (§ 3.3) settings.\nLAMA Full SG Few SG\nfrozen tuned tuned\nImproved ✓ ✓ ✓\nStabilized ✓ ✗ ✓\nTable 2: Task settings and summary of results in our\nexperiments. P-tuning shows improvement over base-\nlines on all task settings, and can stabilize performance\non LAMA and Few SG. For Full SG, the gap between\ndiscrete prompts is not large and training is stable even\nwithout P-Tuning. (Full SG: fully-supervised learn-\ning on SuperGLUE; Few SG: few-shot SuperGLUE;\nImproved: overall performance improved; Stabilized:\ntraining stabilized by minimizing difference between\ndiscrete prompts).\nOn LAMA, following Shin et al. (2020); Jiang\net al. (2020b), language models are frozen and only\nthe discrete or continious prompts are tuned. For\nSuperGLUE, following Schick and Schütze (2020);\nZheng et al. (2021), language models are tuned. In\nour setting, we jointly optimize the language model\nparameters and the continuous prompts. This setup\nnot only follows the common, standard settings in\nprior work, but also allows evaluating P-Tuning\nwith both tuned and frozen language models.\nThe overall task setup and a summary of results\nare shown in Table 2.\n3.1 Knowledge Probing\n3.1.1 Setup\nKnowledge probing, or referred to as fact retrieval,\nevaluates how much real-world knowledge has\nlanguage models gained from pre-training. The\nLAMA (Petroni et al., 2019) dataset evaluates it\nwith cloze tests created from triples selected in the\nknowledge bases.\nDatasets and vocabulary. LAMA enforces allPrompt type Model P@1\nOriginal\n(MP)\nBERT-base 31.1\nBERT-large 32.3\nE-BERT 36.2\nDiscrete\nLPAQA (BERT-base) 34.1\nLPAQA (BERT-large) 39.4\nAutoPrompt (BERT-base) 43.3\nP-tuning BERT-base 48.3\nBERT-large 50.6\nModel MP P-tuning\nBERT-base (109M) 31.7 52.3 (+20.6)\n-AutoPrompt (Shin et al., 2020) - 45.2\nBERT-large (335M) 33.5 54.6 (+21.1)\nRoBERTa-base (125M) 18.4 49.3 (+30.9)\n-AutoPrompt (Shin et al., 2020) - 40.0\nRoBERTa-large (355M) 22.1 53.5 (+31.4)\nGPT2-medium (345M) 20.3 46.5 (+26.2)\nGPT2-xl (1.5B) 22.8 54.4 (+31.6)\nMegatronLM (11B) 23.1 64.2 (+41.1)\nTable 3: Knowledge probing Precision@1 on LAMA-34k (left) and LAMA-29k (right). P-tuning outperforms all\nthe discrete prompt searching baselines. (MP: Manual prompt; PT: P-tuning).\nanswers in single-token format. We first adopt\nthe original LAMA-TREx dataset, consisting of\n41 Wikidata relations and altogether 34,039 test-\ning triples (namely LAMA-34k, which covers all\nBERT vocabularies). Since different pretrained\nmodels share distinct vocabularies, to allow direct\ncomparison, we follow previous work (Shin et al.,\n2020) to adopt a subset that covers the intersection\nof GPT’s and BERT’s vocabularies. This is caled\nLAMA-29k. We again follow Shin et al. (2020) to\nconstruct the training, development, and test data\nto allow for fair comparison.\nSetup. LAMA has provided a handcraft prompt\nfor each relation, as shown in Table 1, which are\neffective but likely sub-optimal. For bidirectional\nmasked language models, we only need to replace\n“[X]” with the subject entity and “[Y]” with the\n[MASK] token; for unidirectional language models\nsuch as GPT, following LAMA’s original setting\non Transformer-XL (Dai et al., 2019), we use the\nnetwork output just before the target position.\nThe number of prompt tokens and positions are\nselected based on the development sets, and for\nsimplicity we choose the (3, sub, org_prompt, 3,\nobj, 3) template for bidirectional models and (3,\nsub, org_prompt, 3, obj) for unidirectional models\nas this configuration performs well for most rela-\ntions (where the number indicates the number of\ncontinuous prompt tokens). Continuous prompts\nare concatenated with original discrete prompts.\nDuring the prompt training, we set the learning rate\nto 1e-5 and use the Adam optimizer.\n3.1.2 Main results\nThe results are presented in Table 3. P-tuning sig-\nnificantly improves the best results of knowledge\nprobing from 43.3% to 50.6% on LAMA-34k and\nfrom 45.2% to 64.2% on LAMA-29k. Moreover,\nP-tuning outperforms previous discrete prompt\nsearching approaches such as AutoPrompt (Shin\net al., 2020) and LPAQA (Jiang et al., 2020b) on\nthe same-size models. This confirms our intuition\nin Section 2 that discrete prompts might not be\noptimal.\n3.2 Fully-supervised Learning\n3.2.1 Setup\nDataset. To evaluate P-tuning on fully-supervised\nlearning tasks, we adopt the SuperGLUE bench-\nmark (Wang et al., 2019b), consisting of 8 challeng-\ning natural language understanding (NLU) tasks.\nWe focus on 7 of them since the ReCoRD (Zhang\net al., 2018) task adopts no discrete prompts, thus\nP-tuning is not directly applicable. The tasks in-\nclude question answering (BoolQ (Clark et al.,\n2019a) & MultiRC (Khashabi et al., 2018)), tex-\ntual entailment (CB (De Marneffe et al., 2019) &\nRTE (Dagan et al., 2005)), co-reference resolution\n(WiC (Pilehvar and Camacho-Collados, 2018)),\ncausal reasoning (COPA (Roemmele et al., 2011)),\nand word sense disambiguation (WSC (Levesque\net al., 2012)).\nComparison methods. We experiment with P-\ntuning on both unidirectional and bidirectional\npretrained models, i.e., GPT and BERT. We\ninclude four variants BERT-Base, BERT-Large,\nGPT2-Base, and GPT-medium. For each model,\nwe compare standard classification finetuning,\nPET (Schick and Schütze, 2020) (a typical fine-\ntuning method based on manual discrete prompts)\nand our P-tuning.\nConfiguration. We use the same metrics as\nin (Wang et al., 2019b). For fully-supervised learn-\ning, we use a large training set to finetune pre-\ntrained models and use a development set for hyper-(a) Fully-supervised performance with base-scale models.\nMethod BoolQ CB WiC RTE MultiRC WSC COPA Avg.(Acc.) (Acc.) (F1) (Acc.) (Acc.) (EM) (F1a) (Acc.) (Acc.)\nBERT-Base\n(109M)\nCLS-FT 72.9 85.1 73.9 71.1 68.4 16.2 66.3 63.5 67.0 66.2\nPET-FT 73.7 87.5 90.8 67.9 70.4 13.7 62.5 60.6 70.0 67.1\nP-tuning 73.9 89.2 92.1 68.8 71.1 14.8 63.3 63.5 72.0 68.4\nGPT2-Base\n(117M)\nCLS-FT 71.2 78.6 55.8 65.5 67.8 17.4 65.8 63.0 64.4 63.0\nPET-FT 74.8 87.5 88.1 68.0 70.0 23.5 69.7 66.3 78.0 70.2\nP-tuning 75.0 91.1 93.2 68.3 70.8 23.5 69.8 63.5 76.0 70.4\n(b) Fully-supervised performance with large-scale models.\nMethod BoolQ CB WiC RTE MultiRC WSC COPA Avg.(Acc.) (Acc.) (F1) (Acc.) (Acc.) (EM) (F1a) (Acc.) (Acc.)\nBERT-Large\n(335M)\nCLS-FT1 77.7 94.6 93.7 74.9 75.8 24.7 70.5 68.3 69.0 72.5\nPET-FT 77.2 91.1 93.5 70.5 73.6 17.7 67.0 80.8 75.0 73.1\nP-tuning 77.8 96.4 97.4 72.7 75.5 17.1 65.6 81.7 76.0 74.6\nGPT2-Med.\n(345M)\nCLS-FT 71.0 73.2 51.2 65.2 72.2 19.2 65.8 62.5 66.0 63.1\nPET-FT 78.3 96.4 97.4 70.4 72.6 32.1 74.4 73.0 80.0 74.9\nP-tuning 78.9 98.2 98.7 69.4 75.5 29.3 74.2 74.0 81.0 75.6\n1 We report the same results taken from SuperGLUE (Wang et al., 2019a).\nTable 4: Fully-supervised performance on SuperGLUE development set.\nparameter and model selection. Specifically, the\nAdamW optimizer with a linearly decayed learn-\ning rate is used for training. We use a learning\nrate of {1e − 5, 2e − 5, 3e − 5}, a batch size of\n{16, 32}, and a warm-up ratio of {0.0, 0.05, 0.1}.\nFor small datasets (i.e., COPA, WSC, CB, RTE),\nwe fine-tune pretrained models for 20 epochs. For\nlarger datasets (i.e., WiC, BoolQ, MultiRC), we\nreduce the number of training epochs to be 10 as\nthe model converges earlier. Early stopping is used\nto avoid over-fitting the training data.\n3.2.2 Main Results\nThe main results of fully-supervised learning are\nshown in Table 4. We observe that P-tuning can\nimprove fully-supervised learning performance on\nboth BERTs and GPTs. (1) Specifically, on the\nBERT-Base model, P-tuning achieves best perfor-\nmance on 5/7 tasks, while with BERT-Large, P-\ntuning outperforms other methods on 4/7 tasks.\nThe exceptions are WiC and MultiRC, both of\nwhich have relatively large training sets. We find\nthat P-tuning might not have large gains over CLS-\nFT on such high-resource tasks, while benefits\nmore on low-resource tasks. On average, P-tuning\nimproves over the considered baselines. (2) On\nGPT2-Base and GPT2-Medium models, P-tuning\nconsistently achieves the best performance on all\ntasks.\n3.3 Few-Shot Learning\nWhile GPT-3 has shown decent few-shot learning\npotential with handcrafted prompts, it still struggles\non some of the challenging tasks (e.g., natural lan-\nguage inference) (Brown et al., 2020). We are mo-\ntivated to study whether P-tuning can also improve\nthe few-shot learning performance of pretrained\nmodels on challenging tasks.\n3.3.1 Setup\nFew-shot Evaluation. The few-shot performance\nis sensitive to lots of factors (e.g., the order of train-\ning examples, random seed, and prompt patterns),\nand thus suffers from high variance (Zhao et al.,\n2021a; Lu et al., 2021; Zhang et al., 2020). There-\nfore, the few-shot evaluation strategy should make\nsure that the improvements are indeed from an im-\nproved method instead of variance. To this end, we\nfollow the FewNLU evaluation procedure (Zheng\net al., 2021) that has addressed and handled the\nissue. Specifically, we use random data splits to\nperform model selection only on a small labeled\nset to prevent overfitting a large dev set.\nDataset. We use the few-shot SuperGLUE (also\nknown as FewGLUE) benchmark (Schick and\nSchütze, 2020) and follow the setting in prior work\n(Zheng et al., 2021) in terms of data split construc-\ntion.\nBaseline and Hyper-parameter.In few-shot learn-ing, we again compare P-tuning with PET (Schick\nand Schütze, 2020), which was shown to out-\nperform GPT-3 on some of the tasks. Similar\nto (Schick and Schütze, 2020), we use ALBERT-\nxxLarge as the base model. For hyper-parameters\nthat are shared by PET and P-tuning (e.g., learn-\ning rate, maximum training step, evaluation fre-\nquency), we use the same search space for fair\ncomparison. Specifically, we search the learning\nrate in {1e − 5, 2e − 5}, the maximum training\nstep in {250, 500}, and the evaluation frequency in\n{0.02, 0.04}.\nConstruction of Prompt Patterns. For PET, we\nuse the same manual prompts reported by Schick\nand Schütze (2020). When constructing prompt\npatterns for P-tuning, based on the same manual\nprompts as PET, we insert different numbers of\ncontinuous prompt tokens into different positions,\nthus formulating a number of pattern candidates.\nWe then select the best pattern for P-tuning using\nthe validation strategy of FewNLU (Zheng et al.,\n2021). We also conduct further analysis of the num-\nber and the position of continuous prompt tokens\nin §3.3.3.\n3.3.2 Main Results\nFew-Shot Performance. Table 5 shows the main\nresults of few-shot learning. We find that, on AL-\nBERT, P-tuning consistently outperform PET on\naverage by more than 1 points. It outperforms\nPromptTuning by more than 13 points. It proves\nthat by automatically learning continuous prompt\ntokens, the pretrained models can achieve better\nfew-shot performance on NLU tasks.\n3.3.3 Ablation Study\nType of Prompt Encoder Prior work (Shin et al.,\n2020) proposes to simply use an MLP as the prompt\nencoder, we perform further ablation analysis for\nprompt encoder selection, and results are shown\nin Table 8. We consider LSTM, MLP, and EMB\n(i.e., we directly optimize the word embeddings\nwithout using additional parameters). From the\nresults, we can see that LSTM, MLP, and EMB\nall work as a prompt encoder. Results show that\nboth LSTM and MLP generally work well on these\ntasks, while EMB is unstable and can substantially\nunder-perform the other two on some tasks (e.g,.\nWiC and CB). To sum up, both LSTM and MLP\ncould be taken into account when working on new\ntasks.\nLocation of Prompt Tokens To study at which\nlocation to insert continuous prompt tokens, we\nperform experiments as Table 7 shows. From the\nresults, we have the following findings.\n1. By comparing #1 (or #2) with #3 (or #4), we find\nthat it would be better if we insert continuous\nprompt tokens at the location where it does not\nsegment the sentences. For example, in case#1,\n“[P]” breaks the completeness of sentence “[Hy-\npothesis]?” while in case#3, “[P]” is located\nbetween sentences.\n2. By comparing #2 (or #3) with #4, we find that\nthere’s no special preference for placing on the\nedge or in the middle of the inputs.\n3. It is suggested to write a number of pattern can-\ndidates and then search over them for the best\nfor each task.\nNumber of Prompt Tokens We also study the in-\nfluence of the number of prompt tokens and show\nthe results in Table 7. By comparing #3, #6, #7,\nand #8, we can conclude that the number of prompt\ntokens has a great impact on the few-shot perfor-\nmance. However, it is not that a larger number of\nprompt tokens would always be better. We conjec-\nture that it could be that due to the limited training\ndata, it becomes difficult to learn the parameters\nwhen excessively increasing the number of contin-\nuous prompt tokens. In practice, it is suggested\nto search for the best number of prompt tokens\nthrough model selection.\n3.3.4 Comparison with Discrete Prompt\nSearch\nPrior work (Gao et al., 2020) proposed to automati-\ncally search discrete prompts and achieved better\nresults than those of manual prompts. We now\nproceed to compare P-Tuning with auto-searched\ndiscrete prompts. For fair comparison, we follow\nthe setting of LM-BFF (Gao et al., 2020) to also\nconduct experiments on some of the GLUE tasks\n(Wang et al., 2018) with RoBERTa-Large model\n(Liu et al., 2019). Since the the evaluation proto-\ncols have large impacts on few-shot performance,\nwe use the top-3 discrete prompts searched by LM-\nBFF and experiment with using only the discrete\nprompts and additionally applying P-Tuning. For\nP-Tuning, the prompt patterns are constructed by\nconcatenating the same discrete prompts as well as\ncontinuous prompts. Results in Table 9 show that\nadditionally incorporating continuous prompts can\nfurther improve few-shot performance. P-Tuning isMethod BoolQ\n(Acc.)\nRTE\n(Acc.)\nWiC\n(Acc.)\nCB\n(Acc.) (F1.)\nMultiRC\n(F1a.) (EM.)\nWSC\n(Acc.)\nCOPA\n(Acc.) Avg\nPrompt Tuning58.47±1.00 54.42±3.05 52.74±2.36 75.45±2.25 67.73±5.70 59.28±4.73 15.03±4.11 74.04±2.99 61.50±4.36 58.56\nPET-FT 76.70±1.85 72.83±1.30 53.87±4.47 84.38±4.47 62.56±7.66 76.51±1.52 36.46±2.13 80.05±2.53 81.75±4.03 70.74\nP-tuning 76.55±2.68 63.27±3.63 55.49±1.21 88.39±3.72 84.24±5.15 75.91±1.74 38.01±0.78 78.85±1.76 85.25±3.30 71.81\nTable 5: The few-shot performance of PET (Schick and Schütze, 2020), Prompt Tuning (Lester et al., 2021) and our\nP-tuning over seven tasks based on ALBERT. Each result is averaged over 4 runs with different data splits. Results\nshow that P-tuning consistently improves average few-shot performance by more than 1 point compared to PET and\nby more than 13 points compared to Prompt Tuning.\nMethod P#0 P#1 P#2 P#3 P#4 P#5 STD\nFSL\n(BoolQ)\nPET-FT 77.10 67.96 74.14 72.48 71.77 60.86 5.68±2.21 ±2.69 ±1.38 ±4.31 ±2.56 ±3.99\nP-tuning 75.41 75.11 73.43 71.35 71.31 65.86 3.52±3.09 ±1.61 ±2.60 ±4.57 ±8.58 ±3.80\nLAMA\n(P17)\nMP 31.3 19.8 31.4 51.1 34.0 32.7 10.1\nP-tuning 57.8 57.8 58.1 58.1 58.9 58.7 0.46\nTable 6: Upper table: Few-shot learning (FSL) of PET and P-tuning in terms of each pattern on SuperGLUE with\nALBERT; Lower table: Manual prompt (MP) and P-tuning performance on LAMA-P17 with BERT-base-cased.\nFor each column, P-tuning and compared methods share the same manual prompts, while P-tuning additionally\nconcatenates continuous prompt tokens. We report the standard deviation over multiple results of different patterns.\nResults show that P-tuning achieves smaller standard deviation, proving that P-tuning can improve stability w.r.t.\nthe choice of discrete patterns.\neasy to be combined with existing discrete prompts,\nwhile further improving stability as discussed in\nSection 3.4.\n3.4 Stabilizing Language Model Adaptation\nIn the above sections, we have shown that P-Tuning\nimproves over performance across multiple set-\ntings. Now we present results to demonstrate that\nP-Tuning also stabilizes language model adapta-\ntion; i.e., reducing the differences between differ-\nent prompts. As we have shown in Table 1, manual\nprompts have a large impact on the performance.\nWhen it comes to few-shot learning, the perfor-\nmance gap of different prompts is prominent due\nto the sensitivity of few-shot learning (Zheng et al.,\n2021). Results in Table 6 show that P-tuning im-\nproves the performance of the worst-performing\npatterns (e.g., P#5), and achieves a smaller stan-\ndard deviation over multiple patterns. Compared to\nPET-FT, P-tuning increases the stability w.r.t. the\nchoice of patterns.\nOn LAMA, we observe similar a phenomenon\nthat while manual prompts often yield quite volatile\nresults, appending trainable continuous prompts on\ntop of the manual prompts can stabilize their per-\nformances, reducing the standard deviation from\n10.1 to 0.46.\n4 Related work\nLanguage Model Prompting. GPT-3 (Brown\net al., 2020) uses in-context examples (Liu et al.,\n2021; Zhao et al., 2021b) as a way of prompting to\ntransfer knowledge from pretraining to downstream\ntasks. Schick and Schütze (2020) proposed to use\ncloze patterns, which removes the constraint that\nthe masked token is the last token of the sentence.\nThis further minimizes the gap between pretrain-\ning and downstream tasks. To improve prompting\nfor NLU, recent works have proposed methods to\nautomatically search for high-performing prompts\nby mining the training corpus (Jiang et al., 2020b),\ngradient-based search (Shin et al., 2020), or using\npretrained generative models (Gao et al., 2020).\nOur approach is different from these prior works\nin that we resort to using continuous prompt em-\nbeddings, which are found to be complementary to\ndiscrete prompts in our experiments.\nRecently, some concurrent works also proposed\nthe use of continuous prompts. Prefix-tuning (Li\nand Liang, 2021) adds continuous prompts at the\nbeginning of the sequence for each layer. In con-\ntrast to our work, prefix-tuning targets natural lan-\nguage generation tasks.\nIn the area of NLU, a few concurrent methods\nwere proposed based on continuous prompts, fo-ID Prompt Patterns of P-tuning Seg. Pos. #[P] Acc. F1. Avg.\n1 [Premise] Question: [Hypothesis] [P] ? Answer: [M]. Yes Mid 1 87.95 76.70 82.33\n2 [Premise] Question [P]: [Hypothesis] ? Answer: [M]. Yes Mid 1 88.39 78.57 83.48\n3 [Premise] Question: [Hypothesis] ? [P] Answer: [M]. No Mid 1 89.29 79.86 84.58\n4 [Premise] [P] Question: [Hypothesis] ? Answer: [M]. No Miid 1 89.73 82.15 85.94\n5 [Premise] Question: [Hypothesis] ? Answer: [M]. [P] No Edge 1 87.50 83.39 85.45\n6 [Premise] Question: [Hypothesis] ? [P][P] Answer: [M]. No Mid 2 88.39 84.74 86.57\n7 [Premise] Question: [Hypothesis] ? [P][P][P][P] Answer: [M].No Mid 4 88.39 85.14 86.76\n8 [Premise] Question: [Hypothesis] ? [P][P][P][P][P][P][P][P] Answer: [M].No Mid 8 83.48 73.32 78.40\nTable 7: The few-shot performance of P-tuning on the CB task on ALBERT with different prompt patterns. “Seg.”\nmeans whether the inserted prompt tokens segment complete sentences. “Pos.” indicates inserting the prompt tokens\nat the edge or in the middle of the inputs. “[P]” is continuous prompt token. “[M]” is the mask token.\nTask LSTM MLP EMB\nWiC-ACC 56.27 ±1.54 55.25 ±3.09 53.96 ±3.23\nCB-ACC. 81.70 ±7.49 88.39 ±3.72 82.59±3.69\nCB-F1. 77.41 ±9.15 84.24±5.15 67.27±6.78\nBoolQ-ACC. 75.41±3.09 76.46±2.84 76.87±1.69\nTable 8: The few-shot performance on WiC, CB and\nBoolQ tasks with ALBERT using different prompt en-\ncoders. Results show that both LSTM and MLP gener-\nally work well on these tasks, while EMB is unstable\nand can substantially under-perform the other two on\nsome tasks (e.g,. WiC and CB). “EMB” means using an\nidentity mapping for the prompt encoder.\nTask LM-BFF (Auto) P-Tuning\nSST-2 92.89 92.78\nMNLI 57.53 58.70\nMRPC 68.26 69.49\nTable 9: Few-shot performance of automatically\nsearched prompts and P-Tuning. We evaluated LM-\nBFF (Auto) using the reported top-3 searched patterns\nunder our evaluation procedure. P-Tuning also uses\nthe same discrete prompts, in concatenation with con-\ntinuous prompts. Results show that P-Tuning can be\neffectively combined with existing discrete patterns and\nachieve further performance improvement.\ncusing on improving knowledge probing (Qin and\nEisner, 2021; Zhong et al., 2021). Lester et al.\n(2021) showed that with large pretrained models,\nonly tuning continuous prompts with a frozen lan-\nguage model achieves comparable performance to\nfull-model tuning.\nCompared to these concurrent works on NLU,\nP-Tuning reaches a unique conclusion that contin-\nuous prompts improve performance and stabilize\ntraining with either frozen or tuned models under\nboth the few-shot and fully-supervised settings. For\nexample, no concurrent works have shown that\ncontinuous prompts can improve performance with\na tuned language model. Technically, P-Tuning\nalso has a few unique designs such as using hy-\nbrid continuous-discrete prompts and employing a\nprompt encoder.\nKnowledge in Language Models. Self-\nsupervised (Liu et al., 2020) pre-trained language\nmodels (Han et al., 2021) including GPT (Rad-\nford et al., 2019), BERT (Devlin et al., 2018), XL-\nNet (Yang et al., 2019), RoBERTa (Liu et al., 2019)\nhave been observed to learn not only contextual-\nized text representations but also linguistic and\nworld knowledge. (Hewitt and Manning, 2019)\ndemonstrates that contextualized representations\nproduced by language models can form a parse tree\nin the embedding space. (Vig, 2019; Clark et al.,\n2019b) look into the multi-head attention patterns\nwithin transformers and discover that certain atten-\ntion heads may correspond to some grammatical\nfunctions, including co-reference and noun modi-\nfiers. LAMA (Petroni et al., 2019, 2020) propose\nthe LAMA task that leverages cloze tests to pre-\ndict the fact triples of knowledge bases to examine\nlanguage model’s ability of memorizing facts with\nanswers in the single-token format. In (Wang et al.,\n2020), the authors investigate the attention matrices\nto find evidence about knowledge triples contained\nin the context. (Jiang et al., 2020a) develops a\nmulti-token fact retrieval dataset based on LAMA.\n5 Conclusions\nIn this paper, we present a method P-Tuning that\nuses continuous prompts in concatenation with dis-\ncrete prompts. P-Tuning improves performance\nand stabilizes training for pretrained language\nmodel adaptation. P-Tuning is effective with both\ntuned and frozen language models under both the\nfew-shot and fully-supervised setings.References\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019a. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924–2936.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019b. What does bert look\nat? an analysis of bert’s attention. arXiv preprint\narXiv:1906.04341.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges Workshop,\npages 177–190. Springer.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung, volume 23,\npages 107–124.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao\nLiu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao\nHan, Minlie Huang, et al. 2021. Pre-trained models:\nPast, present and future. AI Open.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word representa-\ntions. In North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL). Association for Computa-\ntional Linguistics.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a. X-factr:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5943–5959.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. ArXiv, abs/2104.08691.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth International Conference on the Principles of\nKnowledge Representation and Reasoning. Citeseer.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nXiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang,\nLi Mian, Jing Zhang, and Jie Tang. 2020. Self-\nsupervised learning: Generative or contrastive. arXiv\npreprint arXiv:2006.08218, 1(2).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2021. Fantastically\nordered prompts and where to find them: Over-\ncoming few-shot prompt order sensitivity. CoRR,\nabs/2104.08786.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRocktäschel, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. arXiv preprint\narXiv:2005.04611.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? arXiv preprint arXiv:1909.01066.\nMohammad Taher Pilehvar and José Camacho-Collados.\n2018. Wic: 10, 000 example pairs for eval-\nuating context-sensitive representations. CoRR,\nabs/1808.09121.Guanghui Qin and J. Eisner. 2021. Learning how to ask:\nQuerying lms with mixtures of soft prompts. ArXiv,\nabs/2104.06599.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In AAAI Spring Symposium: Logical Formal-\nizations of Commonsense Reasoning, pages 90–95.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. Computing Research Repository,\narXiv:2009.07118.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with\nautomatically generated prompts. arXiv preprint\narXiv:2010.15980.\nJesse Vig. 2019. A multiscale visualization of at-\ntention in the transformer model. arXiv preprint\narXiv:1906.05714.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R Bowman. 2019a. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. arXiv preprint arXiv:1905.00537.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019b. SuperGLUE: A\nStickier Benchmark for General-Purpose Language\nUnderstanding Systems. In NeurIPS 2019, pages\n3261–3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding. ArXiv,\nabs/1804.07461.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage models are open knowledge graphs. arXiv\npreprint arXiv:2010.11967.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. arXiv\npreprint arXiv:1810.12885.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Wein-\nberger, and Yoav Artzi. 2020. Revisiting few-sample\nBERT fine-tuning. CoRR, abs/2006.05987.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021a. Calibrate before use: Im-\nproving few-shot performance of language models.\nCoRR, abs/2102.09690.\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021b. Calibrate before use: Improv-\ning few-shot performance of language models. arXiv\npreprint arXiv:2102.09690.\nYanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Jian\nLi, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder,\nand Zhilin Yang. 2021. Fewnlu: Benchmarking state-\nof-the-art methods for few-shot natural language un-\nderstanding.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [mask]: Learning vs. learning to\nrecall. ArXiv, abs/2104.05240."
    }
}