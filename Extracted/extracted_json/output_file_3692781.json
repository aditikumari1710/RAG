{
    "title": "Journal of Machine Learning Research () Submitted ; Published",
    "content": {
        "page_content": "Journal of Machine Learning Research () Submitted ; Published\nOptimization with Non-Differentiable Constraints with Applications\nto Fairness, Recall, Churn, and Other Goals\nAndrew Cotter ACOTTER @GOOGLE .COM\nHeinrich Jiang HEINRICHJ @GOOGLE .COM\nSerena Wang SERENAWANG @GOOGLE .COM\nTaman Narayan TAMANN @GOOGLE .COM\nMaya Gupta MAYAGUPTA @GOOGLE .COM\nGoogle AI\n1600 Amphitheatre Pkwy\nMountain View, CA, USA\nSeungil You SEUNGIL .YOU @GMAIL .COM\nKakao Mobility\nSeongnam-si, Gyeonggi-do, South Korea\nKarthik Sridharan SRIDHARAN @CS.CORNELL .EDU\nCornell University\nIthaca, NY, USA\nEditor:\nAbstract\nWe show that many machine learning goals, such as improved fairness metrics, can be expressed as\nconstraints on the model’s predictions, which we call rate constraints. We study the problem of train-\ning non-convex models subject to these rate constraints (or any non-convex and non-differentiable\nconstraints). In the non-convex setting, the standard approach of Lagrange multipliers may fail.\nFurthermore, if the constraints are non-differentiable, then one cannot optimize the Lagrangian with\ngradient-based methods. To solve these issues, we introduce the proxy-Lagrangian formulation. This\nnew formulation leads to an algorithm that produces a stochastic classiﬁer by playing a two-player\nnon-zero-sum game solving for what we call a semi-coarse correlated equilibrium, which in turn\ncorresponds to an approximately optimal and feasible solution to the constrained optimization\nproblem. We then give a procedure which shrinks the randomized solution down to one that is a\nmixture of at most m+ 1deterministic solutions, given mconstraints. This culminates in algorithms\nthat can solve non-convex constrained optimization problems with possibly non-differentiable and\nnon-convex constraints with theoretical guarantees. We provide extensive experimental results\nenforcing a wide range of policy goals including different fairness metrics, and other goals on\naccuracy, coverage, recall, and churn.\nKeywords: constrained optimization, non-convex, fairness, swap regret, non-zero-sum game\n1. Introduction\nWe seek to provide better ways to control machine learning to meet societal, legal, and practical\ngoals, and to take advantage of different kinds of side information and intuition that practitioners may\nhave about their machine learning problem. In this paper, we show that many real-world goals and\nc⃝.\narXiv:1809.04198v1  [cs.LG]  11 Sep 2018COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nside information can be expressed as constraints on the model’s prediction rates on different datasets,\nwhich we refer to as rate constraints, turning training into a constrained optimization problem. A\nsimple example of a rate constraint is requiring that a binary classiﬁer predict 80% of examples to be\nthe positive class, leading to the constrained optimization problem:\nmin\nθ\n1\nN\nN∑\nj=1\nℓ(f(xj; θ),yj) + R(θ) (1)\ns.t. 1\nN\nN∑\nj=1\nIf(xj;θ)≥0 ≥0.8,\nwhere {(xj,yj)}is a training set for j = 1,...,N , ℓis the loss, Ris a regularizer, and Iis the usual\nindicator.\n1.1 The Broad Applicability of Rate Constraints\nOne can express a surprisingly large set of real-world goals using rate constraints. Here we preview\nsome categories of goals, with more details in Section 3.\nFairness: Many fairness goals can be expressed as rate constraints, including the popular fairness\ngoal of statistical parity. For example, one can constrain a classiﬁer’s positive prediction rate for\nmen to be within ten percent of its rate for women. Other fairness goals that can be expressed as rate\nconstraints are equal opportunity and equal odds (Hardt et al., 2016), as well as fairness goals we\nencounter in real-world problems but have not previously seen in the machine learning literature,\nsuch as no worse off.\nPerformance Measures: Some metrics can be expressed as rate constraints, though approxi-\nmations may be needed. For example, one can lower-bound the accuracy on particular slices of\nthe data, or the recall. Precision and win-loss ratio (WLR) compared to a baseline classiﬁer can be\nexpressed with rate constraints, however, there are some caveats about how satisfying constraints on\nthese metrics will generalize to test samples (details below). AUC can be approximated as a set of\nrate constraints, using the approximation proposed in Eban et al. (2017).\nChurn: Churn is the probability that each sample’s classiﬁer decision will change if the new\nclassiﬁer is deployed, compared to the decision of the classiﬁer it is replacing. Reducing classiﬁer\nchurn (Cormier et al., 2016; Goh et al., 2016) is important in many practical machine learning\nsystems to improve overall system stability and to make changes easier to measure and test. Churn\ncan be expressed with rate constraints, thus one can constrain the churn of a new classiﬁer to some\ndesired level.\nMultiple Training Datasets: Another application is when one has multiple training sets of\nvarying quality. For example, one might have a small set of expertly rated training data, but also a\nlarge set of noisy training data one would also like to use. One can train using rate constraints to\nrequire the classiﬁer to achieve a certain accuracy on the small expertly rated data, while training to\nminimize errors on large noisy data.\nUnlabeled Datasets: Many of the rate constraints we discuss do not require labels, such as the\nfairness goal of statistical parity, coverage, or churn. This enables one to take advantage of large\nunlabeled datasets, which are cheaper to obtain than labeled data.\n2OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\n1.2 Why Constrain? Why Not Penalize?\nFor many of these problems, one could instead train the model with an additive penalty, rather than a\nconstraint as in (Equation 1), to encourage the desired behavior. Given multiple goals, one could\nuse a linear combination of multiple penalties. However, that approach requires the practitioner\nto determine the right weight for each penalty; and those weights may interact with each other in\nunexpected ways, particularly if the different goals are deﬁned on different datasets.\nWe have found that specifying a desired constraint for each goal is in practice cleaner and easier\nfor practitioners. The key reason is that a constraint has absolute meaning, making it possible for a\npractitioner to specify their goal as a constraint without regard for the presence of other constraints.\nFor example, the meaning of requiring a classiﬁer to have 80% recall in India does not depend on\nwhether there is one or one hundred other such locale-speciﬁc constraints on the classiﬁer. We also\nfound that using hard constraints leads to a more understandable machine learning model because\nit is clearer what the model was trained to do, and it is clearer to measure and verify whether the\ntraining sufﬁciently achieved the practitioner’s intent for each individual goal.\n1.3 Training with Constraints:\nTraining with rate constraints poses some hard challenges:\n1. Non-convex: For nonlinear function classes, such as neural networks, the objective and\nconstraint functions will be non-convex.\n2. Non-differentiable: Rate constraints are linear combinations of positive and negative classiﬁ-\ncation rates, that is, they are made up of indicator functions (0-1 losses) which are not even\nsemi-differentiable (as in the example of Equation 1).\n3. Data-dependent: The constraints are data-dependent, and for large datasets may be expensive\nto compute.\nWhile our motivating optimization problem is training with rate constraints, the analysis and\nalgorithms we present will apply generally to constrained optimization problems of the form:\nmin\nθ∈Θ\ng0(θ) (2)\ns.t.gi(θ) ≤0 for i= 1,...,m,\nwhere g0 and gi may be non-convex, and gi may additionally be non-differentiable and expensive to\nevaluate.\n1.4 The Lagrangian Is Not Optimal for Non-Convex Problems\nA popular approach to constrained optimization problems of the form (Equation 2) is the method of\nLagrange multipliers, using the Lagrangian deﬁned as follows:\nL(θ,λ)\n△\n= g0(θ) +\nm∑\ni=1\nλigi(θ), (3)\nwhere λis an (m+ 1)-dimensional non-negative vector of Lagrange multipliers. The method of\nLagrange multipliers can be viewed as a two-player zero-sum game where one player minimizes\n(Equation 3) with respect to the model parameters θ, and the other player maximizes (Equation 3)\nwith respect to the Lagrange multipliers λ. If the objective and constraints are all convex in θ, this\n3COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nbecomes a convex game, and under general conditions, has a pure Nash equilibrium (von Neumann,\n1928) such that there exists a θfor the ﬁrst player and a vector of Lagrange multipliers λfor the\nsecond player such that neither player has the incentive to change their action given the other player’s\nchoice. In other words, there is no duality gap between the primal (min-max) and dual (max-min)\nproblems in the convex case and thus, solving for a pure Nash equilibrium (equivalently ﬁnding a\nsaddle point in the Lagrangian) gives us an optimal and feasible solution to the original constrained\noptimization problem (Equation 2).\nWhile applying convex solvers to non-convex problems has proved reasonable for many methods\nsuch as stochastic gradient descent (SGD), constrained optimization with Lagrange multipliers may\nnot even have a stationary point, and thus may never converge, instead oscillating between different\nsolutions. Moreover, because of the Lagrange duality gap which exists in non-convex problems, the\nsaddle points found will not in general correspond to the desired solution. In game theory parlance,\nwe show that for the non-convex setting of interest, a pure Nash equilibrium does not exist in general\n(an example can be found in the Appendix).\nHowever, if we allow each player to choose a distribution of solutions (that is, a stochastic\nsolution) instead of a single solution over their respective spaces Θ and Λ, and take the value of the\nLagrangian to be the expected value over these distributions, then an equilibrium will exist.\nIn this paper, we provide algorithms that approximately ﬁnd such stochastic solution equilibria,\nand we show that these correspond to nearly-feasible and nearly-optimal stochastic solutions to the\noriginal constrained optimization problem (Equation 2). The stochastic solution gives a randomized\nmodel: to classify an example x, we sample a θ∗from the distribution over Θ, and our guarantees\nwill be in terms of expectations with respect to this randomized model and the samples.\n1.5 The Lagrangian Is Impractical for Non-differentiable Constraints\nGiven non-differentiable constraints (such as rate constraints), a major shortcoming of the Lagrangian\nis that one cannot use gradient-based methods to optimize it. One approach is to use the Lagrangian\nbut replace non-differentiable constraints with relaxed versions which are differentiable (e.g. Daven-\nport et al., 2010; Gasso et al., 2011; Eban et al., 2017). However optimizing with the relaxed versions\nmay lead to solutions which either over-constrain or fail to satisfy the original constraints.\nWe introduce what we call the proxy-Lagrangian, where the key idea is to only relax the non-\ndifferentiable constraints when necessary. Solving the proxy-Lagrangian poses a considerable amount\nof technical challenges but leads to a number of interesting insights, and we provide algorithms which\nattain solutions with precise optimality and feasibility guarantees on the original non-differentiable\nconstraints.\nOverall, our paper gives an end-to-end recipe to provably (with access to an optimization oracle)\nand efﬁciently solve non-convex optimization problems with possibly non-differentiable constraints,\nand whose ﬁnal solution is a mixture of at most m+ 1 deterministic solutions. In practice, we use\nSGD in place of the oracle. To our knowledge, this is the ﬁrst time such a procedure has been found\nto provably solve such non-convex problems with such irregular constraints and return a sparse\nsolution.\nIn addition, for those practical situations where a stochastic model is unappealing, we will also\nexperimentally consider algorithms that do produce deterministic models, though they do not come\nwith guarantees.\n4OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\n1.6 Main Contributions and Organization\nThe main contributions of this paper are:\n•We show many real-world goals can be addressed and prior knowledge captured by training\nwith rate constraints.\n•We give a new proxy-Lagrangian formulation for optimizing non-convex objectives with\nnon-differentiable constraints.\n•We provide an algorithm that outputs a m+ 1 sparse stochastic classiﬁer with theoretical\nguarantees, where mis the number of constraints.\n•We show that our proxy-Lagrangian formulation can also be used to produce a deterministic\nclassiﬁer that may be more practical for some applications, but without guarantees.\n•We experimentally demonstrate that the proposed optimization can be used to train classiﬁers\nwith rate constraints, on both benchmark datasets and for real-world case studies.\nAlthough our motivation and experimental focus is on the problem of training classiﬁers with\nrate constraints, our proposed proxy-Lagrangian formulation and theoretical results have broader\napplication to other constrained optimization problems.\nWe next review related work. Then in Section 3 we detail many different goals that can be\nexpressed with rate constraints. We then turn to the question of how to actually optimize with\nconstraints, proposing new algorithms and theoretical results in Section 4. Section 5 presents a\ndiverse set of experiments on benchmark and real datasets to illustrate the applicability of rate\nconstraints and the proposed optimization. We close with a discussion of conclusions in Section 6\nand open questions in Section 7.\n2. Related Work\nWe begin by reviewing our own prior work which this paper builds upon, then other work that\nconsiders speciﬁc rate constraints, and then related work in constrained optimization.\n2.1 Our Prior Related Work\nIn Goh et al. (2016), we showed that some fairness goals, low-churn re-training, and recall lower\nbounds can all be optimized for by expressing these goals as constraints on the classiﬁer’s decisions\non targeted datasets, and training the classiﬁer to respect these constraints as part of the empirical\nrisk minimization. We referred to these constraints as dataset constraints, but we now use the more\nprecise term rate constraints to reﬂect our focus on the class of constraints that can be written in\nterms of the classiﬁer’s positive and negative decision rates. In this paper, we broaden the set of goals\nthat can be expressed as rate constraints, and provide more insight into how to use rate constraints in\npractice.\nIn Goh et al. (2016), the proposed constrained optimization algorithm was limited to linear\nclassiﬁers. Our proposed method used a new cutting-plane algorithm to iteratively upper-bound\nthe ramp loss with a convex loss, then solve the resulting inner-loop minimizations using an SVM\nsolver. While amenable to theoretical analysis, this strategy is a bit slow and difﬁcult to scale to more\nthan a handful of constraints. In contrast, in this paper we show we can effectively and efﬁciently\ntrain nonlinear classiﬁers with rate constraints using the more popular and scalable approach of\nstochastic gradients. Overall, compared to our prior work in Goh et al. (2016), we provide many new\nalgorithmic, theoretical, and experimental contributions.\n5COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nOur strategy presented here for treating non-differentiable problems as a non-zero sum two-player\ngame using a proxy Lagrangian formulation can be found on arXiv (Cotter et al. (2018b)), and will\nbe separately submitted to the ALT 2019 conference. This journal paper differs by being more\ncomprehensive and complete, with more discussion of how a broad set of goals can be expressed as\nrate constraints and much more comprehensive experiments.\nA key issue raised in this paper is how well satisfying rate constraints on a training set generalizes\nto new data drawn from the same distribution (e.g. on a testing set, or at evaluation time). We build\non the work presented in this paper to address this generalization question in a separate paper (Cotter\net al., 2018a).\n2.2 Other Related Work in Training with Rate Constraints\nA number of special cases of rate constraints have been previously considered in the literature.\nMann and McCallum (2007) and follow-on work (Bellare et al., 2009; Mann and McCallum,\n2010) optimized probabilistic models with constraints, to incorporate side information about the\nprior priors on class labels (which we call coverage constraints). They note their strategy could also\nbe applied to any constraints that can be written as an expectation over a score on the random (X,Y )\nsamples. They incorporated this side information as an additive regularizer and penalized the relative\nentropy between the given priors and estimated multi-class logistic regression models (Mann and\nMcCallum, 2007). They noted their approximation for the indicator could lead to degenerate solutions\n(see their paper for details and how they addressed the problem with additional regularization).\nNeyman-Pearson classiﬁcation trains with a constraint on the false positive rate (Scott and\nNowak, 2005), and a number of researchers have investigated this special case. Eban et al. (2017)\noptimized the model parameters and Lagrangian multiplier using stochastic gradients with a hinge\napproximation for the indicators in the empirical loss and constraints, and took the last training\niterate as their solution. We compare to that optimization strategy in our experiments (listed as Hinge\nLast in the result tables). Similarly, Davenport et al. (2010) optimized Neyman-Pearson support\nvector machines with hinge loss relaxations, using coordinate descent. Gasso et al. (2011) relaxed\nthe indicators to the ramp loss (both in the objective and constraints).\nAgarwal et al. (2018) recently addressed training classiﬁers with fairness constraints. Like this\nwork, their proposed algorithm is based on the two-player game perspective. Unlike this paper, they\nassume a zero-sum game, which works because they also assume oracle solvers for the two players,\nside-stepping the practical issues of dealing with the non-differentible non-convex indicators in the\nconstraints, which is the focus of our algorithmic and theoretical contributions. Similar to this work,\nthey output a stochastic classiﬁer, but do not provide the sparse m+ 1solution that we present in this\nwork. They also consider a deterministic solution, which they produce by searching over a grid of\nvalues for λfor the best λ. They noted in their experimental section that the resulting deterministic\nsolution was generally as good as their stochastic solutions on test data for those experiments they\ntried it on. As they note, a grid-search over λis less ideal as the number of constraints grows.\nSome other work in training fairer classiﬁers has used weaker constraints or relaxed them\nimmediately to weaker constraints such as correlation, e.g. Zafar et al. (2015). Another set of work\nin fair classiﬁcation only corrects a model post-training by optimizing additive group-speciﬁc bias\nparameters, e.g. Hardt et al. (2016) and Woodworth et al. (2017). Donini et al. (2018) studies\noptimization of fairness constraints for kernel methods by formulating the fairness constraints as\northogonality constraints.\n6OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\n2.3 Other Types of Constraints On Machine Learned Models\nWe focus on rate constraints in this paper, which differ from some other constrained machine learning\ntasks in that: (i) because they depend on f(x), they generally will depend on every parameter in the\nmodel, (ii) thus are usually relatively expensive to compute, and (iii) we do not generally expect to\nhave a very large number of constraints (for practical problems generally 2-1000).\nThese qualities are different than the popular constrained machine learning problem of shape\nconstraints, which may entail adding constraints to training to make the model monotonic (see e.g.\n(Barlow et al., 1972; Groeneboom and Jongbloed, 2014; Gupta et al., 2016; Canini et al., 2016; Luss\nand Rosset, 2017; You et al., 2017; Bonakdarpour et al., 2018) and/or convex/concave (e.g. Pya\nand Wood, 2015; Chen and Samworth, 2016; Gupta et al., 2018). Shape constraints generally entail\nadding many sparse cheap-to-evaluate constraints. For example, for isotonic regression on N sample\npoints, there are O(N) constraints, and each is a function of only two model parameters (Barlow\net al., 1972). As another examples, for diminishing returns constraints on ensembles of lattices, there\nare O(K) constraints where Kis the number of model parameters, but each constraint is a function\nof only three model parameters (Gupta et al., 2018). Problems with many cheap sparse constraints\ncan be well-handled by stochastic sampling of the constraints, as in (Cotter et al., 2016), unlike for\nrate constraints.\nAnother type of constrained machine learning aims to constrain the model parameters to obey\nother properties, such as physical limits on the learned system (see e.g. Long et al. (2018); Stewart\nand Ermon (2017)). These constraints generally do not take the form of rate constraints, but such\nconstrained machine learning models may also beneﬁt form the presented algorithms and theory.\n2.4 Related Work in Constrained Optimization as a Two Player Game\nOur constrained optimization algorithms and analyses build on the long history of treating constrained\noptimization as a two-player game: Arora et al. (2012) surveys some such work, and there are several\nmore recent examples, e.g. (Agarwal et al., 2018; Kearns et al., 2017; Narasimhan, 2018).\nIn this paper we extend prior work in treating constrained optimization as a two-player game in\nthree key ways. First, we introduce a shrinking procedure that signiﬁcantly simpliﬁes a “T-stochastic”\nsolution (i.e. stochastic classiﬁers supported on all T of the iterates) to a sparse “ m-stochastic”\nsolution (stochastic classiﬁers supported on onlym+1 iterates, where mis the number of constraints).\nSecondly, to handle non-differentiable constraints, we propose a new proxy-Lagrangiannon-zero-sum\nformulation, whereas prior work formulates the optimization as a zero-sum game. Third, we consider\na broader set of problems than some of this prior work. For example, Agarwal et al. (2018) propose a\nLagrangian-based approach that is very similar we outline in Section 4.1, but only tackle fairness\nconstraints. Here we can handle any problem of the form of Equation 2.\nFor example, our contributions also apply to robust optimization problems of the form:\nmin\nθ∈Θ\nmax\ni∈[m]\ngi(θ)\nThe recent work of Chen et al. (2017) addresses non-convex robust optimization. Like both us\nand Agarwal et al. (2018), they (i) model such a problem as a two-player game where one player\nchooses a mixture of objective functions, and the other player minimizes the loss of the mixture,\nand (ii) they ﬁnd a distribution over solutions rather than a pure equilibrium. These similarities\nare unsurprising in light of the fact that robust optimization can be reformulated as constrained\n7COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\noptimization via the introduction of a slack variable:\nmin\nθ∈Θ,ξ∈Ξ\nξ (4)\ns.t.∀i∈[m] .ξ≥gi(θ)\nCorrespondingly, one can transform a robust problem to a constrained one at the cost of an extra\nbisection search (e.g. Christiano et al., 2011; Rakhlin and Sridharan, 2013). As this relationship\nsuggests, our main contributions can be adapted to the robust optimization setting. In particular: (i)\nour proposed shrinking procedure can be applied to Equation 4 to yield a distribution over onlym+ 1\nsolutions, and (ii) one could perform robust optimization over non-differentiable (even discontinuous)\nlosses using “proxy objectives,” just as we use proxy constraints.\n2.5 Other Strategies for Constrained Optimization\nThere are other strategies for constrained optimization, each of which we argue is not well-suited to\nthe problem of training classiﬁers with rate constraints.\nThe complexity of the constraints makes it unattractive to use approaches that require projections,\nsuch as projected SGD, or optimization of constrained subproblems, such as Frank-Wolfe (Hazan\nand Kale, 2012; Jaggi, 2013; Garber and Hazan, 2013)).\nAnother strategy for constrained optimization is to penalize violations (e.g. Arora et al., 2012;\nRakhlin and Sridharan, 2013; Mahdavi et al., 2012; Cotter et al., 2016; Yang et al., 2017), for\nexample by adding γmaxi∈[m] max {0,gi(θ)}to the objective, where γ ∈R+ is a hyperparameter,\nand optimizing the resulting problem using a ﬁrst order method. This strategy is a poor match to rate\nconstraints for two reasons. First, if the constraint functions are non-(semi)differentiable, as in the\nindicators used in rate constraints. Second, each rate constraint is data-dependent, so evaluating gi,\nor even determining whether it is positive (as is necessary for such methods, due to the max with 0),\nrequires enumerating over the entire constraint dataset, making this incompatible with the use with a\ncomputationally-cheap stochastic gradient optimizer.\n3. How To Use Rate Constraints\nIn Goh et al. (2016), we showed that a number of useful machine learning goals can be expressed as\nconstraints on the classiﬁer’s mean decisions on different datasets, including recall, coverage (the\npositive or negative classiﬁcation proportion), churn, and some different fairness metrics. In this\nsection, we ﬁrst detail the mathematical formulation of rate constraints and the resulting constrained\nempirical risk minimization training. Then, we provide a list of metrics that can be expressed as rate\nconstraints in Table 1, and detail in the following subsections how these rate constraints can be used\nto impose a broad set of policy goals and take advantage of side information.\nGiven a model f(x) ∈R, a dataset D, and using I to denote the usual indicator, deﬁne the\nclassiﬁer’s positive and negative classiﬁcation rates onDas p+ (D) and p−(D), where,\np+(D)\n△\n= 1\n|D|\n∑\nx∈D\nIf(x;θ)≥0 and p−(D)\n△\n= 1\n|D|\n∑\nx∈D\nIf(x;θ)<0. (5)\nA constraint that can be expressed in terms of a non-negative linear combination of positive\nclassiﬁcation rates p+ (Dk) and negative classiﬁcation rates p−(Dk) over different datasets {Dk}\n8OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nwe call a rate constraint:\nK∑\nk=1\nαkp+ (Dk) + βkp−(Dk) ≤κ. (6)\nTable 1 shows how different choices of scalarsαk,βk,κ ∈R and datasets {Dk}correspond to\ndifferent standard performance metrics like accuracy and recall. One can add mrate constraints to\nthe standard structural risk minimization to train a classiﬁer with parameters θon train dataset D0,\nproducing the constrained empirical risk minimization :\nmin\nθ\n1\n|D0|\n∑\n(x,y)∈D0\nℓ(f(x; θ),y) + R(θ) (7)\ns.t.\nKi∑\nk=1\nαikp+ (Dik) + βikp−(Dik) ≤κi for i= 1,...,m,\nwhere αik,βik ∈R, Dik is the kth dataset for the ith constraint, Ki is the number of datasets used to\nspecify the ith constraint, and κi ∈R.\nThroughout this work, we focus on inequality constraints, for lower-bounding or upper-bounding\nsome rate. Equality constraints can be imposed by using both a lower-bound and upper-bound\ninequality constraint, though we suggest doing so with some margin between the lower and upper\nbound to make the optimization problem easier.\nFor some applications it is notationally more convenient to drop the normalization: let c+ (D)\nand c−(D) denote the count of the positive and negative classiﬁcations:\nc+(D)\n△\n=\n∑\nx∈D\nIf(x;θ)≥0 and c−(D)\n△\n=\n∑\nx∈D\nIf(x;θ)<0. (8)\nIn the rest of this section we show how different rate constraints can be used to impose various\npolicy goals or capture side information. A key insight is that one can add constraints just on speciﬁc\ngroups or subsets of the dataset by the choice of the constraint datasets {Dk}, which makes this\napproach particularly useful for fairness goals or other slice-speciﬁc metrics that are measured in\nterms of statistics on different datasets (see Table 2 and further details below).\n3.1 Coverage Constraints\nCoverage is the proportion of classiﬁcations that are positive: p+(D) (a variant is negative coverage\np−(D).). For example, if a company wants to train a classiﬁer to identify promising repeat customers,\nand knows it will use the classiﬁer to positively predict 10% of all customers to receive a printed\ncatalog, then one could train the classiﬁer with a 10% coverage constraint.\nCoverage constraints can also be used to capture prior knowledge in the training. For example, if\ntraining a model to classify Americans’ sex as male or female, one can regularize the classiﬁer by\nincorporating the prior knowledge that 51% of examples should be predicted to be women, by using\na 51% coverage constraint on the full dataset.\nUsing slice-speciﬁc coverage constraints can capture more side information. For example, for the\nAmerican male/female classiﬁer, in addition to the overall coverage constraint of51%, one could also\nadd constraints capturing prior information about state sex distributions, such as constraining 51.5%\n9COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nTable 1: Examples of Metrics Expressed with Rates\nD Set of examples\nD[∗] Subset of Dthat satisﬁes expression *, e.g. D[x∈male] is\nthe subset of Dof male examples, D[y= 1] is the subset\nof Dwhose label is 1, etc.\np+(D) ∈[0,1] Proportion of Dclassiﬁed positive\np−(D) ∈[0,1] Proportion of Dclassiﬁed negative\nc+(D) ∈N Count of Dclassiﬁed positive: c+(D) = |D|p+(D)\nc−(D) ∈N Count of Dclassiﬁed negative: c−(D) = |D|p−(D)\nCoverage p+(D)\nTrue Positives (TP) c+(D[y= 1])\nFalse Positives (FP) c+(D[y= −1])\nTrue Negatives (TN) c−(D[y= −1])\nFalse Negatives (FN) c−(D[y= 1])\nRecall p+(D[y= 1])\nPrecision c+(D[y= 1])/c+(D)\nAccuracy (c+(D[y= 1]) + c−(D[y= −1])/|D|\nh A ﬁxed classiﬁer taken as given\nAUCROC limL,J→∞ 1\nL\n∑L\nℓ=1 maxj∈[J]:p+\nαj(D[y=−1])≤ℓ\nL\np+\nαj(D[y= 1])\nWins Compared to h c +(D[h= −1,y = 1]) + c−(D[h= 1,y = −1]\nLosses Compared to h c +(D[h= −1,y = −1]) + c−(D[h= 1,y = 1]\nWin Loss Ratio (WLR) Wins Compared to h/ Losses Compared to h\nChurn (c+(D[h= −1] + c−(D[h= 1]))/|D|\nLoss-only Churn (c+(D[h= −1,y = −1]) + c−(D[h= 1,y = 1])/|D[h= y]|\nof examples from New York to be classiﬁed as women, but constraining only 47.6% of examples\nfrom Alaska to be classiﬁed as women.\nA key advantage of coverage constraints is that they do not require labeled examples. This enables\none to train on labeled training examples from a convenient distribution (such as actively-sampled\nexamples), but add a coverage constraint to ensure the classiﬁer is optimized to positively classify\nthe desired proportion of positive classiﬁcations on a larger unlabeled dataset drawn i.i.d. from the\ntrue underlying distribution. This usage of a coverage constraint forms a simple semi-supervised\nregularization of the classiﬁer.\nAnother good use case for coverage constraints is to help make a controlled comparison of two\nmodel structures. For example, suppose one has a model type A (e.g. a kernel SVM), and wonders if\nan alternative B (say, a DNN) is better, where A makes positive predictions on40% of test examples,\nwhile B appears to be more accurate, but only predicts the positive class for 35% of test examples. If\nprecision errors are worse than recall errors, we cannot be sure that B is better than A. We can try to\nquantify the misclassiﬁcation costs of a false negative vs. a false positive, but that may be difﬁcult\nto agree upon. It would be simpler to compare B to A at the same coverage as A, or at some other\nrelevant coverage. Coverage-matching B to A can be done by tuning the decision threshold of B\n10OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\npost-training, but including the coverage constraint in the training can help B learn to be a better\nclassiﬁer when tested at the desired coverage.\n3.2 Constraints on Accuracy, Recall, Precision, AUC\nAs noted in Table 1, many standard performance metrics for classiﬁers, including accuracy, can be\nexpressed in terms of rates, and thus constrained with rate constraints.\nRecall, deﬁned as TP / (TP + FP), can be constrained as p+(D[y= 1]) >κ for the user’s choice\nof κ∈[0,1]. For example, one may wish to train a classiﬁer that awards free lunches to poor students\nfor overall accuracy, but constrain it to obtain at least95% recall on the most needy students.\nPrecision constraints can be used but are more subtle. Precision can be expressed in rates as\nc+(D[y= 1])/c+(D), and thus to get precision of at least κ, one can add a rate constraint:\nc+(D[y= 1]) −κc+(D) ≥0. (9)\nIf (9) holds, then mathematically the precision is lower-bounded by κon the dataset D. However,\nsince the expectation of a ratio does not equal the ratio of the expected numerator and denomina-\ntor, analyzing how well the empirical constraint holding generalizes to new i.i.d. samples is not\nstraightforward, and violating the constraint (9) by some ϵ >0 does not translate directly into\nprecision.\nThe ROC AUC (Area under the ROC curve) can be approximated using a rate constraint, as in\nEban et al. (2017). The ROC curve is obtained by plotting the true positive rate (TPR) vs. the false\npositive rate (FPR). First, slice up the FPR-axis into Lslices (to approximate the required Riemann\nintegral). Then for the ℓth slice, consider J different decision thresholds and choose the threshold\nthat maximizes TPR and satisﬁes the ℓth slice FPR bound ℓ/L, and then the averaged maximum\nprecision across the LFPR slices is bounded:\n1\nL\nL∑\nℓ=1\nmax\nj∈[J]:p+\nαj(D[y=−1])≤ℓ\nL\np+\nαj(D[y= 1]) ≥κ. (10)\nwhere p+\nα(D)\n△\n= 1\n|D|\n∑\nx∈DIf(x;θ)≥α, c+\nα(D)\n△\n= ∑\nx∈DIf(x;θ)≥α, and αj := 2j−1\n2J for j ∈[J]. In\nparticular, p+\n0 ≡p+. Taking L→∞,J →∞ will have the expression on the LHS of (10) converge\nto the exact ROC AUC.\n3.3 Churn and Win Loss Ratio Constraints\nIn practice, a classiﬁer is often being trained to replace an existing model. In such cases, the new\nclassiﬁer hmay be evaluated compared to the existing one.\nOne common metric to compare two classiﬁers is the win-loss ratio (WLR), which for a given\ntest set is the number of times the new classiﬁer is right and the old classiﬁer is wrong, divided by\nthe number of times the new classiﬁer is wrong and the old classiﬁer is right.\nWLR can be expressed in rates as given in Table 1, where we use D[h = −1] to denote the\nsubset of Dthat is labeled negatively by the classiﬁer h, and D[h= −1,y = 1] to denote the subset\nof Dof whose training label yis 1, so that c+(D[h= −1,y = 1]) is the number of wins of the new\n11COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nclassiﬁer over h, and so on. Re-arranging terms, one can constrain for WLR using a rate constraint:\nc+(D[h= −1,y = 1]) + c−(D[h= 1,y = −1])\n−κ(c+(D[h= −1,y = −1]) + c−(D[h= 1,y = 1])) ≥0, (11)\nwhere κ∈R+ is the lower-bound on the WLR. However, like precision, if the constraint holds on\ndataset Dthen the WLR does meet the lowerbound on D, but generalization to i.i.d. samples is not\nstraightforward, since the expectation of a ratio does not equal the ratio of expectations.\nWLR constraints on different slices of the data can ensure that a new classiﬁer’s gains are not\ncoming at the expense of an important subset of examples. (See also our discussion of no worse off\nand no lost beneﬁts for related fairness constraints).\nIn practice, it is common to test a new classiﬁer by drawing a fresh test set of examples whose\nclassiﬁcation decisions have changed between the new classiﬁer and h, and then only label those\nchanged examples. We refer to this as a fresh test. Fresh tests reduce overﬁtting to a ﬁxed re-used\ntest set, and reduce costs by only labeling examples whose decisions have changed. Under such\ntest set-ups, even if the two classiﬁers have the same accuracies, a higher WLR makes it easier to\nstatistically signiﬁcantly conﬁrm that the new classiﬁer is better than h(Cormier et al., 2016).\nThe proportion of a dataset Dwhose decisions are ﬂipped is called churn (Cormier et al., 2016;\nGoh et al., 2016). Note that when using a fresh test, the labeling costs scales linearly with the churn\n(and the size of the test set D). High churn also causes more instability for follow-on systems, and\ncan confuse users. Goh et al. (2016) proposed explicitly constraining the churn, which can be directly\nexpressed as the rate constraint:\nc+(D[h= −1]) + c−(D[h= 1]) ≤κ|D|, (12)\nwhere κ∈[0,1] is the proportion of Dwhose classiﬁcation decision is allowed to ﬂip.\nWe note that constraining churn on different slices of the data with tighter and looser constraints\ncan be useful. For example, if the classiﬁer is to be used worldwide, but evaluating classiﬁer changes\nis more expensive in Norway than in Vietnam, or if there is known to be less headroom to improve\non examples from Norway, then it can be beneﬁcial to constrain the churn tightly for examples from\nNorway, but loosely for examples from Vietnam.\nOf course, constraining churn too tightly limits the potential accuracy gains. Thus we also\npropose considering loss-only churn constraints, which only penalizes changing decisions if they\nused to be correct:\nc+(D[h= −1,y = −1]) + c−(D[h= 1,y = 1]) ≤κ|D|, (13)\nwhere κ∈[0,1] is the proportion of Dwhose classiﬁcation decision is allowed to ﬂip.\nOne disadvantage of constraining loss-only churn is it requires labeled examples, whereas churn\nconstraints can be conveniently employed on a dataset of unlabeled examples.\n3.4 Group-Speciﬁc Goals and Fairness Goals\nAn important use case for rate constraints is enforcing metrics for different groups or categories of\nexamples. For example, ensuring that a classiﬁer that identiﬁes family-friendly videos works roughly\nequally well for different types of adult content. Rate constraints can be used to enforce a broad\n12OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nset of such group-speciﬁc goals, as detailed in Table 2, where kindexes the Kdifferent groups of\ninterest.\nA special case of group-speciﬁc goals are those that can be interpreted as designed to improve\nsome fairness metric, and in these cases the groups are usually deﬁned as different categories of\npeople, such as races or age brackets. Table 2 shows that many of the fairness goals already studied\nin the machine learning literature can be expressed with rate constraints. However, fairness is a\ncomplex moral and policy problem, and depending on the context and application, different concrete\nformulations may be appropriate to improve fairness measures, and some may not be group-based at\nall. Many of these fairness goals are designed for applications where positive classiﬁcation endows\na beneﬁt, such as being awarded a loan, a job, or a free lunch. For example, the goal of statistical\nparity reﬂects that a bank might be legally required to give loans at equal rates to different groups,\nthat is, the classiﬁer is required to provide equal positive rates of classiﬁcation across groups (see e.g.\n(Zafar et al., 2015; Fish et al., 2016; Hardt et al., 2016; Goh et al., 2016)). Statistical parity is also\nknown as demographic parity (Hardt et al., 2016), and equal coverage (Goh et al., 2016). Notice that\na statistical parity constraint ignores the labels of the training data. We introduce the related goal of\nminimum coverage, which enforces a pre-set minimal beneﬁt rate for each group.\nSimilarly, one of the fairness goals we see in practice but have not noticed in the literature is no\nlost beneﬁts: which requires a model to classify examples positively from each group at least as often\nas the classiﬁer hthat it is replacing. No lost beneﬁts is a type of churn goal (see Sec. 3.3) that is\nmeasured for the whole group (rather than for individual decisions).\nThe other fairness goals in the Table 2 depend on the training labels. For example, we add the\ngoal accurate coverage, which requires the classiﬁer give free lunches to each group to match that\ngroup’s positive training label rate. This goal ignores whether the individual predictions are accurate,\nbut tries to ensure that each group overall receives a rate of beneﬁts that it is labeled as deserving.\nReturning to the scenario where the new classiﬁer will replace a current classiﬁer h, we propose\nthe not worse off fairness goal. For example, suppose we invent a new driving test that is more\naccurate than a current written driving test at diagnosing whether illiterate people are safe drivers,\nthen not worse off requires that the new driving test not reduce accuracy compared to the old test for\nother groups, e.g. senior citizens and teenagers. Not worse off is a label-dependent group-speciﬁc\nchurn goal.\nEqual opportunity and equal odds (Hardt et al., 2016) also rely on the training labels, but\ndisregard any previous classiﬁer. For example, equal opportunity requires that if a classiﬁer awards\nfree lunches (positive classiﬁcation) to half of the east-side children who are labeled as deserving\nfree lunches, then it should also award free lunches to half of the west-side children who are labeled\nas deserving free lunches. Notice that equal opportunity imposes no conditions whatsoever on the\nnegatively-labeled examples (in this case, those shudents who are not labeled as deserving of free\nlunches). In contrast, the fairness goal of equal odds requires both the true positive rate and the false\npositive rate to be the same for all groups. We add to this category the related goal ofequal accuracy,\nwhich aims to make the classiﬁer equally accurate for the different groups.\nAnother fairness goal we ﬁnd useful in practice but have not previously seen in the literature\nis minimum accuracy, which requires that every group experience some pre-set level of accuracy.\nMinimum accuracy ensures that no group is left behind, but respects that for some problems some\ngroups may be much easier to classify than other groups. For such problems, cross-group constraints\ncan lead to degenerate solutions, as the only way to make all groups have equal metrics may be to\nproduce a degenerate classiﬁer.\n13COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nTable 2: Group-Speciﬁc and Fairness Goals Expressed As Rate Constraints for Groupsk= 1,...,K\nStatistical Parity p+(Dk) = p+(D) ∀k\nMinimum Coverage p+(Dk) ≥κ∀kand user-speciﬁed κ∈[0,1]\nNo Lost Beneﬁts p+(Dk) ≥|(Dk[h= 1])|/|Dk|∀k\nAccurate Coverage p+(Dk) = |Dk[y= 1]|/|Dk|∀k\nEqual Opportunity p+(Dk[y= 1]) = p+(D[y= 1]) ∀k\nEqual Odds p+(Dk[y= 1]) = p+(D[y= 1]) ∀k\nand p+(Dk[y= −1]) = p+(D[y= −1]) ∀k\nEqual Accuracy (c+(Dk[y= 1]) + c−(Dk[y= −1]))/|Dk|\n= (c+(D[y= 1]) + c−(D[y= −1]))/|D|∀k\nMinimum Accuracy (c+(Dk[y= 1]) + c−(Dk[y= −1]))/|Dk|≥ κ∀k\nAccurate Coverage p+(Dk) = |Dk[y= 1]|/|Dk|∀k\nNot Worse Off (c+(Dk[y= 1]) + c−(Dk[y= −1]))/|Dk|≥| (Dk[y= h])|/|Dk|∀k\nFairness goals that depend on the training labels are most compelling when the training examples\nand labels are believed to have been fairly sampled and labeled. These goals are less compelling\nwhen the training data is not entirely trusted, or thought to be misaligned with the policy goals, a\nsituation referred to as negative legacy (Kamishima et al., 2012).\n3.5 Egregious Examples and Steering Examples\nAnother use of rate constraints is to constrain the performance on auxiliary labeled datasets to control\nthe classiﬁer.\nFor example, Goh et al. (2016) proposed constraining the classiﬁer for high accuracy on a small\nset of particularly egregious examplesthat should deﬁnitely not be mislabeled. Egregious examples\nact as an integrated unit test: as the classiﬁer trains it actively is testing to see if it satisﬁes the\nconstraint on the egregious examples and is able to correct the training if not.\nAnother practical example of using an auxiliary labeled dataset we termsteering examples, which\nwe deﬁne as a set of labeled examples that are more accurately labeled than the training set. For\nexample, one may have access to a large but noisy training set of clicks on news articles. However, a\nclick on a news article might be because it was relevant news, or because it had a catchy headline. We\ncan try to steer the classiﬁer to focus its ﬁtting on the relevant news articles by providing a smaller but\nexpertly-labeled curated set of examples that mark catchy headlines as negative, and then constrain\nthe classiﬁer to achieve some reasonable accuracy on the steering examples (e.g. 70%). The classiﬁer\nwill be forced by the constraint to disregard the incorrectly-labeled training examples. A second\nexample is a classiﬁer whose goal is to determine if an online store should advertise to a given\ncustomer. Suppose there is a large dataset of training examples with the positive label, “customer\nclicked advertisement and visited website”, but a relatively small set of examples where the positive\nlabel is, “customer clicked advertisement and made a purchase.” It may be better to train on the\nlarge set of “visited” examples due to its much larger size and coverage, but also to constrain at least\n14OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nsome speciﬁed accuracy on the smaller “purchase” examples in order to steer the classiﬁer towards\nprioritizing clicks that lead to purchases.\n3.6 Decision Rule Priors\nMachine learning practitioners often have prior knowledge about a classiﬁcation problem that they\ncan communicate as a decision rule on a tiny set of features. For example, “Don’t recommend a book\nto a user if it is in a language they haven’t purchased before.” We propose incorporating such decision\nrule priors into the structural risk minimization problem by constraining a large set of unlabeled\nsamples to satisfy the decision rule.\nSuch decision rule priors can act as regularizers against noisy and poorly-sampled training\nexamples, and can produce a classiﬁer that is more interpretable because it is known to obey the\ngiven decision rules (like all rate constraints, this depends on whether one constrains with slack or\nnot, and exactly how well the satisﬁed constraint generalizes to a different draw of i.i.d. samples or\nnon-i.i.d. samples depends on the dataset used in the constraint, the classiﬁer’s function class, and\nhow hard the constraint is to satisfy).\nThis proposal is similar to Bayesian Rule Lists (BRL) (Letham et al., 2015) in that a decision\nrule (or set of decision rules) is given a priori to training the model. However, BRL training takes as\ninput a large set of decision rules and outputs a posterior over the rules, rather than incorporating a\ndecision rule into a structural risk minimization problem.\n3.7 How To Best Specify Rate Constraints\nFor any rate constraint, one wants to allow some slack in order to ﬁnd a feasible solution. For\nexample, statistical parity could be written as a constraint with additive slack like this:\np+(D) −p+(Dk) ≤κ,\nor with multiplicative slack like this:\np+(D) −κp+(Dk) ≥0.\nOur experience is that additive slack tends to be more likely to produce reasonable solutions than\nmultiplicative slack for many constraints. The danger to watch out for is whether the constraint is\nspeciﬁed in a way that encourages the training to satisfy the constraint in a suboptimal way. For\nexample, suppose one constrains the false positive rate of each groups to be no worse than 125%\nof the overall false positive rate (multiplicative slack), then the training is incentized to increase\nthe overall false positive rate because that loosens the constraint further (due to the slack being\nmultiplicative).\nConstraints can also be expressed pairwise between groups instead of against the global rate\np+(Dj) −p+(Dk) ≤κ,\nfor all j,k pairs. Our experience is that constraints that involve larger datasets are generally preferable,\nas the smaller the dataset used in a constraint the greater the risk of degenerate solutions or overﬁtting.\nEquality constraints can be expressed by using both a lower-bound and upper-bound inequality\nconstraint. In practice, we suggest allowing some slack of wiggle-room between the lower and upper\nbounds in order to make the optimization more stable, as a larger feasible set will make the stochastic\ngradient optimization more stable.\n15COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\n4. Optimizing with Constraints\nFor nonlinear function classes, training a classiﬁer with rate constraints as per (Equation 7) is a\nnon-convex optimization over a non-convex constraint set. In this section we provide new theoretical\ninsights and algorithms to optimize general non-convex problems with non-convex constraints, then\ndemonstrate our algorithmic proposals work well in practice with multiple real-world constraints in\nSection 5. We ﬁrst outline our two main contributions for this section below.\nA Minimal Stochastic Solution: Algorithms that solve non-convex constrained optimization\nproblems based on regret minimization, which includes our approach as well as previous work (e.g.\nChen et al., 2017; Agarwal et al., 2018) will output a distribution over θs which has discrete support\nover T different θ(produced at different iterations of the algorithm), requiring us to store and sample\nfrom T different models. In practice, large T may be problematic to store and analyze. Surprisingly,\nwe prove that there always exists an equilibrium that has sparse support on at most m+ 1 choices of\nmodel parameters, where mis the number of constraints. We use this result to provide a new practical\nalgorithm to shrink the approximated equilibrium down to a nearly-optimal and nearly-feasible\nsolution supported on at most m+ 1models, which is guaranteed to be at least as good as the original\nstochastic classiﬁer supported on T models.\nHandling Non-Differentiable Constraints: A key issue for Equation 7 is the non-\ndifferentiability of the constraints due to the indicators in the rate constraints. To handle this, in\nSection 4.3, we introduce a new formulation we call the proxy-Lagrangian that changes the standard\ntwo-player zero-sum game to a two-player non-zero-sum game, which presents new challenges to\nanalysis. In fact, solving for such a Nash equilibrium is PPAD-complete in the non-zero-sum setting\nChen and Deng (2006). We prove that a particular game theory solution concept, which we call\nsemi-coarse correlated equilibrium, results in a stochastic classiﬁer that is feasible and optimal. This\nis surprising because the semi-coarse correlated equilibrium is a weaker notion of equilibrium than\nNash equilibrium. We go on to provide a novel algorithm that converges to such an equilibrium. To\nour knowledge, we give the ﬁrst reduction to this particular solution concept and the ﬁrst practical\nuse for it, which may be of independent interest. Interestingly, the θ-player needs to only minimize\nthe usual external regret, but the λ-player must minimize the swap regret (Blum and Mansour, 2007),\na stronger notion of regret. While the resulting distribution is supported on (a possibly large number\nof) (θ,λ) pairs, applying the same “shrinking” procedure as before yields a distribution over only\nm+ 1 θs that is at least as good as the original.\nIn Section 4.1, we handle the optimization of the zero-sum Lagrangian game with an oracle-\nbased algorithm and introduce our proposed “shrinking” procedure. Then, in Section 4.2 we\nintroduce the concept of proxy constraints, describe how it is useful to handle non-differentiable\nconstraints, and formulate the non-zero-sum modiﬁcation of the Lagrangian, which we call the proxy-\nLagrangian. Section 4.3 describes the equilibrium required out of this non-zero-sum game so that it\nwill correspond to an approximately feasible and optimal solution to the constrained optimization\nproblem. Section 4.4 gives an oracle-based procedure for solving for such an equilibrium. Section 4.5\ngives a more practical stochastic gradient-based optimizer along with improved guarantees in the\nconvex setting. Finally, Section 4.6 shows that the ”shrinking” procedure holds for the non-zero-sum\nsolution as well.\n16OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nAlgorithm 1 Optimizes the Lagrangian formulation (Equation 3) in the non-convex setting via the\nuse of an approximate Bayesian optimization oracleOρ (Deﬁnition 1) for the θ-player. The parameter\nRis the radius of the Lagrange multiplier space Λ :=\n{\nλ∈Rm\n+ : ∥λ∥1 ≤R\n}\n, and the function ΠΛ\nprojects its argument onto Λ w.r.t. the Euclidean norm.\nOracleLagrangian (R∈R+,L: Θ ×Λ →R,Oρ : (Θ →R) →Θ,T ∈N,ηλ ∈R+):\n1 Initialize λ(1) = 0\n2 For t∈[T]:\n3 Let θ(t) = Oρ\n(\nL\n(\n·,λ(t)))\n// Oracle optimization\n4 Let ∆(t)\nλ be a gradient of L\n(\nθ(t),λ(t))\nw.r.t. λ\n5 Update λ(t+1) = ΠΛ\n(\nλ(t) + ηλ∆(t)\nλ\n)\n// Projected gradient update\n6 Return θ(1),...,θ (T) and λ(1),...,λ (T)\n4.1 Lagrangian Optimization in the Non-convex Setting\nWe start by assuming an approximate Bayesian optimization oracle (deﬁned in Section 4.1.1),\nwhich enables us to use the Lagrangian formulation and not relax the non-convex and/or non-\ndifferentiable constraints. This setting is a slight generalization of that presented in Agarwal et al.\n(2018). Algorithm 1 solves for a stochastic solution to the non-convex constrained optimization\nproblem. It proceeds by playing the following for T rounds: the model parameter player plays\nbest-response (that is, the θ which minimizes the Lagrangian given the last choice of Lagrange\nmultipliers), and the Lagrange multiplier player plays a regret minimizing strategy (here we use\nprojected SGD).\nOur ﬁrst contribution of this section (in Section 4.1.2) is showing that the resulting stochastic\nclassiﬁer is provably approximately feasible and optimal in expectation. This extends the fair\nclassiﬁcation work of Agarwal et al. (2018) to our slightly more general setting. Our second\ncontribution comes in Section 4.1.4: we will show how the support of the stochastic solution can be\nefﬁciently “shrunk” to one that is at least as good, but is supported on only m+ 1 solutions and is\nshown to also have a considerable gain empirically.\n4.1.1 O RACLE FOR UNCONSTRAINED NON-CONVEX MINIMIZATION (ADDITIVE\nAPPROXIMATION )\nAlgorithm 1, like Chen et al. (2017)’s algorithm for robust optimization, requires an oracle for\nperforming approximate non-convex minimization:\nDeﬁnition 1. A ρ-approximate Bayesian optimization oracle is a function Oρ : (Θ →R) →Θ for\nwhich:\nf(Oρ(f)) ≤ inf\nθ∗∈Θ\nf(θ∗) + ρ\nfor any f : Θ →R that can be written as a nonnegative linear combination of the objective and\nconstraint functions g0,g1,...,g m.\nwith the θ-player using this oracle, and the λ-player using projected gradient ascent. We note that\nthis is a standard assumption in order to obtain theoretical guarantees. (e.g. see Chen et al. (2017),\nwhich uses a multiplicative instead of additive approximation).\n17COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\n4.1.2 A PPROXIMATE MIXED NASH EQUILIBRIUM\nWe characterize the relationship between an approximate Nash equilibrium of the Lagrangian game,\nand a nearly-optimal nearly-feasible solution to the non-convex constrained problem (Equation 2) in\nour theorem below. This theorem has a few differences from the more typical equivalence between\nNash equilibria and optimal feasible solutions in the convex setting. First, it characterizes mixed\nequilibria, in that uniformly sampling from the sequences θ(t) and λ(t) can be interpreted as deﬁning\ndistributions over Θ and Λ. Second, we require compact domains in order to prove convergence rates\n(below) so Λ is taken to consist only of sets of Lagrange multipliers with bounded 1-norm 1.\nFinally, as a consequence of this second point, the feasibility guarantee of Equation 1 (right)\nonly holds if the Lagrange multipliers are, on average, smaller than the maximum 1-norm radius\nR. Thankfully, as is shown by the ﬁnal result of Theorem 1, if there exists a point satisfying the\nconstraints with some margin γ >0, then there will exist Rs that are large enough to guarantee\nfeasibility to within O(ϵ).\nTheorem 1. Deﬁne:\nΛ\n△\n= {λ∈Rm\n+ : ∥λ∥1 ≤R} (14)\nand let θ(1),...,θ (T) ∈Θ and λ(1),...,λ (T) ∈Λ be sequences of parameter vectors and Lagrange\nmultipliers that comprise an approximate mixed Nash equilibrium, i.e.:\nmax\nλ∗∈Λ\n1\nT\nT∑\nt=1\nL\n(\nθ(t),λ∗\n)\n− inf\nθ∗∈Θ\n1\nT\nT∑\nt=1\nL\n(\nθ∗,λ(t)\n)\n≤ϵ\nDeﬁne ¯θas a random variable for which ¯θ= θ(t) with probability 1/T, and let ¯λ\n△\n=\n(∑T\nt=1 λ(t)\n)\n/T.\nThen ¯θis nearly-optimal and nearly-feasible in expectation:\nE¯θ\n[\ng0\n(¯θ\n)]\n≤ inf\nθ∗∈Θ:∀i.gi(θ∗)≤0\ng0 (θ∗) + ϵ and max\ni∈[m]\nE¯θ\n[\ngi\n(¯θ\n)]\n≤ ϵ\nR−∥¯λ∥1\nAdditionally, if there exists a θ′∈Θ that satisﬁes all of the constraints with margin γ(i.e. gi(θ′) ≤\n−γfor all i∈[m]), then:\n∥¯λ∥1 ≤ϵ+ Bg0\nγ\nwhere Bg0 ≥supθ∈Θ g0 (θ) −infθ∈Θ g0 (θ) is a bound on the range of the objective function g0.\nProof. This is a special case of Theorem 3 and Lemma 6 in Appendix A.\n4.1.3 C ONVERGENCE OF ALGORITHM 1\nAlgorithm 1’s convergence rate is given by the following lemma:\nLemma 1. Suppose that Λ and Rare as in Theorem 1, and deﬁne B∆ ≥maxt∈[T]\n∆(t)\nλ\n\n2\n. If we\nrun Algorithm 1 with the step size ηλ := R/B∆\n√\n2T, then the result satisﬁes Theorem 1 for:\nϵ= ρ+ RB∆\n√\n2\nT\nwhere ρis the error associated with the oracle Oρ.\n1. In Appendix A, this is generalized to p-norms.\n18OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nProof. In Appendix C.3.\nCombined with Theorem 1, we therefore have that ifRis sufﬁciently large, then Algorithm 1 will\nconverge to a distribution over Θ that is, in expectation, O(ρ)-far from being optimal and feasible at\na O(1/\n√\nT) rate, where ρis deﬁned in Section 4.1.1.\n4.1.4 S HRINKING THE STOCHASTIC SOLUTION\nA disadvantage of Algorithm 1 is that it results in a mixture of T solutions, which may be large and\nthus undesirable in practice. However, we can show that much smaller Nash equilibria exist:\nLemma 2. If Θ is a compact Hausdorff space, Λ is compact, and the objective and constraint\nfunctions g0,g1,...,g m are continuous, then the Lagrangian game (Equation 3) has a mixed Nash\nequilibrium pair (θ,λ) where θis a random variable supported on at most m+ 1 elements of Θ, and\nλis non-random.\nProof. Follows from Theorem 5 in Appendix B.\nWe do not content ourselves with merely having shown the existence of such an equilibrium.\nThankfully, we can re-formulate the problem of ﬁnding the optimal ϵ-feasible mixture of the θ(t)s\nas a linear program (LP) that can be solved to shrink the support set to m+ 1 solutions. We must\nﬁrst evaluate the objective and constraint functions for every θ(t), yielding a T-dimensional vector of\nobjective function values, and msuch vectors of constraint function evaluations, which are then used\nto specify the LP:\nLemma 3. Let θ(1),θ(2),...,θ (T) ∈Θ be a sequence of T “candidate solutions” of Equation 2.\nDeﬁne ⃗ g0,⃗ gi ∈RT such that (⃗ g0)t = g0\n(\nθ(t))\nand (⃗ gi)t = gi\n(\nθ(t))\nfor i∈[m], and consider the\nlinear program:\nmin\np∈∆T\n⟨p,⃗ g0⟩ s.t.∀i∈[m] .⟨p,⃗ gi⟩≤ ϵ\nwhere ∆T is the T-dimensional simplex. Then every vertex p∗of the feasible region—in particular\nan optimal one—has at most m∗+ 1 ≤m+ 1 nonzero elements, where m∗is the number of active\n⟨p∗,⃗ gi⟩≤ ϵconstraints.\nProof. In Appendix B.\nThis lemma suggests a two-phase approach to actually ﬁnding the m+ 1 stochastic solution. In\nthe ﬁrst phase, apply Algorithm 1, yielding a sequence of iterates for which the uniform distribution\nover the θ(t)s is approximately feasible and optimal. Then apply the procedure of Lemma 3 to\nﬁnd the best distribution over these iterates, which in particular can be no worse than the uniform\ndistribution, and is supported on at most m+ 1 iterates.\n4.2 Proxy Constraints and a Non-Zero Sum Game\nMost real-world machine learning implementations use ﬁrst-order methods (even on non-convex\nproblems, e.g. DNNs). To use such a method, however, one must have gradients, and gradients\nare unavailable for rate constraints (as in Equation 7): due to the indicators in the rate constraint\nexpression (Equation 6), the constraint functions are piecewise-constant, so their gradients are zero\nalmost everywhere, and a gradient-based method cannot be expected to succeed. In general, for\n19COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nconstrained optimization problems in the form of Equation 2, non-differentiable constraints arise\nnaturally when one wishes to constrain counts or proportions.\nThe obvious solution is to use a surrogate. For example, we might consider replacing the\nindicators deﬁning a rate with sigmoids, and then optimizing the Lagrangian. This solves the\ndifferentiability problem, but introduces a new one: a (mixed) Nash equilibrium would correspond to\na solution satisfying the sigmoid-relaxed constraints, instead of theactual constraints. Interestingly, it\nturns out that we can seek to satisfy the original un-relaxed constraints, even while using a surrogate.\nOur proposal is motivated by the observation that, while differentiating the Lagrangian (Equation 3)\nw.r.t. θrequires differentiating the constraint functions gi(θ), to differentiate it w.r.t. λwe only need\nto evaluate them. Hence, a surrogate is only necessary for the θ-player; the λ-player can continue to\nuse the original constraint functions.\nWe refer to a surrogate that is used by only one of the two players as a “proxy”, and introduce\nthe notion of “proxy constraints” by taking ˜gi(θ) to be a sufﬁciently-smooth upper bound on gi(θ)\nfor i∈[m], and formulating two functions that we call “proxy-Lagrangians”:\nLθ(θ,λ)\n△\n= λ1g0(θ) +\nm∑\ni=1\nλi+1˜gI(θ) (15)\nLλ(θ,λ)\n△\n=\nm∑\ni=1\nλi+1gi(θ)\nwhere we restrict Λ to be the (m+ 1)-dimensional simplex ∆m+1. The θ-player seeks to minimize\nLθ(θ,λ), while the λ-player seeks to maximize Lλ(θ,λ). Notice that the ˜gis are only used by\nthe θ-player. Intuitively, the λ-player chooses how much to weigh the proxy constraint functions,\nbut—and this is the key to our proposal—does so in such a way as to satisfy the original constraints.\nViewed as a two-player game, what we have changed is that now the θ and λ players each\nhave their own payoff functions Lθ(θ,λ) and Lλ(θ,λ) respectively, making the game non-zero sum.\nFinding a Nash equilibrium of a non-zero-sum game is much more difﬁcult than for a zero-sum\ngame—in fact, it’s PPAD-complete even in the ﬁnite setting (Chen and Deng, 2006). We will\npresent a procedure which approximates a weaker type of equilibrium: instead of converging to a\nNash equilibrium, it converges to a new solution concept, which we call asemi-coarse correlated\nequilibrium. Despite being weaker than a Nash equlibrium, we show that it still corresponds to a\nnearly-optimal and nearly-feasible solution to constrained optimization in expectation.\nThe proxy-Lagrangian formulation leads to a tighter approximation than the popular approach of\nusing a surrogate for both players, as has been proposed e.g. for Neyman-Pearson classiﬁcation (Dav-\nenport et al., 2010; Gasso et al., 2011), and AUC optimization (Eban et al., 2017). These proposals\noptimize a simpler problem (a zero-sum game), but one that is a worse reﬂection of the true goal. In\nthe experimental section, we will provide evidence that the proposed proxy-Lagrangian formulation\ncan provide higher accuracy while still satisying the constraints. This is especially important when\nthe rate constraints express real-world restrictions on how the learned model is permitted to behave.\nFor example, if we require an 80% threshold in terms of the number of positive predictions, we\nwould like that and not a relaxation of this.\n4.3 Proxy-Lagrangian Equilibrium\nFor the proxy-Lagrangian game (Equation 15), we cannot expect to ﬁnd a Nash equilibrium, at least\nnot efﬁciently, since it is non-zero-sum. However, the analogous result to Theorem 1 requires a\n20OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nweaker type of equilibrium: a joint distribution over Θ and Λ w.r.t. which the θ-player can only\nmake a negligible improvement compared to the best constant strategy, and theλ-player compared to\nthe best action-swapping strategy (this is a type of Φ-correlated equilibrium (Rakhlin et al., 2011)).\nWe call this semi-coarse-correlated equilibrium. We present our theorem showing the achievability\nof this type of equilibrium, then we present Algorithm 2 to satisfy the theorem.\nTheorem 2. Deﬁne Mas the set of all left-stochastic (m+ 1) ×(m+ 1) matrices, Λ\n△\n= ∆m+1 as\nthe (m+ 1)-dimensional simplex, and assume that each ˜gi upper bounds the corresponding gi. Let\nθ(1),...,θ (T) ∈Θ and λ(1),...,λ (T) ∈Λ be sequences satisfying:\n1\nT\nT∑\nt=1\nLθ\n(\nθ(t),λ(t)\n)\n− inf\nθ∗∈Θ\n1\nT\nT∑\nt=1\nLθ\n(\nθ∗,λ(t)\n)\n≤ϵθ\nmax\nM∗∈M\n1\nT\nT∑\nt=1\nLλ\n(\nθ(t),M∗λ(t)\n)\n−1\nT\nT∑\nt=1\nLλ\n(\nθ(t),λ(t)\n)\n≤ϵλ\nDeﬁne ¯θ as a random variable for which ¯θ = θ(t) with probability λ(t)\n1 /∑T\ns=1 λ(s)\n1 , and let ¯λ\n△\n=(∑T\nt=1 λ(t)\n)\n/T. Then ¯θis nearly-optimal and nearly-feasible in expectation:\nE¯θ\n[\ng0\n(¯θ\n)]\n≤ inf\nθ∗∈Θ:∀i.˜gi(θ∗)≤0\ng0 (θ∗) + ϵθ + ϵλ\n¯λ1\n(16)\nand,\nmax\ni∈[m]\nE¯θ\n[\ngi\n(¯θ\n)]\n≤ϵλ\n¯λ1\n(17)\nAdditionally, if there exists a θ′ ∈Θ that satisﬁes all of the proxy constraints with margin γ (i.e.\n˜gi(θ′) ≤−γfor all i∈[m]), then:\n¯λ1 ≥γ−ϵθ −ϵλ\nγ+ Bg0\nwhere Bg0 ≥supθ∈Θ g0 (θ) −infθ∈Θ g0 (θ) is a bound on the range of the objective function g0.\nProof. This is a special case of Theorem 4 and Lemma 7 in Appendix A.\nNotice that Equation 17 guarantees feasibility w.r.t. the original constraints, while Equation 16\nshows that the solution minimizes the objective approximately as well as the best solution that’s\nfeasible w.r.t. the proxy constraints. Hence, the guarantee for minimizing the objective is no better\nthan what we would have obtained if we tookgi\n△\n= ˜gi for all i∈[m], and optimized the Lagrangian as\nin Section 4.1. However, because the feasible region w.r.t. the original constraints is larger (perhaps\nsigniﬁcantly so) than that w.r.t. the proxy constraints, the proxy-Lagrangian approach has more\n“room” to ﬁnd a better solution in practice (this is demonstrated in the experiments).\nOne key difference between this result and Theorem 1 is that the Rparameter is absent. Instead,\nits role, and that of\n¯λ\n\n1, is played by the ﬁrst coordinate of ¯λ. Inspection of Equation 15 reveals that,\nif one or more of the constraints are violated, then theλ-player would prefer the corresponding entries\nin λto be higher, which in turn causes λ1 to become closer to 0 from our procedures. Likewise,\nif they are satisﬁed (with some margin), then it would prefer the entries after the ﬁrst in λto be 0\nwhich causes λ1 to be one in our procedures. In other words, the ﬁrst coordinate of λ(t) encodes the\nλ-player’s belief about the feasibility ofθ(t), for which reason θ(t) is weighted by λ(t)\n1 in the density\ndeﬁning ¯θ.\n21COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nAlgorithm 2 Optimizes the proxy-Lagrangian formulation (Equation 15) in the non-convex setting\nvia the use of an approximate Bayesian optimization oracle Oρ (Deﬁnition 1, but with ˜gis instead\nof gis in the linear combination deﬁning f) for the θ-player, with the λ-player minimizing swap\nregret. The π(M) operation on line 3 results in a stationary distribution of M (i.e. a λ∈Λ such that\nMλ = λ, which can be derived from the top eigenvector).\nOracleProxyLagrangian\n(\nLθ,Lλ : Θ ×∆m+1 →R,Oρ : (Θ →R) →Θ,T ∈N,ηλ ∈R+\n)\n:\n1 Initialize M(1) ∈R(m+1)×(m+1) with Mi,j = 1/(m+ 1)\n2 For t∈[T]:\n3 Let λ(t) = π\n(\nM(t))\n// Stationary distribution of M(t)\n4 Let θ(t) = Oρ\n(\nLθ\n(\n·,λ(t)))\n// Oracle optimization\n5 Let ∆(t)\nλ be a gradient of Lλ\n(\nθ(t),λ(t))\nw.r.t. λ\n6 Update ˜M(t+1) = M(t) ⊙.exp\n(\nηλ∆(t)\nλ\n(\nλ(t))T)\n// ⊙and .exp are element-wise\n7 Project M(t+1)\n:,i = ˜M(t+1)\n:,i /\n˜M(t+1)\n:,i\n\n1\nfor i∈[m+ 1] // Column-wise projection\n8 Return θ(1),...,θ (T) and λ(1),...,λ (T)\nAlgorithm 3 Optimizes the proxy-Lagrangian formulation (Equation 15) in the convex setting,\nwith the θ-player minimizing external regret, and the λ-player minimizing swap regret. The π(M)\noperation on line 4 outputs the stationary distribution of M (that is, a λ ∈Λ such that Mλ = λ)\nwhich can be derived from the top eigenvector. The function ΠΘ projects its argument onto Θ w.r.t.\nthe Euclidean norm.\nStochasticProxyLagrangian\n(\nLθ,Lλ : Θ ×∆m+1 →R,T ∈N,ηθ,ηλ ∈R+\n)\n:\n1 Initialize θ(1) = 0 // Assumes 0 ∈Θ\n2 Initialize M(1) ∈R(m+1)×(m+1) with Mi,j = 1/(m+ 1)\n3 For t∈[T]:\n4 Let λ(t) = π\n(\nM(t))\n// Stationary distribution of M(t)\n5 Let ˇ∆(t)\nθ be a stochastic subgradient of Lθ\n(\nθ(t),λ(t))\nw.r.t. θ\n6 Let ∆(t)\nλ be a stochastic gradient of Lλ\n(\nθ(t),λ(t))\nw.r.t. λ\n7 Update θ(t+1) = ΠΘ\n(\nθ(t) −ηθˇ∆(t)\nθ\n)\n// Projected SGD update\n8 Update ˜M(t+1) = M(t) ⊙.exp\n(\nηλ∆(t)\nλ\n(\nλ(t))T)\n// ⊙and .exp are element-wise\n9 Project M(t+1)\n:,i = ˜M(t+1)\n:,i /\n˜M(t+1)\n:,i\n\n1\nfor i∈[m+ 1] // Column-wise projection\n10 Return θ(1),...,θ (T) and λ(1),...,λ (T)\n4.4 Proxy-Lagrangian Optimization Algorithm\nTo optimize the proxy-Lagrangian formulation, we present Algorithm 2, which is motivated by the\nobservation that, while Theorem 2 only requires that the θ(t) sequence suffer low external regret\nw.r.t. Lθ\n(\n·,λ(t))\n, the condition on the λ(t) sequence is stronger, requiring it to suffer low swap\nregret (Blum and Mansour, 2007) w.r.t. Lλ\n(\nθ(t),·\n)\n.\n22OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nHence, the θ-player uses the oracle to minimize external regret, while the λ-player uses a swap-\nregret minimization algorithm of the type proposed by Gordon et al. (2008), yielding the convergence\nguarantee:\nLemma 4. Suppose that Mand Λ are as in Theorem 2, and deﬁne the upper bound B∆ ≥\nmaxt∈[T]\n∆(t)\nλ\n\n∞\n.\nIf we run Algorithm 2 with the step size ηλ :=\n√\n(m+ 1) ln (m+ 1)/TB2\n∆, then the result\nsatisﬁes satisﬁes the conditions of Theorem 2 for:\nϵθ =ρ\nϵλ =2B∆\n√\n(m+ 1) ln (m+ 1)\nT\nwhere ρis the error associated with the oracle Oρ.\nProof. In Appendix C.3.\n4.5 Practical Stochastic Proxy-Lagrangian Algorithm\nAlgorithm 3 is designed for the convex setting (except for the gis), thus we can safely use SGD for\nthe θ-updates instead of the oracle and enjoy a more practical procedure. We stress that this is a\nconsiderable improvement over previous Lagrangian methods in the convex setting, as they require\nboth the loss and constraints to be convex in order to attain optimality and feasibility guarantees.\nHere, while we assume convexity of the objective and proxy-constraints, the original constraints do\nnot need to be convex, but we are still able to prove similar guarantees.\nLemma 5. Suppose that Θ is a compact convex set, Mand Λ are as in Theorem 2, and that the\nobjective and proxy constraint functions g0,˜g1,..., ˜gm are convex (but not g1,...,g m). Deﬁne the\nthree upper bounds BΘ ≥maxθ∈Θ ∥θ∥2, Bˇ∆ ≥maxt∈[T]\nˇ∆(t)\nθ\n\n2\n, and B∆ ≥maxt∈[T]\n∆(t)\nλ\n\n∞\n.\nIf we run Algorithm 3 with the step sizesηθ := BΘ/Bˇ∆\n√\n2T and ηλ :=\n√\n(m+ 1) ln (m+ 1)/TB2\n∆,\nthen the result satisﬁes the conditions of Theorem 2 for:\nϵθ = 2BΘBˇ∆\n√\n1 + 16 ln2\nδ\nT and ϵλ = 2B∆\n√\n2 (m+ 1) ln (m+ 1)\n(\n1 + 16 ln2\nδ\n)\nT\nwith probability 1 −δover the draws of the stochastic (sub)gradients.\nProof. In Appendix C.3.\n4.6 Shrinking the Stochastic Proxy Lagrangian Solution\nLike Algorithm 1, Algorithms 2 and 3 return a stochastic solutions with support on T discrete\nsolutions. Again, we show that we can ﬁnd just as good a stochastic solution with minimal support\non m+ 1 discrete solutions.\nIt turns out that the same existence result that we provided for the Lagrangian game (Lemma 2)—\nof a Nash equilibrium—holds for the proxy-Lagrangian (this is Lemma 8 in Appendix B). Further-\nmore, the exact same linear programming procedure of Lemma 3 can be applied (with the ⃗ gis being\n23COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\ndeﬁned in terms of the original—not proxy—constraints) to yield a solution with support size m+ 1,\nand works equally well. This is easy to verify: since ¯θ, as deﬁned in Theorem 2, is a distribution over\nthe θ(t)s, and is therefore feasible for the LP, the best distribution over the iterates will be at least as\ngood.\n5. Experiments\nWe illustrate the broad applicability of rate constraints and investigate how well different optimization\nstrategies perform. We use the experiments to investigate the following questions:\nDo rate constraints help in practice?\n•Can we effectively solve the rate-constrained optimization problem?\n•Can we get good results at test time by training with rate constraints?\n•Do rate constraints interact well with other types of constraints (e.g. data-independent shape\nconstraints)?\nDoes the proxy-Lagrangian better solve the constrained optimization problem?\n•Does simply using a hinge surrogate for both players over-constrain?\n•Does the proposed proxy-Lagrangian formulation result in better solutions?\n•With the proxy-Lagrangian, is it necessary in practice for the λ-player to minimize the swap\nregret or does simply minimizing the external regret work just as well?\nDo we really need stochastic classiﬁers?\n•Do the iterates oscillate due to non-existence of an equilibrium in the non-convex setting,\ncausing the last iterate to sometimes be very bad?\n•Does the proposed sparsely supported m-stochastic classiﬁer work at least as well in practice\nas the T-stochastic classiﬁer?\n•Does the best iterate perform as well as the stochastic classiﬁers?\nTo investigate these questions, we compared twelve optimization algorithms for each of seven\ndatasets. Table 3 lists the three benchmark and four real-world datasets we used, each randomly\nsplit into train, validation and test sets. We experimented with seven different rate constraints\nand monotonicity constraints (Groeneboom and Jongbloed, 2014) as described in Table 4 and\nthe following subsections. The last column of Table 4 states whether the classiﬁer had access\nto information about the different datasets used in the constraints, for example, if there were ten\nconstraints deﬁned on ten different countries, was there a features in the feature vectorxthat speciﬁed\nwhich country the example belonged to?\nMore details about each dataset and the chosen constraints are given in the following subsections.\nAs listed in Table 3, we performed the experiments on linear models and two types of nonlinear\nmodels: standard two-layer ReLU neural nets (NN), and two-layer calibrated ensemble of random\ntiny lattices (RTLs) (Canini et al., 2016).\nThe rest of this section delves deeper into experimental details and result tables. Then Section 6\ndiscusses the results and how they provide positive and negative evidence for the above research\nquestions – the reader may prefer to skip to Section 6 and only consult the following experimental\ndetails as needed.\n5.1 TensorFlow Implementation\nOur experiments were all run using TensorFlow. We have already open-sourced our implementation\nof Lagrangian and proxy-Lagrangian optimization at https://github.com/tensorflow/\n24OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\ntensorflow/tree/r1.10/tensorflow/contrib/constrained_optimization.\n(Note to Reviewers: by November 2018 we plan to also open-source a user-friendly API for this\npackage, speciﬁcally for rate constraints.)\nThe lattice models were implemented with the open-source Tensor Flow Lattice package, and\nconsist of learned one-dimensional piecewise linear feature transformations followed by an ensemble\nof interpolated look-up tables (aka lattices). All model parameters are jointly trained. For more\ndetails on lattice models see Gupta et al. (2016),Canini et al. (2016),You et al. (2017). Lattice models\ncan be efﬁciently constrained for partial monotonicity shape constraints, where the term partial refers\nto the practitioner specifying which features can only have a positive (or negative) impact onf(x).\nTo produce the desired partial monotoncity, a large number of data-independent linear inequality\nconstraints are needed, each constraining a pair of model parameters. In the Tensor Flow Lattice\npackage, these monotonicity shape constraints are handled by a projection after each minibatch of\nstochastic gradients, see You et al. (2017) for more details.\n5.2 Hyperparameter Optimization\nFor each of the different datasets, we ﬁx the number of loops and model architecture ahead of time\nto perform well for the unconstrained problem. For the presented results, we validate the ADAM\nlearning rate. Then for each of the twelve compared optimizations, we validate the two ADAM\nlearning rates, one for optimizing the model parametersθ, and the other for optimizing the constraints\nparameters λ. All ADAM learning rates were varied by powers of 10 around the usual default of\nADAM learning rate of 0.001.\nThe usual strategy of choosing hyperparameters that score best on the validation set is not satis-\nfying in the constrained optimization setting, because now there are two metrics of interest: accuracy\nand constraint violation, and the appropriate trade-off between them may be problem dependent.\nOne solution researchers turn to is to side-step the issue of choosing one set of hyperparameters,\nand instead present the Pareto frontier of results over many hyperparameters on the test set. While\ncertainly valuable in a research setting, we must be mindful that in practice one cannot see the Pareto\nfrontier on the test set, and must make a choice for hyperparameters based only on the training and\nvalidation sets (as is standard).\nFor our experiments, we investigate the practical setting in which one must choose one set of\nhyperparameters on which to evaluate the test set. For that, we need a heuristic to choose the best\nhyperparameters based only on the training and validation data. We analyzed a number of such\nheuristics that differently balance the validation accuracy and constraint violation, and were unable to\nﬁnd any heuristic that was perfect, but settled on the following strategy that has some nice properties.\nFor any set βof hyperparameter choices, let LossRank(β) be the rank (1 ...,B ) of the validation\nloss using a model trained with hyperparameter set β, with LossRank(β) = 1 corresponding to the\nsmallest loss, and let WorstConstraintRank(β) be the rank ( 1 ...,B ) of the maximum constraint\nviolation on the validation set. Then choose the hyperparameter set βthat satisﬁes:\nargmin\nβ\nmax {LossRank(β),WorstConstraintRank(β)}, (18)\nwith ties broken by the minimizing the validation loss.\nThis strategy chooses the hyperparameter set that has both low loss and small constraint violations,\nand guarantees that no other hyperparameter set choice would have both better validation accuracy\nand smaller constraint violations.\n25COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nTable 3: Datasets and Model Types Used in Experiments\nDataset Features Train Valid Test Model Type # Model\nParameters\nBank Marketing 60 31,647 4,521 9,042 Linear 61\nAdult 122 34,189 4,884 9,768 Linear 123\nCOMPAS 31 4,320 612 1,225 2 Layer NN 10 hidden units\nBusiness Entity 37 11,560 3,856 3,856 2 Layer NN 16 hidden units\nThresholding 7 70,874 10,125 20,250 2 Layer NN 32 hidden units\nMap Intent 32 420,000 60,000 120,000 RTL 93,600\nFiltering 16 1,282,532 183,219 366,440 RTL 3,305\nTable 4: Constraints Used in Experiments\nDataset Constraints (# of constraints) Constraint Group in x?\nBank Marketing Demographic Parity (5) Y\nAdult Equal Opportunity (4) Y\nCOMPAS Equal Opportunity (4) Y\nBusiness Entity Res. Minimum Recall (18) and Equal Accuracy (1) Y\nThresholding Steering Examples Min Acc. (1) N\nMap Intent No Worse Off (10), Monotonicity (148,800) Y\nFiltering Loss-only Churn (11), Monotonicity (9,740) Y\n5.3 Algorithms Tested\nWe experiment with four groups of algorithms:\n1. Unconstrained: the model is trained without any constraints.\n2. Hinge: We use the common approach of using a hinge relaxation of the constraints in place of\nthe actual constraints in the Lagrangian. This approach refers to that of Algorithm 5.\n3. 0-1 swap: This refers to Algorithm 3, which directly uses the 0-1 constraint in the proxy-\nLagrangian, the λ-player minimizes swap-regret and the θ-player minimizes external regret.\n4. 0-1 ext: This refers to Algorithm 4 training the non-zero-sum game where θplayer minimizes\nthe original Lagrangian but the λ-player minimizes external-regret on the Lagrangian with the\noriginal constraints replaced by the proxy constraints. This is the “obvious” non-zero-sum\nanalogue of the Lagrangian, but does not enjoy the theoretical guarantees of the proxy-\nLagrangian. This is used as a comparison to 0-1 swap to see whether minimizing external\nregret (instead of the more complex swap regret) sufﬁces in practice.\nThen, for each constrained optimization technique, we show the results for the following four\nsolution types:\n1. T-stoch: the stochastic solution that is the uniform distribution over theT iterates θ(1),...,θ (T).\n2. m-stoch: the stochastic solution obtained by applying the“shrinking” technique to the T-stoch\nsolution on the training set, which will have support on at most m+ 1 deterministic solutions.\n3. Last: the last iterate (i.e. θ(T)).\n26OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nAlgorithm 4 Optimizes the Lagrangian formulation with proxy constraints. Like the proxy-\nLagrangian, this is a non-zero-sum game, but unlike the proxy-Lagrangian, we have no theoretical\njustiﬁcation for it. That said, it makes intuitive sense, and works well in practice. The λ-player\noptimizes based on the proxy-constraints and the θ-player optimizes based on the original constraints.\nThe parameter Ris the radius of the Lagrange multiplier space Λ :=\n{\nλ∈Rm\n+ : ∥λ∥1 ≤R\n}\n, and\nthe functions ΠΘ and ΠΛ project their arguments onto Θ and Λ (respectively) w.r.t. the Euclidean\nnorm. {gi}m\ni=1,{˜gi}m\ni=1 are respectively the original constraints and proxy-constraints.\nProxyAdditiveExternalLagrangian (R∈R+,g0 : Θ →R,{gi}m\ni=1,{˜gi}m\ni=1,T ∈N,ηθ,ηλ ∈R+):\n1 Initialize θ(1) = 0, λ(1) = 0 // Assumes 0 ∈Θ\n2 For t∈[T]:\n3 Let ˇ∆(t)\nθ be a stochastic subgradient of g0(θ(t)) + ∑m\ni=1 λ(t)\ni gi(θ) w.r.t. θ\n4 Let ∆(t)\nλ be a stochastic gradient of g0(θ(t)) + ∑m\ni=1 λ(t)\ni ˜gi(θ) w.r.t. λ\n5 Update θ(t+1) = ΠΘ\n(\nθ(t) −ηθˇ∆(t)\nθ\n)\n// Projected SGD updates . . .\n6 Update λ(t+1) = ΠΛ\n(\nλ(t) + ηλ∆(t)\nλ\n)\n// . . .\n7 Return θ(1),...,θ (T) and λ(1),...,λ (T)\n4. Best: the “best” iterate out of all T iterates θ(1),...,θ (T), where “best” is chosen by the\nheuristic given in Equation 18 applied on the training set.\nWe note that in the non-convex proxy-Lagrangian setting, the 0-1 swap algorithm’sT-stoch or\nm-stoch solutions come with theoretical guarantees if we replace the SGD with the approximate\noptimization oracle. In contrast, the 0-1 ext algorithm has no such guarantees, but is simpler.\nSimilarly, in the non-convex setting, the deterministic solutions will not have any guarantees, but are\neven simpler.\n5.4 Bank Marketing\nThe Bank Marketing UCI benchmark dataset (Lichman, 2013) classiﬁer predicts whether someone\nwill sign up for the bank product being marketed. This dataset was used to test improving statistical\nparity for a linear model in Zafar et al. (2015) but with only one protected group based on age. We\nsimilarly use a linear model and age as a protected feature, but create 5 protected groups based on\nthe ﬁve training set quantiles of age. We add a statistical parity rate constraint for each of the ﬁve age\nquantiles with an additive slack of p= 2%:\np+(Dk) ≤p+(D)) −.02%,\nwhere Dk are the training examples from the kth protected group for k= 1,2,..., 5, and Dare all\nthe training examples.\nThe results can be found in Table 5. We note that the Hinge Last solution is adegenerate solution\nin that it always predicts the a priori more probable class.\n5.5 Adult\nWe used the benchmark Adult income UCI dataset (Lichman, 2013). The goal is to predict whether\nsomeone makes more than 50k per year, and also do well at the equal opportunity fairness metric.\n27COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nTable 5: Bank Marketing Experiment Results\nAlgorithm Train Err. Valid Err. Test Err. Train Vio. Valid Vio. Test Vio.\nUnconstrained 0.0948 0.0935 0.0937 0.0202 0.0220 0.0152\nHinge m-stoch. 0.0955 0.0954 0.0949 0 -0.0008 -0.0030\nHinge T-stoch. 0.1109 0.1114 0.1121 -0.0177 -0.0181 -0.0179\nHinge Best 0.0964 0.0969 0.0955 -0.0032 -0.0045 -0.0047\nHinge Last 0.1122 0.1129 0.114 -0.02 -0.02 -0.02\n0-1 swap. m-stoch. 0.0939 0.0943 0.0951 0 -0.0005 0.0019\n0-1 swap. T-stoch. 0.0963 0.0955 0.0947 0.0004 -0.0003 -0.0031\n0-1 swap. Best 0.0936 0.0935 0.0932 -0.0004 -0.0009 -0.0041\n0-1 swap. Last 0.0963 0.0957 0.0954 -0.0007 -0.001 -0.0035\n0-1 ext. m-stoch. 0.0946 0.0952 0.0946 0 -0.001 -0.0024\n0-1 ext. T-stoch. 0.1083 0.1087 0.1085 -0.0135 -0.0146 -0.0139\n0-1 ext. Best 0.0963 0.0964 0.0953 -0.0021 -0.0016 -0.0056\n0-1 ext. Last 0.1029 0.1032 0.101 -0.0046 -0.0072 -0.0056\nTable 6: Adult Experiment Results\nAlgorithm Train Err. Valid Err. Test Err. Train Vio. Valid Vio. Test Vio.\nUnconstrained 0.1421 0.1348 0.1428 0.0803 0.0604 0.0555\nHinge m-stoch. 0.1431 0.1348 0.1442 0 -0.0088 0.0025\nHinge T-stoch. 0.1462 0.1394 0.1481 -0.0409 -0.0372 -0.0436\nHinge Best 0.1424 0.1333 0.1447 -0.028 -0.0154 -0.0317\nHinge Last 0.1532 0.1490 0.1551 -0.0174 -0.0217 -0.0254\n0-1 swap. m-stoch. 0.1431 0.1349 0.1432 0.0176 0.0023 0.0559\n0-1 swap. T-stoch. 0.1428 0.1365 0.1436 0.0054 0.0354 0.0285\n0-1 swap. Best 0.1426 0.1354 0.1440 -0.0016 0.0140 0.0154\n0-1 swap. Last 0.1436 0.1358 0.1443 0.0069 0.0248 0.0221\n0-1 ext. m-stoch. 0.1418 0.1348 0.1432 0 -0.0019 0.0059\n0-1 ext. T-stoch. 0.1441 0.1369 0.1447 0.0034 0.022 0.0174\n0-1 ext. Best 0.1420 0.1348 0.1432 -0.0374 -0.0333 -0.0015\n0-1 ext. Last 0.1436 0.1358 0.1448 -0.0116 0.0078 0.0028\n28OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nTable 7: COMPAS Experiment Results\nAlgorithm Train Err. Valid Err. Test Err. Train Vio. Valid Vio. Test Vio.\nUnconstrained 0.3056 0.3160 0.3109 0.1151 0.2143 0.1082\nHinge m-stoch. 0.3711 0.3744 0.3676 0 0.0395 0.0284\nHinge T-stoch. 0.2880 0.3387 0.3198 0.1093 0.1779 0.0917\nHinge Best 0.2840 0.3322 0.3223 0.0803 0.1262 0.0800\nHinge Last 0.2882 0.3322 0.3231 0.1275 0.1968 0.0996\n0-1 swap. m-stoch. 0.3132 0.3015 0.3174 0.0004 0.0851 0.0111\n0-1 swap. T-stoch. 0.2968 0.3208 0.3219 0.0257 0.1286 0.0547\n0-1 swap. Best 0.3009 0.3096 0.3125 0.0281 0.1084 0.0356\n0-1 swap. Last 0.3023 0.3096 0.3158 0.0412 0.1153 0.0480\n0-1 ext. m-stoch. 0.3145 0.3080 0.3146 0 0.0813 0.0147\n0-1 ext. T-stoch. 0.2990 0.3128 0.3086 0.0323 0.1154 0.0321\n0-1 ext. Best 0.3106 0.3160 0.3101 -0.0069 0.0797 -0.0085\n0-1 ext. Last 0.2935 0.3160 0.3125 0.0330 0.1231 0.0325\nWe used four protected groups: two race-based (Black or White) and two gender-based (Male or\nFemale). We preprocessed the dataset consistent with Zafar et al. (2015) and Goh et al. (2016). Goh\net al. (2016) showed that by explicitly constraining the difference in coverage and using a linear\nmodel, they could achieve higher pfairness and better accuracy than earlier work using correlation\nconstraints of Zafar et al. (2015) by up to 0.5% on this dataset.\nFor these experiments, we added four rate constraints to the training to impose equal opportunity\nat 95%, that is for each of the protected groups (Black, White, Female and Male) the constraints force\nthe classiﬁer’s coverage (the proportion classiﬁed positive) on the positively labeled examples for\neach protected group to be at least 95% of the overall coverage on the positively labeled examples:\np+(Dk[y= 1]) ≥0.95 ·p+(D[y= 1]), (19)\nwhere Dk are the training examples from the kth protected group for k= 1,2,..., 4, and Dare all\nthe training examples.\nWe use a linear model. The results can be found in Table 6.\n5.6 COMPAS\nThe positive label in the ProPublicas COMPAS recidivism data is a prediction the person will re-\noffend. The goal is to predict recidivism with fairness constraints and we preprocess this dataset in\na similar manner as in the Adult dataset and the protected groups are also similar: two race-based\n(Black and White) and two gender-based (Male and Female). The classiﬁer we use is a 2 layer neural\nnetwork with 10 hidden units.\nIn this experiment, the goals are quite similar to that of the Adult experiment. Our protected\ngroups are again two races (Black and White) and two genders (Male and Female) and the goal is\nto constrain equal opportunity such that no group is unfairly getting targeted. However, instead of\nexpressing the constraint with multiplicative slack as in the Adult experiments, we expressed it as an\n29COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nadditive slack of 5%:\np+(Dk[y= 1]) ≤p+(D[y= 1]) + .05%,\nwhere Dk are the training examples from the kth protected group for k= 1,2,..., 4, and Dare all\nthe training examples. That is, the positive prediction rate of the positively labeled examples for each\nprotected class can exceed that of the overall dataset by at most 5%.\nThe results are shown in Table 7.\n5.7 Business Entity Resolution\nIn this entity resolution problem from Google, the task is to classify whether a pair of business\ndescriptions describe the same real business. Features include measures of similarity of the two\nbusiness titles, phone numbers, and so on. We add two types of constraints to the training. First, the\ndata is global, and for each of the 16 most frequent countries, we imposed a minimum recall rate\nconstraint:\np+(Dk[y= 1]) ≥95%,\nwhere Dk are the training examples from the kth country for k = 1,2,..., 16. It is also known\nwhether each example is a chain business or not. We impose the same minimum recall rate constraint\non chain business examples and non-chain business examples. Additionally, we add an equal accuracy\nconstraint that the accuracy on not-chain businesses should not be worse than the accuracy on chain\nbusinesses by more than ten percent, as a proxy to making sure large and small businesses receive\nsimilar performance from the model:\nc+(DnotCh[y= 1]) + c−(DnotCh[y= −1])\n|DnotCh| ≥c+(Dch[y= 1]) + c−(Dch[y= −1])\n|Dch| −0.1,\nwhere ch is an abbreviation for chain.\nWe ran this experiment a two-layer neural network, the results are shown in Table 8). In the top\nrow, one sees that the unconstrained model has a very high maximum constraint violations, because\nit is very difﬁcult to achieve 95% recall for all regions.\n5.8 Thresholding\nFor this Google problem, a ranked list of tens or hundreds of business results is given for a speciﬁc\nquery (e.g. [coffee near me]), and the task is to threshold the list to return only the results worth\nshowing a user. To do this efﬁciently in the production setting, a binary classiﬁer decides if the 2nd\nresult is worth keeping, and if its decision is positive, continues down the ranked list, and once a\nresult is classiﬁed as not worth keeping all lower-ranked results are discarded. For simplicity, all\nexamples are treated as independent even if they originally came from the same ranked list. We use a\n2 layer neural network with 32 hidden units as the classiﬁer.\nA medium-size labeled set is available with labels that are known to be noisy, and the label noise\nis not zero-mean and not homogenous across the feature space. That set is broken uniformly and\nrandomly into train/validation/test sets.\nWe also have an auxiliary independent set of1,814 steering examples (see Section 3.5) which\nwere more carefully labeled by expert labelers, and were actively sampled to pinpoint key types\nof problems. If one only uses the steering examples (ignoring the noisy labeled data), previous\nexperiments have shown that one can stably achieve a 33% cross-validation error rate on the steering\n30OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nTable 8: Business Entity Resolution Experiment Results: 2 Layer NN\nAlgorithm Train Err. Valid Err. Test Err. Train Vio. Valid Vio. Test Vio.\nUnconstrained 0.1223 0.1505 0.1520 0.1727 0.2172 0.2357\nHinge m-stoch. 0.2405 0.2509 0.2535 0 0.0341 0.0282\nHinge T-stoch. 0.3308 0.3351 0.3446 -0.0258 0.0196 -0.0082\nHinge Best 0.2657 0.2720 0.2786 -0.0083 0.0437 0.0026\nHinge Last 0.2483 0.2624 0.2617 -0.0175 0.0125 0.0421\n0-1 swap. m-stoch. 0.1751 0.1953 0.1983 0 0.0745 0.0898\n0-1 swap. T-stoch. 0.1506 0.1749 0.1760 0.0950 0.1427 0.1933\n0-1 swap. Best 0.1407 0.1687 0.1696 0.0681 0.1224 0.1706\n0-1 swap. Last 0.1699 0.1910 0.1927 0.0252 0.0864 0.0846\n0-1 ext. m-stoch. 0.1891 0.2060 0.2063 0 0.0741 0.0752\n0-1 ext. T-stoch. 0.1934 0.2082 0.2092 0.0011 0.0652 0.0770\n0-1 ext. Best 0.1889 0.2053 0.2049 0.0026 0.0750 0.0750\n0-1 ext. Last 0.1968 0.2118 0.2130 0.0008 0.0594 0.0750\nexamples. The goal is to have a model that gets that 33% error on the steering examples, but also\nworks as well as possible on the larger noisy data.\nThe top row of Table 9 shows simply training on the noisy data produces a error rate of 35% on\nthe noisy test data, but violates our goal of 33% error on the steering examples by 3% (that is, it has\nerror rate 36% on the steering examples).\nThe other extreme of training only on the steering examples is also unsatisfying: as reported in\nthe second row of Table 9 that overﬁts the steering examples and performs poorly on the large (noisy)\ntest set with an error rate of 39%, because the steering example set is too small and does not cover\nthe entire feature space.\nFor the rest of the rows in Table 9, we train on the noisy data with a minimum accuracy rate\nconstraint for 67% accuracy on the steering examples:\nc+(Dsteering[y= 1]) + c−(Dsteering[y= −1])\n|Dsteering| ≥0.67.\nAll of the different optimizations ﬁnd essentially feasible solutions, with many able to achieve\nthe same or better test set performance as the unconstrained training (top row).\n5.9 Map Intent\nFor this Google problem, the task is to classify whether a query is seeking a result on a map. For\nexample, the query [coffee near me] would be labeled positive, while [coffee health beneﬁts] would\nbe labeled negative. We add ten rate constraints for ten regions that constrain the new model training\nto be at least as accurate as the production classiﬁer is for each of those ten regions.\nc+(Dregion[y= 1]) + c−(Dregion[y= −1])\n|Dregion| ≥κregion,\n31COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nTable 9: Thresholding Experiment Results\nAlgorithm Train Err. Valid Err. Test Err. Steering Violation\nUnconstrained 0.3595 0.3491 0.3538 0.0316\nUnconstrained Trained on Steering 0.3909 0.3924 0.3930 -0.0456\nHinge m-stoch. 0.3601 0.3512 0.3582 0\nHinge T-stoch. 0.3635 0.3558 0.3594 -0.0037\nHinge Best 0.3606 0.3509 0.3560 -0.0031\nHinge Last 0.3621 0.3542 0.3594 -0.0003\n0-1 swap. m-stoch. 0.3574 0.3500 0.3557 -0.0025\n0-1 swap. T-stoch. 0.3593 0.3513 0.3551 0.0010\n0-1 swap. Best 0.3561 0.3484 0.3532 -0.0020\n0-1 swap. Last 0.3584 0.3497 0.3543 -0.0020\n0-1 ext. m-stoch. 0.3605 0.3504 0.3568 -0.0009\n0-1 ext. T-stoch. 0.3602 0.352 0.3553 0.0010\n0-1 ext. Best 0.3569 0.3486 0.3515 -0.0014\n0-1 ext. Last 0.3579 0.3500 0.3539 -0.0009\nwhere κregion is the accuracy of the production classiﬁer for that region. The feature vector xincludes\nten Bool features that indicate if xbelongs to one of these ten regions (some examples do not belong\nto any of the ten regions).\nThirty-two dense and categorical features are available. We train an RTL model that is an ensem-\nble of 300 lattices, where each lattice acts on 8 of the 32 features, with shared calibrators, and the\nlattices are interpolated using multilinear interpolation, all implemented using the TensorFlow Lattice\npackage. We enforce monotonicity constraints on 28 of the 32 features, resulting in an additional\n148,800 constraints (each one is a linear inequality constraint on a pair of model parameters) applied\nduring training; see You et al. (2017), Canini et al. (2016), Gupta et al. (2016) for more technical\ndetails.\n5.10 Filtering\nFor this Google problem, the task is to classify whether a candidate result for a query should be\nimmediately discarded as too irrelevant to be worth showing to a user. For this problem we take\nas given a base classiﬁer h, and the goal is to maximize accuracy with minimal loss-only churn\n(see Section 3.3 for details). The baseclassiﬁer hwas trained as a regression model to minimize\nmean squared error with respect to a real-valued label on [−1,1], but then used as a classiﬁer by\nthresholding the model’s estimates at 0.0. Here we use the same training data, but we pre-threshold\nthe real-valued training examples to form binary classiﬁcation labels, then train the new classiﬁer to\nminimize the classiﬁcation error rate. We add ten loss-only churn rate constraints to individually\nrestrict the loss-only churn with respect to the production model for each of ten mutually-exclusive\ngeographic regions to less than 5%:\n32OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nTable 10: Map Intent Experiment Results\nAlgorithm Train Err. Valid Err. Test Err. Train Vio. Valid Vio. Test Vio.\nUnconstrained 0.3093 0.3122 0.3104 0.0187 0.0162 0.0319\nHinge m-stoch. 0.3130 0.3129 0.3124 0.0182 0.0176 0.0313\nHinge T-stoch. 0.3096 0.3136 0.3106 0.0194 0.0197 0.0210\nHinge Best 0.3056 0.3131 0.3104 0.0172 0.0194 0.0247\nHinge Last 0.3058 0.3130 0.3099 0.0177 0.0189 0.0220\n0-1 swap. m-stoch. 0.2949 0.3002 0.2997 -0.0003 0.0025 0.0176\n0-1 swap. T-stoch. 0.3004 0.3022 0.3024 0.0022 0.0061 0.0204\n0-1 swap. Best 0.2949 0.3002 0.2997 -0.0003 0.0025 0.0176\n0-1 swap. Last 0.2953 0.3004 0.3002 0.0013 0.0034 0.0192\n0-1 ext. m-stoch. 0.3069 0.3115 0.3101 0.0094 0.0144 0.0231\n0-1 ext. T-stoch. 0.3101 0.3121 0.3107 0.0132 0.0157 0.0243\n0-1 ext. Best 0.3069 0.3115 0.3101 0.0094 0.0144 0.0231\n0-1 ext. Last 0.3071 0.3111 0.3103 0.0096 0.0140 0.0242\nc+(Dregion[y= −1,h = −1]) + c−(Dregion[y= 1,h = 1])\n|Dregion[h= y]| ≤0.05.\nThat is, we ask that no more than ﬁve percent of the base classiﬁer’s wins are lost for each of the ten\nregions. The feature vector xincludes ten Bool features that indicate if xbelongs to one of these ten\nregions (some examples do not belong to any of the ten regions).\nBoth the production regression model hand the new classiﬁer f(x) use the same model archi-\ntecture: both are RTL models that are an ensemble of 50 lattices, where each lattice acts on 6 of 16\ncontinuous-valued features, each feature is calibrated by a monotonic piecewise linear transform\nthat is shared across the lattices, the lattices are interpolated using multilinear interpolation, all\nmodel parameters trained jointly using the TensorFlow Lattice package. We enforce monotonicity\nconstraints on 14 of the 16 features, resulting in an additional 9,740 constraints applied during\ntraining (each of these is simply a linear inequality constraint on a pair of model parameters); see\nYou et al. (2017), Canini et al. (2016), Gupta et al. (2016) for more technical details.\nThe production classiﬁer hhad a test error rate of 39.72%. As hoped, by training speciﬁcally for\nthis classiﬁcation task, the new classiﬁer f(x) achieves lower test error rates: as low as 27.61% for\nthe unconstrained training. However, the high test constraint violation of 32.27% (measured as the\nmaximum violation over the ten regions) shows that the new unconstrained classiﬁer loses a large\nnumber of the wins the base classiﬁer had for at least one of the ten countries considered.\n6. Discussion of Experimental Results\nNow that we have presented the experimental results, we return to discuss the experimental and\ntheoretical evidence for and against the hypotheses and questions posed at the beginning of Section 5.\n33COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nTable 11: Filtering Experiment Results\nAlgorithm Train Err. Valid Err. Test Err. Train Vio. Valid Vio. Test Vio.\nUnconstrained 0.2747 0.2723 0.2761 0.3164 0.3107 0.3227\nHinge m-stoch. 0.3363 0.3362 0.3369 0 -0.0023 -0.0012\nHinge T-stoch. 0.3658 0.3656 0.3665 -0.0297 -0.0262 -0.0243\nHinge Best 0.3404 0.3403 0.3409 -0.0075 -0.0080 -0.0068\nHinge Last 0.3622 0.3618 0.3630 -0.0239 -0.0239 -0.0242\n0-1 swap. m-stoch. 0.3230 0.3231 0.3239 0 0.0071 0.0130\n0-1 swap. T-stoch. 0.3205 0.3208 0.3217 0.0096 0.0192 0.0227\n0-1 swap. Best 0.3175 0.3178 0.3186 0.0081 0.0116 0.0156\n0-1 swap. Last 0.3185 0.3189 0.3195 0.0112 0.0146 0.0118\n0-1 ext. m-stoch. 0.3231 0.3234 0.3243 0 0.0048 0.0065\n0-1 ext. T-stoch. 0.3300 0.3302 0.3309 0.0004 0.0008 0.0014\n0-1 ext. Best 0.3180 0.3179 0.3190 0.0079 0.0116 0.0138\n0-1 ext. Last 0.3268 0.3272 0.3278 0.0021 0.0055 0.0087\n6.1 Do Rate Constraints Help in Practice?\nYes, overall the experiments show rate constraints are are a useful machine learning tool. Let us\nconsider some more speciﬁc questions.\n6.1.1 C AN WE EFFECTIVELY SOLVE THE RATE -CONSTRAINED OPTIMIZATION PROBLEM ?\nYes, but the optimization algorithm does matter. Note here we are asking whether the optimization\nproblem is well-solved, and thus we focus on the training error and the training violation.\nThe good news is that compared to unconstrained (top row in result tables) the 0-1 swap\nregret m-stochastic optimization (row 6 in result tables) consistently across all experiments did\nproduce lower training constraint violations while still achieving reasonable training error compared\nwith the unconstrained. (Recall that each m-stochastic solves a linear program that sparsiﬁes the\ncorresponding T-stochastic such that the constraints are exactly satisﬁed if the T-stochastic solution\nis feasible, so it is by design that the m-stochastic solution train constraint violation is exactly 0.0 for\nmany of the experiments). For Adult (see Table 6), the train error is only.001 worse, but the train\nviolation drops from .0803 to .0176. For Bank Marketing (see Table 5), the train error is slightly\nbetter for 0-1 swap m-stochastic, and the train violation drops from .0202 to 0.0. Similarly for\nCOMPAS (see Table 7), the 0-1 swapm-stochastic has slightly higher training error but drops the\ntrain constraint violation from 0.1151 to almost zero. For Business Entity Resolution (Table 8), the\ntraining error does increase with 0-1 swap m-stochastic, but it is a reasonable price to pay in training\naccuracy for the huge reduction of the worst case equal-accuracy or min-recall constraint violation\nfrom 0.1727 to 0.0. For the Thresholding problem (Table 9), the 0-1 swap m-stochastic is again\nslightly better on training error and effectively reduces the constraint violation to 0.0, and similarly\nfor the Map Intent experiment (Table 10), the training error is lower and the training constraint\nviolation is lower. For Filtering (Table 11), the training error for 0-1 swap regret m-stochastic did go\nup signiﬁcantly from 0.2747 to 0.3230, but the unconstrained training violation was horrendous at\n34OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\n0.3164 whereas the m-stochastic found a feasible solution. In conclusion for all experiments run, we\nfound the 0-1 swap regret m-stochastic did a good or reasonable job at the optimization problem of\nminimizing training error and satisfying the constraints on the training set.\nIn contrast, one can see that using the baseline strategy of approximating all indicators with the\nhinge throughout the optimization can provide poor or even worse results than the unconstrained. For\nexample, on the Map Intent experiment (see Table 10), the hingeT-stochastic solution manages to\nhave slightly both worse training error and worse training constraint violation than the unconstrained.\nThe other hinge optimizations are also un-compelling in this experiment. In contrast, the swap\nregret optimizations consistently ﬁnd good solutions with lower training error and roughly zero\ntraining constraint violations. This is a challenging optimization problem because there are ten rate\nconstraints on ten regions of differing sizes.\nThe baseline strategy of simply taking the last iterate often does a good job at solving the\nconstrained problem, but sometimes is worse at optimizing the constrained problem than the un-\nconstrained solver! For example, on COMPAS (see Table 7) the Hinge Last training violation is\nactually bigger than the unconstrained training violation. While Hinge Last does achieve slightly\nbetter training error, it hasn’t achieve better validation error (or test error), so we don’t believe this\nwas simply an unlucky validation of hyperparameter choice. For more details on why last iterate can\nperform badly, see Section 6.3.1.\nWhile theory dictates a stochastic solution is necessary for guarantees, in practice theT-stochastic\nsolutions can be quite poor, for example the T-stochastic solution on Map Intent (Table ??) the\nHinge T-stochastic solution is worse than unconstrained on both training error and training constraint\nviolation. This may be due to bad early iterates, which would be diluted with a longer run time.\nCompared to the T-stochastic solutions, the m-stochastic solutions are always better on training error\nand never more violating, as designed.\nThe best iterate is by deﬁnition always at least as good as the last iterate on the training error\nand/or training violation. For all three optimization strategies (hinge, 0-1 swap regret, 0-1 external\nregret), it manages to consistently produce solutions that are better than the unconstrained in terms of\ntraining violations and have reasonable or good training errors.\n6.1.2 C AN WE GET GOOD TEST RESULTS BY TRAINING WITH RATE CONSTRAINTS ?\nYes, mostly. The m-stochastic and best iterate solutions do result in lower test violations and\nreasonable test errors for six of the seven experiments. However, for Adult (Table 6), the 0-1 swap\nm-stochastic failed to produce lower test violation nor lower test error than the unconstrained,\ndespite having much lower training and validation violations. Sadly, the good training and validation\nperformance simply did not generalize to the test set. This case is hard in part because the Black\nconstraint in the Adult dataset is based on a relatively small sample: only 345 positive training\nexamples, 42 positive validation examples, and 179 positive test examples.\nOverall, small constraint datasets can lead to poor generalization that can signiﬁcantly hurt the\noverall metrics. The worst generalization happened with the Business Entity Resolution, where\ntraining violations for the proxy-Lagrangian methods ranged from [0 −.095], but the test violations\nranged from [0.075 −0.19]. For that experiment, the hinge solutions generalized better, but at the\ncost of much higher test errors. Business Entity is a particularly hard problem because there are\n16 constraints on different regions, some of which have very small datasets, and just like training a\nmodel, there is a greater risk of poor generalization if the datasets used in the constraints are small.\n35COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nFor the larger datasets (Map Intent in Table 10 and Filtering in Table 11), the classiﬁer perfor-\nmance was much more similar on training and test sets.\nFor a further discussion of generalization for rate-constraints, with some theoretical results and\npractical strategies, see Cotter et al. (2018a).\n6.1.3 D O RATE CONSTRAINTS INTERACT WELL WITH OTHER TYPES OF CONSTRAINTS ?\nWe did not see any problems from combining rate constraints with shape constraints. For Map Intent\n(Table 10), both 0-1 optimization strategies worked reasonably, with the 0-1 swap regret producing\nattractive solutions that both notably lowered training error and satisﬁed the constraints on the traning\nset. This shows that the addition of the 148,800 pairwise monotonicity constraints did not cause\na problem in optimizing the rate constrained problem. Similarly, for the Filtering (Table 11), the\naddition of the 9,740 sparse linear inequality constraints to enforce monotonicity did not keep the\noptimizers from satisfying the rate constraints.\n6.2 Does the Proxy-Lagrangian Better Solve the Constrained Optimization Problem?\nWe break this question into a few speciﬁc questions.\n6.2.1 D OES SIMPLY USING HINGE SURROGATE FOR BOTH PLAYERS OVERCONSTRAIN ?\nWe hypothesized that using the hinge loss as a convex relaxation to the 0-1 indicators in the rate\nconstraints would cause the constrained optimization to ﬁnd overly-constrained solutions at the cost\nof more training accuracy than needed to satisfy the constraints. This was not as large an effect as\nwe expected. However, it can be seen in the Business Entity Resolution (Table 8) experiment where\nthe hinge training violations are negative and the training errors are relatively high, whereas the 0-1\nm-stochastic solutions crisply achieve the constraint with much lower training errors.\n6.2.2 D OES THE PROXY -LAGRANGIAN FORMULATION RESULT IN BETTER SOLUTIONS ?\nIn most experiments, there were trade-offs between test constraint violation and test accuracy which\nmake it difﬁcult to compare the hinge solutions to the proxy-Lagrangian solutions (denoted 0-1 in\nthe tables) on the test metrics.\nOn the training metrics, there is stronger evidence the 0 −1 m-stochastic optimization is in\nfact doing a better job solving the optimization problem than the hinge m-stochastic. For 7 of the 7\nexperiments, the 0 −1 ext. m-stochastic produced both lower train error and lower train violation\nthan the Hinge m-stochastic solution. This was also true for 5 out of the 7 experiments for the 0 −1\nswap m-stochastic, and the solutions were close for the remaining 2 experiments.\nIn the case of Map Intent (Table 10), we clearly see that the Hinge solutions perform worse in\nboth accuracy and fairness constraints than the 0-1 proxy-Lagrangian procedures on both training\nand testing. In the case of Thresholding, we see that the Hinge procedures seem to do slightly\nworse in ﬁnal accuracy at the cost of over-constraining. We see that in Business Entity Resolution\n(Table 8), the Hinge procedures attain signiﬁcantly higher errors than the other methods but do attain\nbetter constraint satisfaction on testing. Thus, even though proxy-Lagrangian formulation may seem\nbetter on a few of the datasets, this effect was not seen consistently across the remaining datasets\nand thus, the question of whether the proxy-Lagrangian attains better solutions in practice remains\ninconclusive.\n36OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nFigure 1: The plots for the errors and constraint violations for each iteration during training on the\nCOMPAS dataset. The oscillation due to the conﬂicting goals of accuracy and constraints\nsuggest that there may be no pure equilibrium to converge to in the non-convex setting.\n6.2.3 I S MINIMIZING SWAP REGRET NECESSARY , OR DOES EXTERNAL REGRET SUFFICE ?\nOur theoretical results show that in the proxy-Lagrangian setting, the appropriate type of equilibrium\n(i.e. semi-coarse correlated equilibrium) has optimality and feasibility guarantees for the original\nconstrained optimization problem. In order to attain such an equilibrium, we needed the λ-player\nto minimize swap-regret (while the θ-player minimizes the classic external regret). However,\nminimizing swap-regret involves a more complicated procedure. We used the strategy of Gordon\net al. (2008), who showed that any external regret minimizing procedure can be turned into one that\nminimizes swap regret by a meta-algorithm which runs mcopies of the procedure. We questioned\nwhether it would be just as good in practice to use the simpler external-regret minimizing procedure,\nwhich still leads to a coarse-correlated equilibrium (which is a weaker notion than semi-coarse\ncorrelated equilibrium).\nComparing the swap regret to the external regret for the same solution type ( m-stochastic/T-\nstochastic/best/last), the external regret usually ends up with a solution with slightly lower test\nviolations but slightly higher test error. The only exception was the Map Intent experiment in\nwhich the swap-regret solutions were both considerably more accurate and better at satisfying the\nconstraints. In conclusion, we have not seen experimental evidence that the extra complexity of swap\nregret is warranted in practice, though it may sometimes produce notably worse results (as in Map\nIntent).\n6.3 Do We Really Need Stochastic Classiﬁers?\nNext, we investigate some speciﬁc questions regarding the necessity of stochastic solutions over a\ndeterministic classiﬁer.\n6.3.1 D O THE ITERATES OSCILLATE IN THE NON -CONVEX SETTING ?\nAs noted in Section 6.1.1, simply taking the last iterate can produce worse constraint violations\nto the optimization problem then solving the unconstrained problem. Figure 1 plots the error and\nconstraints for each of the iterates on the COMPAS dataset which shows such oscillation. This\n37COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nsuggests that, as we showed in Section 1.5, the phenomenon of the non-convex Lagrangian having\nno pure Nash equilibrium to which it can converge, may occur in practice.\n6.3.2 D OES m-STOCHASTIC BEAT T-STOCHASTIC ?\nOur theoretical results guarantee that the m-stochastic solution (which is obtained through solving a\nsimple LP on the T-stochastic iterates) will be no worse than the T-stochastic solution by forcing the\nm-stochastic solution to be at least as feasible as the T stochastic solution, while having no worse\nerror (at least on the training set). Our hope is therefore that our “shrinking” procedure will ﬁnd\nbetter solutions on test data.\nWe see consistently across datasets as well as optimization techniques that the m-stochastic is\nindeed better than the T-stochastic in terms of both error and constraint violation on training. Part of\nthis effect may be due to the fact that many of the iterates of the T-stochastic perform poorly, for\nexample the early iterates before our procedures are able to get to reasonable solutions. Or during\nphase-transitions if there is oscillation between satisfying constraints and satisfying error. Fortunately,\nthe shrinking procedure seems to be able to choose a good re-weighting of the T-stochastic solution\nin order to attain well-performing ﬁnal results.\nWe also see that in the vast majority of situations, the test performance for them-stochastic either\nsurpasses that of the T-stochastic, or there is an accuracy-fairness trade-off between the two (and\nhence, not straightforward to compare the two).\n6.3.3 D OES THE BEST ITERATE PERFORM AS WELL AS THE STOCHASTIC CLASSIFIERS ?\nWe have already established that a stochastic solution is needed in theory (Section 1.5). However,\nstochastic solutions are unappealing in practice: they take more memory, are harder to test and debug\ndue to their inherent randomness, and a randomized decision may feel less fair in certain contexts\n(even if the outcomes statistically improve the desired fairness metric). Here, we ask if a stochastic\nsolution is needed in practice, based on test metrics.\nFirst, we compare the 0-1 swap regret m-stochastic solution, which is our theoretically preferred\nstochastic solution, to the 0-1 swap regret best iterate. The 0-1 swap best iterate is never a strictly\nworse choice than the 0-1 swap m-stochastic. In some cases the m-stochastic solution puts all or\nmost of its weight on the best iterate—for example, for the Map Intent problem (Table 10) the two\nsolutions are identical. In other experiments the solutions differ but both achieve reasonable different\ntrade-offs of test error and test violation, for example on the Thresholding problem (Table 9) and\nCOMPAS (see Table 7), the best iterate has a lower test error, but a higher test constraint violation.\nComparing the m-stochastic solution and best iterate solution for the hinge optimization and the\n0-1 external regret optimization similarly suggests that much of the time the best iterate works just as\nwell in practice.\n6.3.4 D OES BEST ITERATE PERFORM BETTER IN PRACTICE THAN LAST ITERATE ?\nWe have established that using the best iterate works well in practice. Now we discuss how much\nbetter best is than simply taking the last iterate. In fact, the last iterate is strictly worse at test metrics\nthan the best iterate for 4 of the 7 experiments: Bank, Thresholding, Adult, and Compass; and the\ntwo solutions are similar for the other three experiments.\nIf there were truly oscillations seen in practice, then the last iterate could be highly unstable and\ncould produce undesirable solutions. In practice, the strongest evidence for last being a risky choice\n38OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nis Hinge Last on COMPAS, where test error went up from 0.3109 to 0.3231, and training violation\nonly went down from 0.1082 to 0.0996.\nOverall, the experimental results suggest that the best iterate should be preferable to the last\niterate.\n7. Conclusions, Advice to Practitioners, and Open Questions\nIn this paper, we provide the most comprehensive study to-date of training classiﬁers with a broad\narray of rate constraints, with new theoretical, algorithmic, and experimental results as well as\npractical insights and guidance for using rate constraints to solve real-world problems. Next, we\nprovide some conclusions, speciﬁcally draw out our best advice to practitioners, and note some open\nquestions.\n7.1 Advice to Practitioners: How To Train Classiﬁers with Rate Constraints\nBased on our experiments, our advice to practitioners is to optimize the rate-constrained training\nusing either our proposed proxy-Lagrangian formulation (0-1 swap regretm-stochastic), or the easier\nalternative of optimizing a non-zero-sum variant of the normal Lagrangian formulation (0-1 external\nregret best iterate).\nThe 0-1 external regret best iterate optimization procedure is simple: when optimizing the model\nparameters θuse stochastic gradient descent as usual with a hinge relaxation of the indicators in the\nconstraints, and when optimizing the Lagrange multipliers λuse stochastic gradient descent, but\ndo not relax the indicators in the rate constraints. One downside though is the best iterate requires\nstoring all the candidate iterates on the Pareto Frontier during training, in order to rank them by\nthe training objective and training error at the end, and in the worst case that could be all candidate\niterates. However one can control the number of candidate iterates. Simply taking the last iterate\nmay also yield reasonable results, but we saw a number of situations where the last iterate performed\nstrictly worse under all metrics compared to the best iterate.\nWe caution against relaxing the indicators for both the θ-player and λ-player. It is hardly simpler\nthan the 0-1 external regret best iterate optimization, and experimentally generally (but not always)\nproduced worse test results, sometimes notably worse.\n7.2 More Experimental Conclusions\nThe clearest experimental ﬁnding is that treating the optimization as a non-zero-sum two-player\ngame where the λ-player does not relax the indicators in the rate constraints (notated as 0-1 in the\nexperimental tables) does generally help, both in ﬁnding a better solution to the optimization problem\n(i.e. train metrics), and in practice (i.e. test metrics). Another fairly clear experimental ﬁnding is that\nthe T-stochastic solution can effectively be sparsiﬁed to an m-stochastic solution, generally with\nimproved metrics.\nWhile the T-stochastic solution has better theoretical guarantees than any of our deterministic\nsolutions, especially for large T, in practice we found the deterministic best iterate generally worked\nbetter than the T-stochastic solution. Other comparisons were more cloudy, see Section 6 for details.\n39COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\n7.3 Advice to Practitioners: Plan To Overﬁt the Constraints\nA key issue with using rate constraints is generalization: satisfying the constraints on the training\nexamples does not necessarily mean that they will be satisﬁed on new test sets, and the generalization\nmay be worse if the test examples are drawn from a different distribution. In expressing the rate\nconstraints, one should add in some slack to account for generalization issues, especially if the\nconstraints are optimized on small datasets.\n7.4 Open Questions on Generalization\nIn some speciﬁc applications, the post-training correction approach using a separate validation\ndataset of Woodworth et al. (2017) can improve generalization performance. More generally, in\nCotter et al. (2018a), we extend the ideas of this paper with a focus on generalization. We show\nthat providing different datasets to the two players, instead of (or in addition to) different constraint\nfunctions, can theoretically and practically improve generalization.\n7.5 Advice to Practitioners: How the Constraints Are Speciﬁed Matters\nThough not explored in this paper’s experiments, we have learned that in practice, how one speciﬁes\nthe datasets and slack in a rate constraint are very important - see Section 3.7 for details.\n7.6 Open Questions on Nonlinear Rate Constraints\nWe have limited our focus to rate constraints that can be written as in Equation 6, that is linear\nnon-negative combinations of the positive and negative classiﬁcation rates on datasets. It remains\nan open question how well the presented techniques would work for nonlinear rate constraints both\ntheoretically and experimentally, and whether other strategies would be needed. We touched on these\nissues in Section 3 in our discussion of win-loss ratio and precision, but did not present experimental\nresults with nonlinear rate constraints.\n7.7 Some Open Theoretical Questions\nOne open question is how tight our optimality and feasibility guarantees are for our procedures in the\nfollowing aspects:\n•The dependence on the number of iterations T for our guarantees is O\n(√\n1\nT\n)\n. This rate is an\nartifact of our usage of regret-minimization procedures, but it could be improved through a\nnumber of possible techniques, such as variance reduction (e.g. Johnson and Zhang, 2013), or\nby making stronger assumptions (e.g. strong convexity and/or smoothness).\n•The dependence on m, the number of constraints, isO(√mlog m), which also comes from the\nregret-minimization procedures. This is because the λ-player essentially chooses a distribution\nover m+ 1 actions and this dependence on the number of arms is tight in the context of\nregret-minimization, but the question remains of whether there are situations where this could\nbe improved upon for constrained optimization for either feasibility or optimality.\n•Our results also have a dependence on the model complexity in both feasibility and optimality\nguarantees. This may be undesirable in models with a large number of parameters, such as\nmodern neural networks. We explored the question of whether we can improve upon this\ndependence further in follow-up work of Cotter et al. (2018a), which improves the feasibility\n40OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nguarantee. However, further investigation is required to either establish matching lower bounds\nand/or obtaining tighter results.\nReferences\nA. Agarwal, A. Beygelzimer, M. Dud´ık, J. Langford, and H. Wallach. A reductions approach to fair\nclassiﬁcation. In ICML, 2018.\nS. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: a meta-algorithm and\napplications. Theory of Computing, 8(6):121–164, 2012.\nR. E. Barlow, D. J. Bartholomew, J. M. Bremner, and H. D. Brunk. Statistical inference under order\nrestrictions; the theory and application of isotonic regression. Wiley, New York, USA, 1972.\nA. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex\noptimization. Oper. Res. Lett., 31(3):167–175, May 2003.\nK. Bellare, G. Druck, and A. McCallum. Alternating projections for learning with expectation\nconstraints. UAI, 2009.\nA. Blum and Y . Mansour. From external to internal regret.JMLR, 8:1307–1324, 2007.\nH. F. Bohnenblust, S. Karlin, and L. S. Shapley. Games with continuous, convex pay-off. Contribu-\ntions to the Theory of Games, 1(24):181–192, 1950.\nM. Bonakdarpour, S. Chatterjee, R. F. Barber, and J. D. Lafferty. Prediction rule reshaping. In ICML,\n2018.\nK. Canini, A. Cotter, M. R. Gupta, M. Milani Fard, and J. Pfeifer. Fast and ﬂexible monotonic\nfunctions with ensembles of lattices. In NIPS, pages 2919–2927, 2016.\nR. S. Chen, B. Lucier, Y . Singer, and V . Syrgkanis. Robust optimization for non-convex objectives.\nIn NIPS’17, 2017.\nX. Chen and X. Deng. Settling the complexity of two-player Nash equilibrium. In FOCS’06, pages\n261–272. IEEE, 2006.\nY . Chen and R. J. Samworth. Generalized additive and index models with shape constraints.Journal\nRoyal Statistical Society B, 2016.\nP. Christiano, J. A. Kelner, A. Madry, C. A. Spielman, and S. Teng. Electrical Flows, Laplacian\nSystems, and Faster Approximation of Maximum Flow in Undirected Graphs. In STOC ’11, pages\n273–282, 2011.\nQ. Cormier, M. Milani Fard, K. Canini, and M. R. Gupta. Launch and iterate: Reducing prediction\nchurn. Advances in Neural Information Processing Systems (NIPS), 2016.\nA. Cotter, M. R. Gupta, and J. Pfeifer. A Light Touch for heavily constrained SGD. In 29th Annual\nConference on Learning Theory, pages 729–771, 2016.\n41COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nA. Cotter, M. Gupta, H. Jiang, N. Srebro, K. Sridharan, S. Wang, B. Woodworth, and S. You. Training\nfairness-constrained classiﬁers to generalize. FAT/ML Workshop, 2018a.\nA. Cotter, H. Jiang, and K. Sridharan. Two-player games for efﬁcient non-convex constrained\noptimization, 2018b. URL https://arxiv.org/abs/1804.06500.\nM. Davenport, R. G. Baraniuk, and C. D. Scott. Tuning support vector machines for minimax and\nNeyman-Pearson classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n2010.\nM. Donini, L. Oneto, S. Ben-David, J. Shawe-Taylor, and M. Pontil. Empirical risk minimization\nunder fairness constraints. arXiv preprint arXiv:1802.08626, 2018.\nE. Eban, M. Schain, A. Mackey, A. Gordon, R. A. Saurous, and G. Elidan. Scalable learning of\nnon-decomposable objectives. AISTATS’17, 2017.\nB. Fish, J. Kun, and A. D. Lelkes. A conﬁdence-based approach for balancing fairness and accuracy.\nProc. SIAM Intl. Conf. Data Mining, 2016.\nD. Garber and E. Hazan. Playing non-linear games with linear oracles. In FOCS, pages 420–428.\nIEEE Computer Society, 2013.\nG. Gasso, A. Pappaionannou, M. Spivak, and L. Bottou. Batch and online learning algorithms\nfor nonconvex Neyman-Pearson classiﬁcation. ACM Transactions on Intelligent Systems and\nTechnology, 2011.\nI. L. Glicksberg. A further generalization of the Kakutani ﬁxed point theorem with application to\nNash equilibrium points. Amer. Math. Soc., 3:170–174, 1952.\nG. Goh, A. Cotter, M. Gupta, and M. P Friedlander. Satisfying real-world goals with dataset\nconstraints. In NIPS’16, pages 2415–2423. 2016.\nG. J. Gordon, A. Greenwald, and C. Marks. No-regret learning in convex games. In ICML’08, pages\n360–367, 2008.\nP. Groeneboom and G. Jongbloed. Nonparametric estimation under shape constraints. Cambridge\nPress, New York, USA, 2014.\nM. R. Gupta, A. Cotter, J. Pfeifer, K. V oevodski, K. Canini, A. Mangylov, W. Moczydlowski, and\nA. van Esbroeck. Monotonic calibrated interpolated look-up tables. JMLR, 17(109):1–47, 2016.\nM. R. Gupta, D. Bahri, A. Cotter, and K. Canini. Diminishing returns shape constraints for inter-\npretability and regularization. arXiv, 2018.\nM. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. NIPS, 2016.\nE. Hazan and S. Kale. Projection-free online learning. In ICML’12, 2012.\nM. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML’13,\nvolume 28, pages 427–435, 2013.\n42OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance\nreduction. In NIPS’13, pages 315–323, 2013.\nT. Kamishima, S. Akaho, H. Asoh, and J. Sakuma. Fairness-aware classiﬁer with prejudice remover\nregularizer. Machine Learning and Knowledge Discovery in Databases, pages 35–50, 2012.\nM. Kearns, S. Neel, A. Roth, and Z. S. Wu. Preventing fairness gerrymandering: Auditing and\nlearning for subgroup fairness, 2017. URL https://arxiv.org/abs/1711.05144.\nB. Letham, C. Rudin, T. H. McCormick, and D. Madigan. Interpretable classiﬁers using rules and\nBayesian analysis: building a better stroke prediction model. Annals of Applied Statistics, 2015.\nM. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/\nml.\nZ. Long, Y . Lu, X. Ma, and B. Dong. PDE-Net: Learning PDEs from Data. In ICML, 2018.\nR. Luss and S. Rosset. Bounded isotonic regression. Electronic Journal of Statistics, 11(2):4488–\n4514, 2017.\nM. Mahdavi, T. Yang, R. Jin, S. Zhu, and J. Yi. Stochastic gradient descent with only one projection.\nIn NIPS’12, pages 494–502. 2012.\nG. S. Mann and A. McCallum. Simple, robust, scalable semi-supervised learning with expectation\nregularization. In ICML, 2007.\nG. S. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning with\nweakly labeled data. Journal of Machine Learning Research, 11, 2010.\nH. Narasimhan. Learning with complex loss functions and constraints. In AISTATS’18, 2018.\nA. Nemirovski and D. Yudin. Problem complexity and method efﬁciency in optimization. John Wiley\n& Sons Ltd, 1983.\nT Parthasarathy. Equilibria of continuous two-person games. Paciﬁc Journal of Mathematics, 57(1):\n265–270, 1975.\nN. Pya and S. N. Wood. Shape constrained additive models. Statistics and Computing, 2015.\nA. Rakhlin and K. Sridharan. Optimization, learning, and games with predictable sequences. In\nNIPS’13, pages 3066–3074, 2013.\nA. Rakhlin, K. Sridharan, and A. Tewari. Online Learning: Beyond Regret. In COLT’11, pages\n559–594, 2011.\nC. D. Scott and R. D. Nowak. A Neyman-Pearson approach to statistical learning.IEEE Transactions\non Information Theory, 2005.\nN. Srebro, K. Sridharan, and A. Tewari. On the universality of online mirror descent. In NIPS’11,\n2011.\n43COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nR. Stewart and S. Ermon. Label-free supervision of neural networks with physics and domain\nknowledge. Proc. AAAI, 2017.\nJ von Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295–320, 1928.\nB. E. Woodworth, S. Gunasekar, M. I. Ohannessian, and N. Srebro. Learning Non-Discriminatory\nPredictors. In COLT’17, pages 1920–1953, 2017.\nT. Yang, Q. Lin, and L. Zhang. A richer theory of convex constrained optimization with reduced\nprojections and improved rates. In International Conference on Machine Learning, pages 3901–\n3910, 2017.\nS. You, K. Canini, D. Ding, J. Pfeifer, and M. R. Gupta. Deep lattice networks for learning partial\nmonotonic functions. NIPS, 2017.\nM. B. Zafar, I. Valera, M. G. Rodriguez, and K. P. Gummadi. Fairness constraints: A mechanism for\nfair classiﬁcation. In ICML Workshop on Fairness, Accountability, and Transparency in Machine\nLearning, 2015.\n44OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nAppendix A. Proofs of Sub{optimality,feasibility}Guarantees\nTheorem 3. (Lagrangian Sub{optimality,feasibility}) Deﬁne Λ =\n{\nλ∈Rm\n+ : ∥λ∥p ≤R\n}\n, and\nconsider the Lagrangian of Equation 2 (Equation 3). Suppose that θ ∈Θ and λ∈Λ are random\nvariables such that:\nmax\nλ∗∈Λ\nEθ[L(θ,λ∗)] − inf\nθ∗∈Θ\nEλ[L(θ∗,λ)] ≤ϵ (20)\ni.e. θ,λ is an ϵ-approximate Nash equilibrium. Then θis ϵ-suboptimal:\nEθ[g0 (θ)] ≤ inf\nθ∗∈Θ:∀i∈[m].gi(θ∗)≤0\ng0 (θ∗) + ϵ\nFurthermore, if λis in the interior of Λ, in the sense that\n¯λ\n\np < Rwhere ¯λ:= Eλ[λ], then θis\nϵ/\n(\nR−\n¯λ\n\np\n)\n-feasible:\n(Eθ[g: (θ)])+\n\nq ≤ ϵ\nR−\n¯λ\n\np\nwhere g: (θ) is the m-dimensional vector of constraint evaluations, and (·)+ takes the positive part of\nits argument, so that\n(Eθ[g: (θ)])+\n\nq is the q-norm of the vector of expected constraint violations.\nProof. First notice that Lis linear in λ, so:\nmax\nλ∗∈Λ\nEθ[L(θ,λ∗)] − inf\nθ∗∈Θ\nL\n(\nθ∗,¯λ\n)\n≤ϵ (21)\nOptimality: Choose θ∗to be the optimal feasible solution in Equation 21, so that gi(θ∗) ≤0\nfor all i∈[m], and also choose λ∗= 0, which combined with the deﬁnition of L(Equation 3) gives\nthat:\nEθ[g0 (θ)] −g0 (θ∗) ≤ϵ\nwhich is the optimality claim.\nFeasibility: Choose θ∗= θin Equation 21. By the deﬁnition of L(Equation 3):\nmax\nλ∗∈Λ\nm∑\ni=1\nλ∗\niEθ[gi(θ)] −\nm∑\ni=1\n¯λiEθ[gi(θ)] ≤ϵ\nThen by the deﬁnition of a dual norm, H ¨older’s inequality, and the assumption that\n¯λ\n\np <R:\nR\n(Eθ[g: (θ)])+\n\nq −\n¯λ\n\np\n(Eθ[g: (θ)])+\n\nq ≤ϵ\nRearranging terms gives the feasibility claim.\nLemma 6. In the context of Theorem 3, suppose that there exists a θ′∈Θ that satisﬁes all of the\nconstraints, and does so with q-norm margin γ, i.e. gi(θ′) ≤0 for all i∈[m] and ∥g: (θ′)∥q ≥γ.\nThen: ¯λ\n\np ≤ϵ+ Bg0\nγ\nwhere Bg0 ≥supθ∈Θ g0 (θ) −infθ∈Θ g0 (θ) is a bound on the range of the objective function g0.\n45COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nProof. Starting from Equation 20 (in Theorem 3), and choosing θ∗= θ′and λ∗= 0:\nϵ≥Eθ[g0 (θ)] −Eλ\n[\ng0\n(\nθ′)\n+\nm∑\ni=1\nλigi\n(\nθ′)\n]\nϵ≥Eθ\n[\ng0 (θ) −inf\nθ′∈Θ\ng0\n(\nθ′)]\n−\n(\ng0\n(\nθ′)\n−inf\nθ′∈Θ\ng0\n(\nθ′))\n+ γ\n¯λ\n\np\nϵ≥−Bg0 + γ\n¯λ\n\np\nSolving for\n¯λ\n\np yields the claim.\nTheorem 4. (Proxy-Lagrangian Sub{optimality,feasibility}) Let\nM:=\n{\nM ∈R(m+1)×(m+1) : ∀i∈[m+ 1].M:,i ∈∆m+1\n}\nbe the set of all left-stochastic (m+ 1) ×(m+ 1) matrices, and consider the “proxy-Lagrangians”\nof Equation 2 (Equation 15). Suppose that θ∈Θ and λ∈Λ are jointly distributed random variables\nsuch that:\nEθ,λ[Lθ(θ,λ)] − inf\nθ∗∈Θ\nEλ[Lθ(θ∗,λ)] ≤ϵθ (22)\nmax\nM∗∈M\nEθ,λ[Lλ(θ,M∗λ)] −Eθ,λ[Lλ(θ,λ)] ≤ϵλ\nDeﬁne ¯λ:= Eλ[λ], let (Ω,F,P) be the probability space, and deﬁne a random variable ¯θsuch that:\nPr\n{¯θ∈S\n}\n=\n∫\nθ−1(S) λ1 (x) dP(x)\n∫\nΩ λ1 (x) dP(x)\nIn words, ¯θis a version of θthat has been resampled with λ1 being treated as an importance weight.\nIn particular E¯θ\n[\nf\n(¯θ\n)]\n= Eθ,λ[λ1f(θ)] /¯λ1 for any f : Θ →R. Then ¯θis nearly-optimal:\nE¯θ\n[\ng0\n(¯θ\n)]\n≤ inf\nθ∗∈Θ:∀i∈[m].˜gi(θ∗)≤0\ng0 (θ∗) + ϵθ + ϵλ\n¯λ1\nand nearly-feasible: \n(\nE¯θ\n[\ng:\n(¯θ\n)])\n+\n\n∞\n≤ϵλ\n¯λ1\nNotice the optimality inequality is weaker than it may appear, since the comparator in this equation\nis not the optimal solution w.r.t. the constraintsgi, but rather w.r.t. theproxy constraints ˜gi.\nProof. Optimality: If we choose M∗to be the matrix with its ﬁrst row being all-one, and all other\nrows being all-zero, then Lλ(θ,M∗λ) = 0 , which shows that the ﬁrst term in the LHS of the\nsecond line of Equation 22 is nonnegative. Hence, −Eθ,λ[Lλ(θ,λ)] ≤ϵλ, so by the deﬁnition of Lλ\n(Equation 15), and the fact that ˜gi ≥gi:\nEθ,λ\n[m∑\ni=1\nλi+1˜gi(θ)\n]\n≥−ϵλ\n46OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nNotice that Lθ is linear in λ, so the ﬁrst line of Equation 22, combined with the above result and the\ndeﬁnition of Lθ (Equation 15) becomes:\nEθ,λ[λ1g0 (θ)] − inf\nθ∗∈Θ\n(\n¯λ1g0 (θ∗) +\nm∑\ni=1\n¯λi+1˜gi(θ∗)\n)\n≤ϵθ + ϵλ (23)\nChoose θ∗to be the optimal solution that satisﬁes the proxy constraints ˜g, so that ˜gi(θ∗) ≤0 for all\ni∈[m]. Hence:\nEθ,λ[λ1g0 (θ)] −¯λ1g0 (θ∗) ≤ϵθ + ϵλ\nwhich is the optimality claim.\nFeasibility: We’ll simplify our notation by deﬁning ℓ1 (θ) := 0 and ℓi+1 (θ) := gi(θ) for\ni ∈[m], so that Lλ(θ,λ) = ⟨λ,ℓ: (θ)⟩. Consider the ﬁrst term in the LHS of the second line of\nEquation 22:\nmax\nM∗∈M\nEθ,λ[Lλ(θ,M∗λ)] = max\nM∗∈M\nEθ,λ[⟨M∗λ,ℓ: (θ)⟩]\n= max\nM∗∈M\nEθ,λ\n\n\nm+1∑\ni=1\nm+1∑\nj=1\nM∗\nj,iλiℓj (θ)\n\n\n=\nm+1∑\ni=1\nmax\nM∗\n:,i∈∆m+1\nm+1∑\nj=1\nEθ,λ\n[\nM∗\nj,iλiℓj (θ)\n]\n=\nm+1∑\ni=1\nmax\nj∈[m+1]\nEθ,λ[λiℓj (θ)]\nwhere we used the fact that, sinceM∗is left-stochastic, each of its columns is a(m+ 1)-dimensional\nmultinoulli distribution. For the second term in the LHS of the second line of Equation 22, we can\nuse the fact that ℓ1 (θ) = 0:\nEθ,λ\n[m+1∑\ni=2\nλiℓi(θ)\n]\n≤\nm+1∑\ni=2\nmax\nj∈[m+1]\nEθ,λ[λiℓj (θ)]\nPlugging these two results into the second line of Equation 22, the two sums collapse, leaving:\nmax\ni∈[m+1]\nEθ,λ[λ1ℓi(θ)] ≤ϵλ\nBy the deﬁnition of ℓi, and the fact that ℓ1 = 0:\n(Eθ,λ[λ1g: (θ)])+\n\n∞\n≤ϵλ\nwhich is the feasibility claim.\n47COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nLemma 7. In the context of Theorem 4, suppose that there exists a θ′∈Θ that satisﬁes all of the\nproxy constraints with margin γ, i.e. ˜gi(θ′) ≤−γfor all i∈[m]. Then:\n¯λ1 ≥γ−ϵθ −ϵλ\nγ+ Bg0\nwhere Bg0 ≥supθ∈Θ g0 (θ) −infθ∈Θ g0 (θ) is a bound on the range of the objective function g0.\nProof. Starting from Equation 23 (in the proof of Theorem 4), and choosing θ∗= θ′:\nEθ,λ[λ1g0 (θ)] −\n(\n¯λ1g0\n(\nθ′)\n+\nm∑\ni=1\n¯λi+1˜gi\n(\nθ′)\n)\n≤ϵθ + ϵλ\nSince ˜gi(θ′) ≤−γfor all i∈[m]:\nϵθ + ϵλ ≥Eθ,λ[λ1g0 (θ)] −¯λ1g0\n(\nθ′)\n+\n(\n1 −¯λ1\n)\nγ\n≥Eθ,λ\n[\nλ1\n(\ng0 (θ) −inf\nθ′∈Θ\ng0\n(\nθ′))]\n−¯λ1\n(\ng0\n(\nθ′)\n−inf\nθ′∈Θ\ng0\n(\nθ′))\n+\n(\n1 −¯λ1\n)\nγ\n≥−¯λ1Bg0 +\n(\n1 −¯λ1\n)\nγ\nSolving for ¯λ1 yields the claim.\nAppendix B. Proofs of Existence of Sparse Equilibria\nTheorem 5. Consider a two player game, played on the compact Hausdorff spaces Θ and Λ ⊆Rm.\nImagine that the θ-player wishes to minimize Lθ : Θ ×Λ →R, and the λ-player wishes to maximize\nLλ : Θ ×Λ →R, with both of these functions being continuous in θand linear in λ. Then there\nexists a Nash equilibrium θ, λ:\nEθ[Lθ(θ,λ)] = min\nθ∗∈Θ\nLθ(θ∗,λ)\nEθ[Lλ(θ,λ)] = max\nλ∗∈Λ\nEθ[Lλ(θ,λ∗)]\nwhere θis a random variable placing nonzero probability mass on at most m+ 1 elements of Θ, and\nλ∈Λ is non-random.\nProof. There are some extremely similar (and in some ways more general) results than this in the\ngame theory literature (e.g. Bohnenblust et al., 1950; Parthasarathy, 1975), but for our particular\n(Lagrangian and proxy-Lagrangian) setting it’s possible to provide a fairly straightforward proof.\nTo begin with, Glicksberg (1952) gives that there exists a mixed strategy in the form of two\nrandom variables ˜θand ˜λ:\nE˜θ,˜λ\n[\nLθ\n(\n˜θ,˜λ\n)]\n= min\nθ∗∈Θ\nE˜λ\n[\nLθ\n(\nθ∗,˜λ\n)]\nE˜θ,˜λ\n[\nLλ\n(\n˜θ,˜λ\n)]\n= max\nλ∗∈Λ\nE˜θ\n[\nLλ\n(\n˜θ,λ∗\n)]\n48OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nSince both functions are linear in ˜λ, we can deﬁne λ:= E˜λ\n[\n˜λ\n]\n, and these conditions become:\nE˜θ\n[\nLθ\n(\n˜θ,λ\n)]\n= min\nθ∗∈Θ\nLθ(θ∗,λ) := ℓmin\nE˜θ\n[\nLλ\n(\n˜θ,λ\n)]\n= max\nλ∗∈Λ\nE˜θ\n[\nLλ\n(\n˜θ,λ∗\n)]\nLet’s focus on the ﬁrst condition. Let pϵ := Pr\n{\nLθ\n(\n˜θ,λ\n)\n≥ℓmin + ϵ\n}\n, and notice that p1/n must\nequal zero for any n ∈{1,2,... }(otherwise we would contradict the above), implying by the\ncountable additivity of measures that Pr\n{\nLθ\n(\n˜θ,λ\n)\n= ℓmin\n}\n= 1. We therefore assume henceforth,\nwithout loss of generality, that the support of ˜θ consists entirely of minimizers of Lθ(·,λ). Let\nS ⊆Θ be this support set.\nDeﬁne G :=\n{\n∇˜λLλ(θ′,λ) : θ′∈S\n}\n, and take ¯Gto be the closure of the convex hull of G.\nSince E˜θ\n[\n∇˜λLλ\n(\n˜θ,λ\n)]\n∈ ¯G ⊆Rm, we can write it as a convex combination of at most m+ 1\nextreme points of ¯G, or equivalently of m+ 1 elements of G. Hence, we can take θto be a discrete\nrandom variable that places nonzero mass on at most m+ 1 elements of S, and:\nEθ\n[\n∇˜λLλ(θ,λ)\n]\n= E˜θ\n[\n∇˜λLλ\n(\n˜θ,λ\n)]\nLinearity in λ then implies that Eθ[Lλ(θ,·)] and E˜θ\n[\nLλ\n(\n˜θ,·\n)]\nare the same function (up to a\nconstant), and therefore have the same maximizer(s). Correspondingly, θis supported on S, which\ncontains only minimizers of Lθ(·,λ) by construction.\nLemma 8. If Θ is a compact Hausdorff space and the objective, constraint and proxy constraint\nfunctions g0,g1,...,g m,˜g1,..., ˜gm are continuous, then the proxy-Lagrangian game (Equation 15)\nhas a mixed Nash equilibrium pair (θ,λ) where θis a random variable supported on at most m+ 1\nelements of Θ, and λis non-random.\nProof. Applying Theorem 5 directly would result in a support size of m+ 2, rather than the desired\nm+ 1, since Λ is (m+ 1)-dimensional. Instead, we deﬁne ˜Λ =\n{\n˜λ∈Rm\n+ :\n˜λ\n\n1\n≤1\n}\nas the\nspace containing the last mcoordinates of Λ. Then we can rewrite the proxy-Lagrangian functions\n˜Lθ, ˜Lλ : Θ ×˜Λ →R as:\n˜Lθ\n(\nθ,˜λ\n)\n=\n(\n1 −\n˜λ\n\n1\n)\ng0 (θ) +\nm∑\ni=1\n˜λi˜gi(θ)\n˜Lλ\n(\nθ,˜λ\n)\n=\nm∑\ni=1\n˜λigi(θ)\nThese functions are linear in ˜λ, which is a m-dimensional space, so the conditions of Theorem 5\napply, yielding the claimed result.\n49COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nLemma 3. Let θ(1),θ(2),...,θ (T) ∈Θ be a sequence of T “candidate solutions” of Equation 2.\nDeﬁne ⃗ g0,⃗ gi ∈RT such that (⃗ g0)t = g0\n(\nθ(t))\nand (⃗ gi)t = gi\n(\nθ(t))\nfor i∈[m], and consider the\nlinear program:\nmin\np∈∆T\n⟨p,⃗ g0⟩\ns.t.∀i∈[m] .⟨p,⃗ gi⟩≤ ϵ\nwhere ∆T is the T-dimensional simplex. Then every vertex p∗of the feasible region—in particular\nan optimal one—has at most m∗+ 1 ≤m+ 1 nonzero elements, where m∗is the number of active\n⟨p∗,⃗ gi⟩≤ ϵconstraints.\nProof. The linear program contains not only the mexplicit linearized functional constraints, but also,\nsince p∈∆T, the T nonnegativity constraints pt ≥0, and the sum-to-one constraint ∑T\nt=1 pt = 1.\nSince pis T-dimensional, every vertex p∗of the feasible region must include T active constraints.\nLetting m∗ ≤m be the number of active linearized functional constraints, and accounting for\nthe sum-to-one constraint, it follows that at least T −m∗−1 nonnegativity constraints are active,\nimplying that p∗contains at most m∗+ 1 nonzero elements.\nAppendix C. Proofs of Convergence Rates\nC.1 Non-Stochastic One-Player Convergence Rates\nTheorem 6. (Mirror Descent) Let f1,f2,... : Θ →R be a sequence of convex functions that\nwe wish to minimize on a compact convex set Θ. Suppose that the “distance generating function”\nΨ : Θ →R+ is nonnegative and 1-strongly convex w.r.t. a norm∥·∥with dual norm ∥·∥∗.\nDeﬁne the step size η=\n√\nBΨ/TB2\nˇ∇, where BΨ ≥maxθ∈Θ Ψ (θ) is a uniform upper bound on\nΨ, and Bˇ∇≥\nˇ∇ft\n(\nθ(t))\n∗is a uniform upper bound on the norms of the subgradients. Suppose\nthat we perform T iterations of the following update, starting from θ(1) = argminθ∈Θ Ψ (θ):\n˜θ(t+1) =∇Ψ∗\n(\n∇Ψ\n(\nθ(t)\n)\n−ηˇ∇ft\n(\nθ(t)\n))\nθ(t+1) = argmin\nθ∈Θ\nDΨ\n(\nθ|˜θ(t+1)\n)\nwhere ˇ∇ft(θ) ∈ ∂ft(θ(t)) is a subgradient of ft at θ, and DΨ (θ|θ′) := Ψ ( θ) −Ψ (θ′) −\n⟨∇Ψ (θ′),θ −θ′⟩is the Bregman divergence associated with Ψ. Then:\n1\nT\nT∑\nt=1\nft\n(\nθ(t)\n)\n−1\nT\nT∑\nt=1\nft(θ∗) ≤2Bˇ∇\n√\nBΨ\nT\nwhere θ∗∈Θ is an arbitrary reference vector.\nProof. Mirror descent (Nemirovski and Yudin, 1983; Beck and Teboulle, 2003) dates back to 1983,\nbut this particular statement is taken from Lemma 2 of Srebro et al. (2011).\n50OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nCorollary 1. (Gradient Descent)Let f1,f2,... : Θ →R be a sequence of convex functions that we\nwish to minimize on a compact convex set Θ.\nDeﬁne the step size η= BΘ/Bˇ∇\n√\n2T, where BΘ ≥maxθ∈Θ ∥θ∥2, and Bˇ∇≥\nˇ∇ft\n(\nθ(t))\n2 is\na uniform upper bound on the norms of the subgradients. Suppose that we perform T iterations of\nthe following update, starting from θ(1) = argminθ∈Θ ∥θ∥2:\nθ(t+1) = ΠΘ\n(\nθ(t) −ηˇ∇ft\n(\nθ(t)\n))\nwhere ˇ∇ft(θ) ∈∂ft(θ(t)) is a subgradient of ft at θ, and ΠΘ projects its argument onto Θ w.r.t. the\nEuclidean norm. Then:\n1\nT\nT∑\nt=1\nft\n(\nθ(t)\n)\n−1\nT\nT∑\nt=1\nft(θ∗) ≤BΘBˇ∇\n√\n2\nT\nwhere θ∗∈Θ is an arbitrary reference vector.\nProof. Follows from taking Ψ (θ) = ∥θ∥2\n2 /2 in Theorem 6.\nCorollary 2. Let M:=\n{\nM ∈R˜m×˜m : ∀i∈[ ˜m] .M:,i ∈∆ ˜m}\nbe the set of all left-stochastic˜m×˜m\nmatrices, and let f1,f2,... : M→ R be a sequence of concave functions that we wish to maximize.\nDeﬁne the step size η =\n√\n˜mln ˜m/TB2\nˆ∇, where Bˆ∇≥\nˆ∇ft\n(\nM(t))\n∞,2\nis a uniform upper\nbound on the norms of the supergradients, and∥·∥∞,2 :=\n√∑˜m\ni=1 ∥M:,i∥2\n∞is the L∞,2 matrix norm.\nSuppose that we perform T iterations of the following update starting from the matrix M(1) with all\nelements equal to 1/˜m:\n˜M(t+1) =M(t) ⊙.exp\n(\nηˆ∇ft\n(\nM(t)\n))\nM(t+1)\n:,i = ˜M(t+1)\n:,i /\n˜M(t+1)\n:,i\n\n1\nwhere −ˆ∇ft\n(\nM(t))\n∈∂\n(\n−ft(M(t))\n)\n, i.e. ˆ∇ft\n(\nM(t))\nis a supergradient of ft at M(t), and the\nmultiplication and exponentiation in the ﬁrst step are performed element-wise. Then:\n1\nT\nT∑\nt=1\nft(M∗) −1\nT\nT∑\nt=1\nft\n(\nM(t)\n)\n≤2Bˆ∇\n√\n˜mln ˜m\nT\nwhere M∗∈M is an arbitrary reference matrix.\nProof. Deﬁne Ψ : M→ R := ˜mln ˜m+∑\ni,j∈[ ˜m] Mi,j ln Mi,j as ˜mln ˜mplus the negative Shannon\nentropy, applied to its (matrix) argument element-wise (˜mln ˜mis added to make Ψ nonnegative on\nM). As in the vector setting, the resulting mirror descent update will be (element-wise) multiplicative.\nThe Bregman divergence satisﬁes:\nDΨ\n(\nM|M′)\n=Ψ (M) −Ψ\n(\nM′)\n−\n⟨\n∇Ψ\n(\nM′)\n,M −M′⟩\n=\nM′\n1,1 −∥M∥1,1 +\n˜m∑\ni=1\nDKL\n(\nM:,i∥M′\n:,i\n)\n(24)\n51COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nwhere ∥M∥1,1 = ∑˜m\ni=1 ∥M:,i∥1 is the L1,1 matrix norm. This incidentally shows that one projects\nonto Mw.r.t. DΨ by projecting each column w.r.t. the KL divergence, i.e. by normalizing the\ncolumns.\nBy Pinsker’s inequality (applied to each column of anM ∈M):\nM −M′2\n1,2 ≤2\n˜m∑\ni=1\nDKL\n(\nM:,i∥M′\n:,i\n)\nwhere ∥M∥1,2 =\n√∑˜m\ni=1 ∥M:,i∥2\n1 is the L1,2 matrix norm. Substituting this into Equation 24, and\nusing the fact that ∥M∥1,1 = ˜mfor all M ∈M, we have that for all M,M ′∈M:\nDΨ\n(\nM|M′)\n≥1\n2\nM −M′2\n1,2\nwhich shows that Ψ is 1-strongly convex w.r.t. the L1,2 matrix norm. The dual norm of the L1,2\nmatrix norm is the L∞,2 norm , which is the last piece needed to apply Theorem 6, yielding the\nclaimed result.\nLemma 9. Let Λ := ∆ ˜m be the ˜m-dimensional simplex, deﬁne\nM:=\n{\nM ∈R˜m×˜m : ∀i∈[ ˜m] .M:,i ∈∆ ˜m}\nas the set of all left-stochastic ˜m×˜mmatrices, and take f1,f2,... : Λ →R to be a sequence of\nconcave functions that we wish to maximize.\nDeﬁne the step size η =\n√\n˜mln ˜m/TB2\nˆ∇, where Bˆ∇ ≥\nˆ∇ft\n(\nλ(t))\n∞\nis a uniform upper\nbound on the ∞-norms of the supergradients. Suppose that we perform T iterations of the following\nupdate, starting from the matrix M(1) with all elements equal to 1/˜m:\nλ(t) = ﬁxM(t)\nA(t) =\n(\nˆ∇ft\n(\nλ(t)\n))(\nλ(t)\n)T\n˜M(t+1) =M(t) ⊙.exp\n(\nηA(t)\n)\nM(t+1)\n:,i = ˜M(t+1)\n:,i /\n˜M(t+1)\n:,i\n\n1\nwhere ﬁx M is a stationary distribution of M (i.e. a λ∈Λ such that Mλ = λ—such always exists,\nsince M is left-stochastic), −ˆ∇ft\n(\nλ(t))\n∈∂\n(\n−ft(λ(t))\n)\n, i.e. ˆ∇ft\n(\nλ(t))\nis a supergradient of ft at\nλ(t), and the multiplication and exponentiation of the third step are performed element-wise. Then:\n1\nT\nT∑\nt=1\nft\n(\nM∗λ(t)\n)\n−1\nT\nT∑\nt=1\nft\n(\nλ(t)\n)\n≤2Bˆ∇\n√\n˜mln ˜m\nT\nwhere M∗∈M is an arbitrary left-stochastic reference matrix.\n52OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nProof. This algorithm is an instance of that contained in Figure 1 of Gordon et al. (2008).\nDeﬁne ˜ft(M) := ft\n(\nM(t)λ(t))\n. Observe that since ˆ∇ft\n(\nλ(t))\nis a supergradient of ft at λ(t),\nand M(t)λ(t) = λ(t):\nft\n(\n˜Mλ(t)\n)\n≤ft\n(\nM(t)λ(t)\n)\n+\n⣨\nˆ∇ft\n(\nλ(t)\n)\n, ˜Mλ(t) −M(t)λ(t)\n⟩\n≤ft\n(\nM(t)λ(t)\n)\n+ A(t) ·\n(\n˜M −M(t)\n)\nwhere the matrix product on the last line is performed element-wise. This shows that A(t) is a\nsupergradient of ˜ft at M(t), from which we conclude that the ﬁnal two steps of the update are\nperforming the algorithm of Corollary 2, so:\n1\nT\nT∑\nt=1\n˜ft(M∗) −1\nT\nT∑\nt=1\n˜ft\n(\nM(t)\n)\n≤2Bˆ∇\n√\n˜mln ˜m\nT\nwhere the Bˆ∇of Corollary 2 is a uniform upper bound on the L∞,2 matrix norms of the A(t)s.\nHowever, by the deﬁnition of A(t) and the fact that λ(t) ∈∆ ˜m, we can instead take Bˆ∇to be a\nuniform upper bound on\nˆ∇(t)\n\n∞\n. Substituting the deﬁnition of ˜ft and again using the fact that\nM(t)λ(t) = λ(t) then yields the claimed result.\nC.2 Stochastic One-Player Convergence Rates\nTheorem 7. (Stochastic Mirror Descent)Let Ψ, ∥·∥, DΨ and BΨ be as in Theorem 6, and let\nf1,f2,... : Θ →R be a sequence of convex functions that we wish to minimize on a compact convex\nset Θ.\nDeﬁne the step size η =\n√\nBΨ/TB2\nˇ∆, where Bˇ∆ ≥\nˇ∆(t)\n∗ is a uniform upper bound on\nthe norms of the stochastic subgradients. Suppose that we perform T iterations of the following\nstochastic update, starting from θ(1) = argminθ∈Θ Ψ (θ):\n˜θ(t+1) = ∇Ψ∗\n(\n∇Ψ\n(\nθ(t)\n)\n−ηˇ∆(t)\n)\nθ(t+1) = argmin\nθ∈Θ\nDΨ\n(\nθ|˜θ(t+1)\n)\nwhere E\n[ˇ∆(t) |θ(t)]\n∈∂ft(θ(t)), i.e. ˇ∆(t) is a stochastic subgradient of ft at θ(t). Then, with\nprobability 1 −δover the draws of the stochastic subgradients:\n1\nT\nT∑\nt=1\nft\n(\nθ(t)\n)\n−1\nT\nT∑\nt=1\nft(θ∗) ≤2Bˇ∇\n√\n2BΨ\n(\n1 + 16 ln1\nδ\n)\nT\nwhere θ∗∈Θ is an arbitrary reference vector.\nProof. This is nothing more than the usual transformation of a uniform regret guarantee into a\nstochastic one via the Hoeffding-Azuma inequality—we include a proof for completeness.\nDeﬁne the sequence:\n˜ft(θ) = ft\n(\nθ(t)\n)\n+\n⣨\nˇ∆(t),θ −θ(t)\n⟩\n53COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nThen applying non-stochastic mirror descent to the sequence ˜ft will result in exactly the same\nsequence of iterates θ(t) as applying stochastic mirror descent (above) to ft. Hence, by Theorem 6\nand the deﬁnition of ˜ft (notice that we can take Bˇ∇= Bˇ∆):\n1\nT\nT∑\nt=1\n˜ft\n(\nθ(t)\n)\n−1\nT\nT∑\nt=1\n˜ft(θ∗) ≤2Bˇ∇\n√\nBΨ\nT\n1\nT\nT∑\nt=1\nft\n(\nθ(t)\n)\n−1\nT\nT∑\nt=1\nft(θ∗) ≤2Bˇ∇\n√\nBΨ\nT + 1\nT\nT∑\nt=1\n(\n˜ft(θ∗) −ft(θ∗)\n)\n≤2Bˇ∇\n√\nBΨ\nT + 1\nT\nT∑\nt=1\n⣨\nˇ∆(t) −ˇ∇ft\n(\nθ(t)\n)\n,θ∗−θ(t)\n⟩\n(25)\nwhere the last step follows from the convexity of the fts. Consider the second term on the RHS.\nObserve that, since the ˇ∆(t)s are stochastic subgradients, each of the terms in the sum is zero in\nexpectation (conditioned on the past), and the partial sums therefore form a martingale. Furthermore,\nby H¨older’s inequality:\n⣨\nˇ∆(t) −ˇ∇ft\n(\nθ(t)\n)\n,θ∗−θ(t)\n⟩\n≤\nˇ∆(t) −ˇ∇ft\n(\nθ(t)\n)\n∗\nθ∗−θ(t)\n≤4Bˇ∆\n√\n2BΨ\nthe last step because\nθ∗−θ(t)≤\nθ∗−θ(1)+\nθ(t) −θ(1)≤2 supθ∈Θ\n√\n2DΨ\n(\nθ|θ(1))\n≤\n2√2BΨ, using the fact that DΨ is 1-strongly convex w.r.t.∥·∥, and the deﬁnition of θ(1). Hence, by\nthe Hoeffding-Azuma inequality:\nPr\n{\n1\nT\nT∑\nt=1\n⣨\nˇ∆(t) −ˇ∇ft\n(\nθ(t)\n)\n,θ∗−θ(t)\n⟩\n≥ϵ\n}\n≤exp\n(\n− Tϵ2\n64BΨB2\nˇ∆\n)\nequivalently:\nPr\n\n\n\n1\nT\nT∑\nt=1\n⣨\nˇ∆(t) −ˇ∇ft\n(\nθ(t)\n)\n,θ∗−θ(t)\n⟩\n≥8Bˇ∆\n√\nBΨ ln 1\nδ\nT\n\n\n≤δ\nsubstituting this into Equation 25, and applying the inequality √a+\n√\nb ≤\n√\n2a+ 2b, yields the\nclaimed result.\nCorollary 3. (Stochastic Gradient Descent)Let f1,f2,... : Θ →R be a sequence of convex\nfunctions that we wish to minimize on a compact convex set Θ.\nDeﬁne the step size η = BΘ/Bˇ∆\n√\n2T, where BΘ ≥maxθ∈Θ ∥θ∥2, and Bˇ∆ ≥\nˇ∆(t)\n2 is a\nuniform upper bound on the norms of the stochastic subgradients. Suppose that we perform T\niterations of the following stochastic update, starting from θ(1) = argminθ∈Θ ∥θ∥2:\nθ(t+1) = ΠΘ\n(\nθ(t) −ηˇ∆(t)\n)\n54OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nwhere E\n[ˇ∆(t) |θ(t)]\n∈∂ft(θ(t)), i.e. ˇ∆(t) is a stochastic subgradient of ft at θ(t), and ΠΘ projects\nits argument onto Θ w.r.t. the Euclidean norm. Then, with probability1 −δover the draws of the\nstochastic subgradients:\n1\nT\nT∑\nt=1\nft\n(\nθ(t)\n)\n−1\nT\nT∑\nt=1\nft(θ∗) ≤2BΘBˇ∇\n√\n1 + 16 ln1\nδ\nT\nwhere θ∗∈Θ is an arbitrary reference vector.\nProof. Follows from taking Ψ (θ) = ∥θ∥2\n2 /2 in Theorem 7.\nCorollary 4. Let M:=\n{\nM ∈R˜m×˜m : ∀i∈[ ˜m] .M:,i ∈∆ ˜m}\nbe the set of all left-stochastic˜m×˜m\nmatrices, and let f1,f2,... : M→ R be a sequence of concave functions that we wish to maximize.\nDeﬁne the step size η =\n√\n˜mln ˜m/TB2\nˆ∆, where Bˆ∆ ≥\nˆ∆(t)\n\n∞,2\nis a uniform upper bound\non the norms of the stochastic supergradients, and ∥·∥∞,2 :=\n√∑˜m\ni=1 ∥M:,i∥2\n∞is the L∞,2 matrix\nnorm. Suppose that we perform T iterations of the following stochastic update starting from the\nmatrix M(1) with all elements equal to 1/˜m:\n˜M(t+1) =M(t) ⊙.exp\n(\nηˆ∆(t)\n)\nM(t+1)\n:,i = ˜M(t+1)\n:,i /\n˜M(t+1)\n:,i\n\n1\nwhere E\n[\n−ˆ∆(t) |M(t)\n]\n∈∂\n(\n−ft(M(t))\n)\n, i.e. ˆ∆(t) is a stochastic supergradient of ft at M(t),\nand the multiplication and exponentiation in the ﬁrst step are performed element-wise. Then with\nprobability 1 −δover the draws of the stochastic supergradients:\n1\nT\nT∑\nt=1\nft(M∗) −1\nT\nT∑\nt=1\nft\n(\nM(t)\n)\n≤2Bˆ∆\n√\n2 ( ˜mln ˜m)\n(\n1 + 16 ln1\nδ\n)\nT\nwhere M∗∈M is an arbitrary reference matrix.\nProof. The same reasoning as was used to prove Corollary 2 from Theorem 6 applies here (but\nstarting from Theorem 7).\nLemma 10. Let Λ := ∆ ˜m be the ˜m-dimensional simplex, deﬁne\nM:=\n{\nM ∈R˜m×˜m : ∀i∈[ ˜m] .M:,i ∈∆ ˜m}\nas the set of all left-stochastic ˜m×˜mmatrices, and take f1,f2,... : Λ →R to be a sequence of\nconcave functions that we wish to maximize.\nDeﬁne the step size η=\n√\n˜mln ˜m/TB2\nˆ∆, where Bˆ∆ ≥\nˆ∆(t)\n\n∞\nis a uniform upper bound on\nthe ∞-norms of the stochastic supergradients. Suppose that we perform T iterations of the following\n55COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nupdate, starting from the matrix M(1) with all elements equal to 1/˜m:\nλ(t) = ﬁxM(t)\nA(t) = ˆ∆(t)\n(\nλ(t)\n)T\n˜M(t+1) =M(t) ⊙.exp\n(\nηA(t)\n)\nM(t+1)\n:,i = ˜M(t+1)\n:,i /\n˜M(t+1)\n:,i\n\n1\nwhere ﬁx M is a stationary distribution of M (i.e. a λ∈Λ such that Mλ = λ—such always exists,\nsince M is left-stochastic), E\n[\n−ˆ∆(t) |λ(t)\n]\n∈∂\n(\n−ft(λ(t))\n)\n, i.e. ˆ∆(t) is a stochastic supergradient\nof ft at λ(t), and the multiplication and exponentiation of the third step are performed element-wise.\nThen with probability 1 −δover the draws of the stochastic supergradients:\n1\nT\nT∑\nt=1\nft\n(\nM∗λ(t)\n)\n−1\nT\nT∑\nt=1\nft\n(\nλ(t)\n)\n≤2Bˆ∆\n√\n2 ( ˜mln ˜m)\n(\n1 + 16 ln1\nδ\n)\nT\nwhere M∗∈M is an arbitrary left-stochastic reference matrix.\nProof. The same reasoning as was used to prove Lemma 9 from Corollary 2 applies here (but starting\nfrom Corollary 4).\nC.3 Two-Player Convergence Rates\nLemma 1. (Algorithm 1) Suppose that Λ and R are as in Theorem 1, and deﬁne B∆ ≥\nmaxt∈[T]\n∆(t)\nλ\n\n2\n. If we run Algorithm 1 with the step size ηλ := R/B∆\n√\n2T, then the result\nsatisﬁes Theorem 1 for:\nϵ= ρ+ RB∆\n√\n2\nT\nwhere ρis the error associated with the oracle Oρ.\nProof. Applying Corollary 1 to the optimization over λgives:\n1\nT\nT∑\nt=1\nL\n(\nθ(t),λ∗\n)\n−1\nT\nT∑\nt=1\nL\n(\nθ(t),λ(t)\n)\n≤BΛB∆\n√\n2\nT\nBy the deﬁnition of Oρ (Deﬁnition 1):\n1\nT\nT∑\nt=1\nL\n(\nθ(t),λ∗\n)\n− inf\nθ∗∈Θ\n1\nT\nT∑\nt=1\nL\n(\nθ∗,λ(t)\n)\n≤ρ+ BΛB∆\n√\n2\nT\nUsing the linearity of Lin λ, the fact that BΛ = R, and the deﬁnitions of ¯θand ¯λ, yields the claimed\nresult.\n56OPTIMIZATION WITH NON-DIFFERENTIABLE CONSTRAINTS\nAlgorithm 5 Optimizes the Lagrangian formulation (Equation 3) in the convex setting. The parameter\nRis the radius of the Lagrange multiplier space Λ :=\n{\nλ∈Rm\n+ : ∥λ∥1 ≤R\n}\n, and the functions ΠΘ\nand ΠΛ project their arguments onto Θ and Λ (respectively) w.r.t. the Euclidean norm.\nStochasticLagrangian (R∈R+,L: Θ ×Λ →R,T ∈N,ηθ,ηλ ∈R+):\n1 Initialize θ(1) = 0, λ(1) = 0 // Assumes 0 ∈Θ\n2 For t∈[T]:\n3 Let ˇ∆(t)\nθ be a stochastic subgradient of L\n(\nθ(t),λ(t))\nw.r.t. θ\n4 Let ∆(t)\nλ be a stochastic gradient of L\n(\nθ(t),λ(t))\nw.r.t. λ\n5 Update θ(t+1) = ΠΘ\n(\nθ(t) −ηθˇ∆(t)\nθ\n)\n// Projected SGD updates . . .\n6 Update λ(t+1) = ΠΛ\n(\nλ(t) + ηλ∆(t)\nλ\n)\n// . . .\n7 Return θ(1),...,θ (T) and λ(1),...,λ (T)\nLemma 11. (Algorithm 5)Suppose that Θ is a compact convex set, Λ and Rare as in Theorem 1,\nand that the objective and constraint functions g0,g1,...,g m are convex. Deﬁne the three upper\nbounds BΘ ≥maxθ∈Θ ∥θ∥2, Bˇ∆ ≥maxt∈[T]\nˇ∆(t)\nθ\n\n2\n, and B∆ ≥maxt∈[T]\n∆(t)\nλ\n\n2\n.\nIf we run Algorithm 5 with the step sizes ηθ := BΘ/Bˇ∆\n√\n2T and ηλ := R/B∆\n√\n2T, then the\nresult satisﬁes the conditions of Theorem 1 for:\nϵ= 2 (BΘBˇ∆ + RB∆)\n√\n1 + 16 ln2\nδ\nT\nwith probability 1 −δover the draws of the stochastic (sub)gradients.\nProof. Applying Corollary 3 to the two optimizations (over θ and λ) gives that with probability\n1 −2δ′over the draws of the stochastic (sub)gradients:\n1\nT\nT∑\nt=1\nL\n(\nθ(t),λ(t)\n)\n−1\nT\nT∑\nt=1\nL\n(\nθ∗,λ(t)\n)\n≤2BΘBˇ∆\n√\n1 + 16 ln 1\nδ′\nT\n1\nT\nT∑\nt=1\nL\n(\nθ(t),λ∗\n)\n−1\nT\nT∑\nt=1\nL\n(\nθ(t),λ(t)\n)\n≤2BΛB∆\n√\n1 + 16 ln 1\nδ′\nT\nAdding these inequalities, taking δ = 2δ′, using the linearity of Lin λ, the fact that BΛ = R, and\nthe deﬁnitions of ¯θand ¯λ, yields the claimed result.\nLemma 4. (Algorithm 2)Suppose that Mand Λ are as in Theorem 2, and deﬁne the upper bound\nB∆ ≥maxt∈[T]\n∆(t)\nλ\n\n∞\n.\nIf we run Algorithm 2 with the step size ηλ :=\n√\n(m+ 1) ln (m+ 1)/TB2\n∆, then the result\nsatisﬁes satisﬁes the conditions of Theorem 2 for:\nϵθ =ρ\nϵλ =2B∆\n√\n(m+ 1) ln (m+ 1)\nT\n57COTTER , JIANG , WANG , NARAYAN , GUPTA , YOU, AND SRIDHARAN\nwhere ρis the error associated with the oracle Oρ.\nProof. Applying Lemma 9 to the optimization over λ(with ˜m:= m+ 1) gives:\n1\nT\nT∑\nt=1\nLλ\n(\nθ(t),M∗λ(t)\n)\n−1\nT\nT∑\nt=1\nLλ\n(\nθ(t),λ(t)\n)\n≤2B∆\n√\n(m+ 1) ln (m+ 1)\nT\nBy the deﬁnition of Oρ (Deﬁnition 1):\n1\nT\nT∑\nt=1\nLθ\n(\nθ(t),λ(t)\n)\n− inf\nθ∗∈Θ\n1\nT\nT∑\nt=1\nLθ\n(\nθ∗,λ(t)\n)\n≤ρ\nUsing the deﬁnitions of ¯θand ¯λyields the claimed result.\nLemma 5. (Algorithm 3) Suppose that Θ is a compact convex set, Mand Λ are as in Theo-\nrem 2, and that the objective and proxy constraint functions g0,˜g1,..., ˜gm are convex (but not\ng1,...,g m). Deﬁne the three upper bounds BΘ ≥maxθ∈Θ ∥θ∥2, Bˇ∆ ≥maxt∈[T]\nˇ∆(t)\nθ\n\n2\n, and\nB∆ ≥maxt∈[T]\n∆(t)\nλ\n\n∞\n.\nIf we run Algorithm 3 with the step sizesηθ := BΘ/Bˇ∆\n√\n2T and ηλ :=\n√\n(m+ 1) ln (m+ 1)/TB2\n∆,\nthen the result satisﬁes the conditions of Theorem 2 for:\nϵθ =2BΘBˇ∆\n√\n1 + 16 ln2\nδ\nT\nϵλ =2B∆\n√\n2 (m+ 1) ln (m+ 1)\n(\n1 + 16 ln2\nδ\n)\nT\nwith probability 1 −δover the draws of the stochastic (sub)gradients.\nProof. Applying Corollary 3 to the optimization over θ, and Lemma 10 to that over λ(with ˜m:=\nm+ 1), gives that with probability 1 −2δ′over the draws of the stochastic (sub)gradients:\n1\nT\nT∑\nt=1\nLθ\n(\nθ(t),λ(t)\n)\n−1\nT\nT∑\nt=1\nLθ\n(\nθ∗,λ(t)\n)\n≤2BΘBˇ∆\n√\n1 + 16 ln 1\nδ′\nT\n1\nT\nT∑\nt=1\nLλ\n(\nθ(t),M∗λ(t)\n)\n−1\nT\nT∑\nt=1\nLλ\n(\nθ(t),λ(t)\n)\n≤2B∆\n√\n2 (m+ 1) ln (m+ 1)\n(\n1 + 16 ln 1\nδ′\n)\nT\nTaking δ= 2δ′, and using the deﬁnitions of ¯θand ¯λ, yields the claimed result.\n58"
    }
}