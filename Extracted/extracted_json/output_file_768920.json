{
    "title": "Synthetic Dataset Generation for Adversarial Machine Learning Research",
    "content": {
        "page_content": "Synthetic Dataset Generation for Adversarial Machine Learning Research\nXiruo Liu* 1 Shibani Singh* 1 Cory Cornelius1 Colin Busho2 Mike Tan2 Anindya Paul1 Jason Martin1\nAbstract\nExisting adversarial example research focuses on\ndigitally inserted perturbations on top of exist-\ning natural image datasets. This construction of\nadversarial examples is not realistic because it\nmay be difﬁcult, or even impossible, for an at-\ntacker to deploy such an attack in the real-world\ndue to sensing and environmental effects. To\nbetter understand adversarial examples against\ncyber-physical systems, we propose approximat-\ning the real-world through simulation. In this\npaper we describe our synthetic dataset genera-\ntion tool that enables scalable collection of such\na synthetic dataset with realistic adversarial ex-\namples. We use the CARLA simulator to col-\nlect such a dataset and demonstrate simulated at-\ntacks that undergo the same environmental trans-\nforms and processing as real-world images. Our\ntools have been used to collect datasets to help\nevaluate the efﬁcacy of adversarial examples,\nand can be found at https://github.com/\ncarla-simulator/carla/pull/4992.\n1. Introduction\nDeep Neural Networks (DNN) have revolutionized the ﬁeld\nof artiﬁcial intelligence with signiﬁcant success in many\nemerging ﬁelds like computer vision and natural language\nprocessing. As the production and deployment of DNN\nmodels are on the rise in security and safety critical applica-\ntions, numerous studies have shown that DNN models are\nsusceptible to adversarial examples (Szegedy et al., 2018;\nCarlini & Wagner, 2017; Goodfellow et al., 2015; Papernot\net al., 2016).\nIn the computer vision domain, although the robustness of\ndefense schemes are often analyzed through digital pertur-\nbation attacks, these attacks are very difﬁcult to deploy in\n1Intel Corporation, Hillsboro, OR, USA 2The MITRE Cor-\nporation, McLean, V A, USA. Correspondence to: Xiruo Liu\n<xiruo.liu@intel.com>.\nProceedings of the39 th International Conference on Machine\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-\nright 2022 by the author(s).\n(a) Green Patch in CARLA\n (b) Digital Patch\n(c) DAPRICOT Patch\n (d) Rendered Adversarial Patch\nFigure 1.A comparison of patch insertion methods. (b) shows a\ndigital adversarial patch that is composed directly onto the image\nand obviously does not blend into the surrounding environment.\nIn (c), a DAPRICOT (Threet et al., 2021) patch is crafted to mimic\nenvironmental effects, but artifacts are still noticeable. In (a),\na green patch is ﬁrst inserted into CARLA, then an adversarial\ntexture is streamed onto it as shown in (d). Hence, this patch is\nrendered by CARLA in the same way as other objects in the scene.\npractice as they manipulate images directly after the inputs\nhave been captured and digitized inside the system. Digital\ninsertion attacks are not realistic as they do not account\nfor real-world environmental transforms, such as shading,\nlighting, occlusions and sensor noise. Figure 1 shows an\nexemplar comparison between the realism of three types of\npatch attacks. Other work has shown that physical adversar-\nial examples need to be robust against multi-distance and\nmulti-perspective view of the camera in order to have an\nimpact in a real-world scenario (Lu et al., 2017). Hence, it\nis not a fair evaluation for many defense schemes, if any at\nall, with these types of digital attacks.\nWe propose a simulation tool that is able to generate large\nscale synthetic datasets with realistic adversarial examples.\nBy leveraging the capabilities of CARLA (Dosovitskiy et al.,\n2017), our simulated datasets have the following distinct\nattributes: multi-modal sensory measurements, instance seg-\narXiv:2207.10719v1  [cs.CV]  21 Jul 2022Synthetic Dataset Generation for Adversarial Machine Learning Research\nmentation labels, diverse backgrounds with varying weather\nconditions, and multi-perspective views of the scenes.\n2. Related Work\nThere has been a signiﬁcant amount of literature on gen-\nerating synthetic datasets for various applications, includ-\ning autonomous driving through diverse scenes with syn-\nthetic pedestrians and objects (Geiger et al., 2012; Cordts\net al., 2016; Richter et al., 2016). For example, Sim 200k\n(Johnson-Roberson et al., 2017) dataset consists of 200k\nsynthetic images of vehicles driving in different times of the\nday under diverse weather and lighting conditions. A lot of\nrecent work relies on synthetic data to create benchmarks, or\nto compensate for the lack of training data with data augmen-\ntation. SynDataGeneration (Dwibedi et al., 2017) cut and\npaste object patches in scenes to create images thus focusing\non patch-level realism. This technique requires preexisting\nobject masks which are generated using pixel-objectness\nmodels with bi-linear pooling.\nPrior work (Brown et al., 2017) have shown that it is possi-\nble to generate scene-independent adversarial patches which\ncan be placed anywhere and is able to alter the behaviour of\nthe targeted model. Generating patches, which work under\nvarious lighting conditions and transfer to different target\nmodels, is a computationally intensive optimization process.\nMore recent work (Pintor et al., 2022) released a small ad-\nversarial patch dataset for fast robustness evaluation, where\npatches are applied with translation and rotation and com-\nposed onto images from ImageNet. Imagenet-C (Hendrycks\n& Dietterich, 2019) and WILDS (Koh et al., 2021) datasets\naddress distribution shifts in the dataset. Imagenet-C has im-\nage corruptions (e.g., blurring, jitter) and WILDS addresses\ndomain generalization and sub-population shifts.\nAPRICOT open source dataset (Braunegg et al., 2020) was\nthe ﬁrst step to provide a benchmark to evaluate the ro-\nbustness of object detection models against realistic attacks.\nAPRICOT photographed printed adversarial patches on real\nobjects and scenes in real-world environments. However,\nthere are several drawbacks of this dataset: not every object\nin the image is labelled; there is no complexity to the attacks\nsince the attacks are static and not adaptive in nature; not\nall patches work against all models. Moreover, this manual\ndata collection is time-consuming, not conﬁgurable to new\nenvironmental factors and does not capture multi-modal\ninputs beside RGB images. Dynamic APRICOT (DAPRI-\nCOT) (Threet et al., 2021) is one step further towards a\nrealistic adversarial dataset using CARLA with customized\nassets. Customized green screen patches, which serve as\nthe place holders for adversarial patches, are inserted into\nthe scene. Then, color transforms these patches undergo are\ntaken and used for adjusting adversarial patches such that\nthe adversarial patches blend into the simulated environment\nmore realistically. While a step closer to realistic attacks\nleveraging simulation, this attack cannot map textures to 3D\nobjects. Prior work (Cornelius et al., 2019) showed how\nto use CARLA to demonstrate the efﬁcacy of ShapeShifter\n(Chen et al., 2018) attacks against driving scenarios. That\nwork also highlighted the importance of validating realistic\nattacks by reproducing scenarios across varying environ-\nmental conditions, which CARLA already supports.\nThere are other advancements of placing adversarial at-\ntacks physically and capturing images in different weather\nconditions. For example, with the sign embedding attack\n(Sitawarin et al., 2018), trafﬁc signs are modiﬁed to improve\nrobustness to noisy transformations happened during the im-\nage capture stage. This attack also exempliﬁes the difﬁculty\nin creating datasets with realistic attacks.\nPrivacy concerns also raise for real-world datasets collected\nfrom public places, especially for tasks like pedestrian detec-\ntion and tracking. To address privacy concerns, generating\na large diverse synthetic dataset using a rendering game\nengine is explored in (Fabbri et al., 2021). The dataset has\ntemporally consistent bounding boxes, instance segmenta-\ntion, depth maps and pose occlusion information along with\nvaried environment conditions, camera viewpoints, object\ntextures, lighting conditions, weather, seasonal changes, and\nobject identities.\n3. Motivation\nWhile aforementioned datasets maybe useful for the pur-\nposes of augmenting benign datasets, there is an imminent\nneed for frameworks that enable the exploration of realistic\nadversarial attacks with varying threat models.\nWe deﬁne realistic attacksas those attacks that use the same\nend-to-end processing pipeline as the defense models. In\nFigure 2, the complete pipeline for image classiﬁcation\ntasks typically includes four stages. Attacks may come in\nat different stages. For example, digital attacks such as\nProjected Gradient Descent (PGD) (Madry et al., 2017) are\ninjected at the ﬁnal recognition stage, while DAPRICOT\nattacks are performed at preprocessing stage. Physical at-\ntacks and static physical attacks are injected at the very\nfront and applied to real-world objects, which makes them\nrealistic. However, these physical attacks are very expen-\nsive and hence are costly to scale. Therefore, we adopt an\nalternative approach by replacing the real-world with a sim-\nulated world created by CARLA. As a result, we can apply\nadversarial textures onto 3D objects, which then undergo\nall environmental transforms in the simulation as shown in\nFigure 2.Synthetic Dataset Generation for Adversarial Machine Learning Research\nReal Stop Sign\nCapture\nPreprocess\nRecognize\nPhysical Attack using Differentiable Rendering(e.g., ShapeShifter)\n Digital Attackw/”weak renderer”(e.g., Dynamic APRICOT)\nDigital Attack(e.g., PGD)\nStatic Physical Attack(e.g., APRICOT)\nSimulation\nAdversarial texture is applied to the target object and goes the complete 4-stage pipelineOriginal stop signStop sign after applied adversarial texture\nReal World\nFigure 2.Comparison of performing attacks in the real world ver-\nsus in the simulation. The end-to-end image classiﬁcation pipeline\nincludes four stages. As shown at the top, in the real world, a\nrealistic attack needs to be applied to a real object to undergo the\ncomplete pipeline, which makes it very challenging. However,\nwith simulation, it is convenient to apply a adversarial texture to\nthe target stop sign and force the attack to undergo all the same en-\nvironmental transformations and processing as the defense model.\n4. Data Collection Tool\nMotivated by the lack of large scale dataset with re-\nalistic adversarial examples, we developed a scalable\ndata collection tool that is built on top of CARLA.\nWe open sourced this tool at https://github.com/\ncarla-simulator/carla/pull/4992.\nThis tool enables the collection of multimodal data (as\nshown in Figure 3) and different scenes from sensors placed\nat different locations. A wide range of scenarios can be con-\nﬁgured to collect data with varying weather patterns, times\nof day, trafﬁc, and crowd conditions. The tool provides\naccurate labels and minimizes the costs involved in gener-\nating annotations for the dataset. The annotations include\nbounding boxes, classiﬁcation labels, instance segmentation\nmasks and frame sequence order for temporal tasks.\nCARLA supports various atmospheric conditions and illu-\nmination settings with tunable conﬁgurations for position\nand color of the sun, the intensity and color of diffuse sky\nradiation, ambient occlusion, atmospheric fog, cloudiness,\nprecipitation, as well as lighting conditions by time of day.\nCARLA allows conﬁgurable placements of sensors from\ndifferent modalities, such as RGB, depth, LIDAR, RADAR,\nFigure 3.Multimodality is well supported in CARLA, while it may\nbe very difﬁcult to obtain synced data from different sensors in a\nreal-world dataset. From left to right, the depth image, the RGB\nimage and the ground truth (i.e., instance segmentation masks)\nprovide well aligned data for research that explores the relations\namong different modalities.\noptical ﬂow, event based dynamic vision sensor and ground-\ntruth sensors (instance segmentation and semantic segmen-\ntation). Taking advantage of these features from CARLA,\nwe developed a data collection tool, which supports various\nconﬁguration of synchronized sensor suites, weather and\nlighting conditions, trafﬁc and crowd settings.\nThe tool consists of two major components: a data saver\nand an annotator. The data saver takes a conﬁguration ﬁle\n(details in Listing 1) as an input and then instantiates a\nCARLA simulation accordingly. We support three kinds\nof sensor placements: statically placed sensors, sensors\nattached to vehicles or pedestrians, and sensors that move.\nThe three supported sensor movement patterns are: (1) linear\nmotion, where the sensor moves straight from the source\nposition to the destination position; (2) rotation, where the\nsensor rotates within a predeﬁned angle range; (3) jitter,\nwhere the sensor jitters randomly within a conﬁgured range.\nAll the three types of movement can be applied together to\na moving sensor. Also, jitter and rotation can be added to a\n“static” sensor too.\nThe annotator supports two types of annotation formats:\nkwcoco (Crall, 2020) and Multi Object Tracking and Seg-\nmentation (MOTS) (V oigtlaender et al., 2019). The kwcoco\nannotation format is an extension of the COCO format with\nCOCOAPI support. We choose this format to support tempo-\nral annotations that are not supported in COCO. The MOTS\nformat has two forms, text and PNG, to support object track-\ning and segmentation scenarios. Figure 4 shows a RGB\nimage and the visualization of its annotations generated by\nour tool.\nWe focus on ﬁve speciﬁc aspects that we ﬁnd lacking in\nthe existing datasets and synthetic data generation frame-\nworks: conﬁgurability with different settings via a YAML\nconﬁguration ﬁle, reproducibility of data with a seed and\nconﬁguration, the enablement of realistic attacks that go\nthrough the same environmental transforms as the defenses,\nextensibility to augment a existing dataset to accommodate\nnew requirements, ease of use with docker to bypass theSynthetic Dataset Generation for Adversarial Machine Learning Research\n(a) Scene\n (b) Annotated scene\nFigure 4.An image captured by a RGB camera from CARLA using\nthe data saver and the visualization of the generated annotations\nusing the annotator tool: (a) scene and (b) annotated scene.\ncomplexity from CARLA and Unreal Engine installations\nand simplify running simulations, and scalability to be able\nto run multiple simulations in containers simultaneously on\ndifferent GPUs.\n5. Capabilities to Enable Benign and\nAdversarial Machine Learning Research\nConﬁgurability and extensibility have been of huge impor-\ntance to the development of datasets that advance machine\nlearning research. Quite often, when attempting to deep\ndive into complex problems and decouple a set of mingled\nfactors, researchers ﬁnd out that it is very challenging to\nobtain new customized data from the real world that can\nhelp them conduct experiments and pinpoint the root cause.\nThey may have to use the initial real world dataset due to\nthe heavy cost in collecting and labelling that speciﬁc data.\nHowever, with our tool, extending an existing dataset with\ncustomized conﬁgurations to meet new research require-\nments become feasible. Because the simulation can isolate\none environmental factor at a time, researchers can deter-\nmine speciﬁc effects that lighting, weather, object orienta-\ntion, or time of day have on a model’s ability to correctly per-\nform its task by easily creating new data that only changes\na chosen environmental factor. As a result, researchers can\nhave better understanding of the model and hence improve\nthe model correspondingly. Moreover, as the data collection\nis deﬁned by conﬁgurations in our tool, it is easy to extract\nthe statistics of the dataset so that we can better understand\nthe characteristics and limitations of the models trained on\nthe dataset. And if more data is needed at a later time, it is\nalso convenient to generate more data with new customized\nconﬁgurations.\nFrom an adversarial perspective, an attacker can also enjoy\nall the beneﬁts. Just as model training may overﬁt to a given\ntraining dataset, an attack may overﬁt to a set of data as\nwell. By being able to easily create additional data and\nchange environmental factors, an attacker can generalize\nthe attack better to a given situation. Figure 5 shows an\nFigure 5.An example of how the simulation tool helps a patch\nattack generalizes better with more object tracking data. The left\nﬁgure shows that the attack overﬁts to a single pedestrian tracking\nvideo. In the middle ﬁgure, the attack improves as it is computed\nover ﬁve tracking videos. The attack shown in the right ﬁgure is\noptimized with ten diverse tracking videos and hence performs the\nbest among the three.\nexample of how a patch attack may overﬁt to an object\ntracking model in a scenario, where a child walks by a wall\nwith an adversarial patch on it. As more diverse data is\ncreated from simulations, however, the attack improves and\ngeneralizes better as it is able to attack the tracking model in\ncase of any person walking in front of the adversarial patch\non the wall from any directions.\nResearchers can test and challenge assumptions for machine\nlearning defenses and attacks, when given the ability to cre-\nate synthetic data in this manner. As an example, Figure 6\nshows a machine learning defense that assumes a static\nbackground. The ﬁgures in the ﬁrst row show the scene\nwhere an adversarial patch is placed on the background wall.\nThe ﬁgures in the second row show the defense mechanism\nof ablating the background in order to remove the patch.\nIn the static setting, it is clear that this defense scheme is\nquite effective by masking out the background. However, a\nmore advanced attack may challenge this static assumption\nand test how well the defense holds up under a different as-\nsumption. The third row of Figure 6 shows a set of images,\nwhich is a moving scene as the camera changes its trans-\nform slightly. The ﬁgures in the fourth row show how the\ndefense performs in this dynamic setting. The adversarial\npatch is not removed and the defense model fails to ablate\nthe background as expected. This experiment shows that\neven a slight shift in the background of the scene causes\nthe background ablation defense to be largely ineffective.\nThis type of study would be time consuming and expen-\nsive, if at all possible, in a real world setting. A researcher\nwould need to recollect a scene, ideally keeping most of\nthe environmental factors as similar as possible. Using the\ndata collection tool we introduced allows the recollection\nof a scene with a simple change (e.g. adding sensor jitter\nmovement) to a conﬁguration ﬁle. This allows researchers\nto better understand what makes their machine learning\ndefenses generalize better.\nAs another example, Figure 7 demonstrates how an attack’s\nefﬁcacy may vary when placed in different scenes by takingSynthetic Dataset Generation for Adversarial Machine Learning Research\nFigure 6.Generalization of the adversarial patch defense using\nbackground ablation. Figures at top two rows show that an adver-\nsarial attack on a video with a static background, and the defense\nsuccessful ablates the adversarial patch. However, as shown by\nthe ﬁgures in the bottom two rows, an attack on a tracking video\nwith dynamic background causes the defense fail to ablate the\nadversarial patch.\ninto consideration of physical constraints in the scene. An\nadversarial patch defense may attempt to locate a patch by\ndetecting anomalous natural scene statistics. The success\nof the defense largely depends on where other objects are\nlocated within the scene as well as the natural environmental\nstatistics of the scene. Figure 7 provides a comparison on\nhow a patch defense performs on low resolution images and\nhigh resolution images. As it was trained on low resolution\nimages, the defense performs well on the RGB image in\nthe left column and can locate the patch as shown in the\nother three images in the left column. However, given a\nhigh resolution RGB image in the right column, the defense\nperformance degrades as the patch is located at a wrong\nplace in the third image in the right column. While the\ndefense still identiﬁes the patch in the second image, it\nalso generates many false positives as well. These failures\nindicate potential generalizability issues of this defense that\nuses physical constraints.\nOur tool can help better generalize defenses by enabling\nresearchers to easily generate additional data with variability\nfor different scenarios. On the other hand, attackers could\nalso receive beneﬁts as they can take this information and\nuse it to their advantage. For example, an adversarial patch\nFigure 7.Generalization of adversarial patch defense using phys-\nical constraints (Feng et al., 2022). The left column includes a\nlow resolution RGB image and the visualization after applying the\ndefense, while the right column is a high resolution RGB image\nwith the visualized defenses. The ﬁrst row shows two adversarial\nimages with an adversarial patch in the top left quadrant. The\nsecond row shows masks of pixels with anomalous colors. The\nthird row shows masks in regions with high frequency contents.\nAnd the fourth row shows masks in regions with high hue and\nsaturation pixel values. This defense, which was trained on low\nresolution images, performs well on the left column images as it is\nable to identify the patch location. However, it does not generalize\nwell on high resolution images as shown in the right column where\nthe patch is located at wrong position or false positives occur.\ncan be placed in a location that better “blends in” to the\nscene. Or the patch may be optimized to more closely\nmatch the current environment where it is placed.\nMultimodal machine learning is one of the most vibrant\nresearch areas that aims to process and learn related infor-\nmation from different modalities. As it is similar to how\nhumans sense and learn the world, multimodal learning\nshows increasing importance and extraordinary potential.\nHowever, a big challenge in this research area is the lack\nof high quality, well aligned data from different modalities.\nIt is very difﬁcult to obtain real-world datasets with data\ncaptured from different types of sensors at the exact same\nlocation and perspective. On the other side, it is easy to gen-\nerate a multimodal synthetic dataset using CARLA, which\ncurrently supports RGB camera, depth camera, optical ﬂow\ncamera, LIDAR and RADAR sensors. Figure 3 shows an\nexample where a depth camera and a RGB camera are per-Synthetic Dataset Generation for Adversarial Machine Learning Research\nfectly aligned (i.e., placed at the exact same location with\nthe same rotation) and share the same ground truth.\nIn the adversarial machine learning domain, digital pertur-\nbation attacks, such as PGD, are widely used for evaluating\nthe robustness of models. However, these attacks are of-\nten too powerful in the sense that they directly attack the\nimage pixels after the image has been captured and hence\ndo not undergo the environmental transforms and the pro-\ncessing pipeline that a defense scheme needs to consider for\naccuracy and robustness.\nWe show a comparison between varying methods of insert-\ning an adversarial patch into a scene captured by CARLA\nin Figure 1. The digital patch in Figure 1(b) is composed\ndirectly onto the image, which bypasses all the environmen-\ntal transforms. Note that there are no shadows on the patch\neven though it is located in the shadowy area of the sidewalk.\nFigure 1(c) shows a DAPRICOT patch that estimates the\nenvironmental transforms and calculates a color correction\nto be applied to the patch before digitally inserting into the\nscene. Finally, using CARLA, a green patch in Figure 1(a)\nis ﬁrst inserted into the scene as a place holder. Then the ad-\nversarial patch is applied as a texture to the designated area\nin the simulation, and then rendered in CARLA as shown\nin Figure 1(d). This approach makes the patch look very\nrealistic as it takes the environmental transforms (such as\nshadow, lighting, reﬂections) and applies them to the patch\nin the same way as other objects in the scene. To evaluate\nthe robustness of machine learning models fairly and accu-\nrately, it is critical to perform realistic attacks that undergo\nthe complete processing pipeline as normal models would\ntake. Our synthetic dataset generation has the capability to\nforce the attacker to consider the effects of environmental\ntransformations. As a result, we are able to provide an even\nplayground for both the attacker and the defender.\nMoreover, from the attacker’s perspective, in real life it is\nvery expensive and slow to compute and apply a physical\nadversarial perturbation. However, with the simulation tool,\nit is much faster to iterate the perturbation computations\nand evaluate the effectiveness of the realistic attack. The\ntool also allows an attacker to choose any suitable technique\nto craft adversarial examples for speciﬁc scenes. Figure 8\nshows an example of validating a ShapeShifter (Chen et al.,\n2018) attack, where an adversarial texture is computed and\nthen applied to a Tesla Model 3 car in CARLA. This attack\nis successful as it causes the vehicle to be misclassiﬁed as\nan orange. Without simulation, to validate this physically re-\nalizable attack, a car needs to be painted with the computed\nadversarial texture to validate the effectiveness of the attack\nin the real world. Even worse, the attack optimization may\ngo through this paint-validation-repaint process multiple\ntimes, which makes it very expensive to conduct realistic\nattack research in real world. On contrary, our simulation\ntool brings signiﬁcant beneﬁts for developing realistic attack\ntechniques, as it saves the heavy overhead of creating real\nadversarial objects while at the same time ensures the attack\nundergoes the same environmental transformations as the\ndefense.\nFigure 8.Validation of a ShapeShifter attack in CARLA. An adver-\nsarial texture is computed and then applied to a Tesla Model 3 car\nin the simulation. This attack is successful as it causes the vehicle\nto be misclassiﬁed as an orange.\nHence, our tool provides a cheap and convenient alternative\nto the real-world adversarial dataset generation, which has\nan prohibiting cost for realistic adversarial examples and\nthus is very difﬁcult to be of large scale, if even possible.\nWith the simulation tool, it becomes much easier to insert\n2D or 3D adversarial perturbations back into the simula-\ntion. And due to its low cost and low time consumption,\ngenerating a large scale synthetic adversarial dataset with\nrealistic attack samples becomes feasible. Even further, as\ndiscussed in Figure 1, streaming texture capability provided\nby CARLA, which allows the texture of an object to be\nchanged in real time during the simulation, opens an door\nfor the adversarial training with realistic adversarial noises.\n6. Conclusion\nAdversarial machine learning research suffers from a lack\nof large scale, high quality datasets with realistic, physically\nplausible adversarial examples. We overcome these limita-\ntions by using simulation to insert adversarial examples and\ngenerate synthetic data. Our tools enables a new way for\ndistributing datasets that are reproducible, conﬁgurable, ex-\ntensible, and scalable. Rather than downloading large, static\narchives, researchers can distribute a dataset in the form\nof conﬁguration ﬁles and locally re-create the dataset by\nrunning our tools and CARLA. They can also easily extend\nthe dataset with new conﬁgurations based on their speciﬁc\nneeds and dynamically insert adversarial examples in a real-\nistic manner. In future work, we would like to conduct large\nscale studies on the efﬁcacy of attacks from various sources\nsuch as those shown in Figure 1. We believe this tool will\nenable more realistic evaluation of adversarial examples.Synthetic Dataset Generation for Adversarial Machine Learning Research\nAcknowledgements\nThe work contained herein is developed under the DARPA\nGuaranteeing AI Robustness against Deception (GARD)\nprogram. The views, opinions and/or ﬁndings contained in\nthis report are those of The MITRE Corporation and should\nnot be construed as an ofﬁcial government position, policy,\nor decision, unless designated by other documentation.\nApproved for Public Release by MITRE; Distribution Un-\nlimited. Public Release Case Number 22-2197. This tech-\nnical data deliverable was developed using contract funds\nunder MITRE Basic Contract No. W56KGU-18-D-0004.\n©2022 The MITRE Corporation. All rights reserved.\nReferences\nBraunegg, A., Chakraborty, A., Krumdick, M., Lape, N.,\nLeary, S., Manville, K., Merkhofer, E., Strickhart, L., and\nWalmer, M. APRICOT: A dataset of physical adversarial\nattacks on object detection. In European Conference on\nComputer Vision, pp. 35–50. Springer, 2020.\nBrown, T., Mane, D., Roy, A., Abadi, M., and Gilmer,\nJ. Adversarial patch. 2017. URL https://arxiv.\norg/pdf/1712.09665.pdf.\nCarlini, N. and Wagner, D. Towards evaluating the ro-\nbustness of neural networks. In 2017 IEEE Symposium\non Security and Privacy (SP), pp. 39–57, 2017. doi:\n10.1109/SP.2017.49.\nChen, S.-T., Cornelius, C., Martin, J., and Chau, D. H. P.\nShapeshifter: Robust physical adversarial attack on Faster\nR-CNN object detector. In Joint European Confer-\nence on Machine Learning and Knowledge Discovery\nin Databases, pp. 52–68. Springer, 2018.\nCordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler,\nM., Benenson, R., Franke, U., Roth, S., and Schiele, B.\nThe cityscapes dataset for semantic urban scene under-\nstanding. In Proc. of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2016.\nCornelius, C., Chen, S.-T., Martin, J., and Chau, D. H.\nTalk proposal: Towards the realistic evaluation of evasion\nattacks using CARLA. ArXiv, abs/1904.12622, 2019.\nCrall, J. Kitware coco, 2020. URL https://gitlab.\nkitware.com/computer-vision/kwcoco.\nDosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and\nKoltun, V . CARLA: An open urban driving simulator.\nIn Proceedings of the 1st Annual Conference on Robot\nLearning, pp. 1–16, 2017.\nDwibedi, D., Misra, I., and Hebert, M. Cut, paste and\nlearn: Surprisingly easy synthesis for instance detection.\nIn Proceedings of the IEEE international conference on\ncomputer vision, pp. 1301–1310, 2017.\nFabbri, M., Bras´o, G., Maugeri, G., Cetintas, O., Gasparini,\nR., Oˇsep, A., Calderara, S., Leal-Taix´e, L., and Cucchiara,\nR. MOTSynth: How can synthetic data help pedestrian\ndetection and tracking? In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 10849–\n10859, 2021.\nFeng, R., Jha, S., and Prakash, A. Constraining the attack\nspace of machine learning models with distribution clamp-\ning preprocessing. arXiv preprint arXiv:2205.08989,\n2022.\nGeiger, A., Lenz, P., and Urtasun, R. Are we ready\nfor autonomous driving? the kitti vision benchmark\nsuite. In Computer Vision and Pattern Recognition\n(CVPR), 2012 IEEE Conference on, pp. 3354–3361.\nIEEE, 2012. URL https://ieeexplore.ieee.\norg/abstract/document/6248074.\nGoodfellow, I., Shlens, J., and Szegedy, C. Explaining\nand harnessing adversarial examples. In International\nConference on Learning Representations, 2015. URL\nhttp://arxiv.org/abs/1412.6572.\nHendrycks, D. and Dietterich, T. Benchmarking neural\nnetwork robustness to common corruptions and perturba-\ntions. arXiv preprint arXiv:1903.12261, 2019.\nJohnson-Roberson, M., Barto, C., Mehta, R., Sridhar, S. N.,\nRosaen, K., and Vasudevan, R. Driving in the matrix: Can\nvirtual worlds replace human-generated annotations for\nreal world tasks? In 2017 IEEE International Conference\non Robotics and Automation (ICRA), pp. 746–753, 2017.\ndoi: 10.1109/ICRA.2017.7989092.\nKoh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang,\nM., Balsubramani, A., Hu, W., Yasunaga, M., Phillips,\nR. L., Gao, I., et al. Wilds: A benchmark of in-the-\nwild distribution shifts. In International Conference on\nMachine Learning, pp. 5637–5664. PMLR, 2021.\nLu, J., Sibai, H., Fabry, E., and Forsyth, D. A. No need to\nworry about adversarial examples in object detection in\nautonomous vehicles. ArXiv, abs/1707.03501, 2017.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and\nVladu, A. Towards deep learning models resistant to\nadversarial attacks. arXiv preprint arXiv:1706.06083,\n2017.\nPapernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik,\nZ. B., and Swami, A. The limitations of deep learning in\nadversarial settings. In 2016 IEEE European Symposium\non Security and Privacy (EuroS P), pp. 372–387, 2016.\ndoi: 10.1109/EuroSP.2016.36.Synthetic Dataset Generation for Adversarial Machine Learning Research\nPintor, M., Angioni, D., Sotgiu, A., Demetrio, L., Demontis,\nA., Biggio, B., and Roli, F. Imagenet-Patch: A dataset\nfor benchmarking machine learning robustness against\nadversarial patches. ArXiv, abs/2203.04412, 2022.\nRichter, S. R., Vineet, V ., Roth, S., and Koltun, V . Playing\nfor data: Ground truth from computer games. ArXiv,\nabs/1608.02192, 2016.\nSitawarin, C., Bhagoji, A. N., Mosenia, A., Mittal, P., and\nChiang, M. Rogue signs: Deceiving trafﬁc sign recog-\nnition with malicious ads and logos. arXiv preprint\narXiv:1801.02780, 2018.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing properties\nof neural networks. In Joint European Conference on Ma-\nchine Learning and Knowledge Discovery in Databases,\npp. 52–68. Springer, 2018.\nThreet, M., Busho, C., Harguess, J., Jutras, M., Lape, N.,\nLeary, S., Manville, K., Tan, M., and Ward, C. Physical\nadversarial attacks in simulated environments. In 2021\nIEEE Applied Imagery Pattern Recognition Workshop\n(AIPR), pp. 1–5, 2021. doi: 10.1109/AIPR52630.2021.\n9762099.\nV oigtlaender, P., Krause, M., Osep, A., Luiten, J., Sekar, B.\nB. G., Geiger, A., and Leibe, B. Mots: Multi-object track-\ning and segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npp. 7942–7951, 2019.Synthetic Dataset Generation for Adversarial Machine Learning Research\nA. Appendix\nThe synthetic data collection tool described in Section 4 consists of two major components, a data saver and an annotator.\nThe data saver takes a conﬁguration ﬁle as input to conﬁgure the CARLA simulation scenario. This conﬁguration includes\nCARLA server conﬁgurations, weather settings, and conﬁgurations for spawning actors (i.e., vehicles and pedestrians) and\nsensors for data capture. Listing 1 shows an example of such a conﬁguration that captures 300 frames from three sensors\n(RGB camera, depth camera, and instance segmentation sensor) with two pedestrians. These sensors are statically placed to\ncapture the two pedetrians as they walk past these sensors. The two pedestrians that walk past each other are speciﬁed by\ntheir the start positions, destination positions, and walking speeds. Finally, the conﬁguration sets a seed to enable repeated\ngenerations of a particular scenario. Figure 9 shows example frames captured from three different captures under different\nweather conditions using conﬁgurations shown in Listing 1 and Listing 2. Our tool enables reproducible capture under a\nvariety of conditions like weather and lighting.\n(a) Sunny\n (b) Rainy\n (c) Foggy\nFigure 9.Our tool can repeatably collect data over a weather distribution shift. We collected frames from the same scenario for different\nweather conditions: (a) sunny, (b) rainy, and (c) foggy.\nListing 1.An example config.yaml ﬁle that generates 2 pedestrians with 3 static sensors (RGB, depth and instance segmentation).\n1 carla:\n2 host: \"127.0.0.1\"\n3 port: 2000\n4 timeout: 5.0\n5 sync:\n6 fps: 30\n7 timeout: 2.0\n8 seed: 30\n9 townmap: \"Town10HD\"\n10 traffic_manager_port: 8000\n11 retry: 10\n12\n13 output_dir: \"_out\"\n14 max_frames: 300\n15\n16 # Sunny weather\n17 weather:\n18 cloudiness: 0.0\n19 precipitation: 0.0\n20 precipitation_deposits: 0.0\n21 wind_intensity: 0.0\n22 sun_azimuth_angle: 0.0\n23 sun_altitude_angle: 10.0\n24 fog_density: 0.0\n25 fog_distance: 0.0\n26 wetness: 0.0\n27\n28 spawn_actors:\n29 # Spawn Pedestrian 1Synthetic Dataset Generation for Adversarial Machine Learning Research\n30 - blueprint:\n31 name: \"walker.pedestrian.*\"\n32 attr: {role_name: \"hero1\", is_invincible: \"false\"}\n33 speed: 1.4 # Between 1 and 2 m/s (default is 1.4 m/s).\n34 transform:\n35 location: {x: -91, y: 170, z: 0.6}\n36 rotation: {yaw: -90.0}\n37 destination_transform:\n38 location: {x: -91, y: 150, z: 0.6}\n39 # Spawn statically placed sensors - RGB, Depth and Instance Segmentation\n40 - blueprint:\n41 name: \"sensor.camera.rgb\"\n42 attr: {\"image_size_x\": \"800\", \"image_size_y\": \"600\"}\n43 transform:\n44 location: {x: -95, y: 160, z: 1.6}\n45 rotation: {yaw: 0.0}\n46 - blueprint:\n47 name: \"sensor.camera.depth\"\n48 attr: {\"image_size_x\": \"800\", \"image_size_y\": \"600\"}\n49 transform:\n50 location: {x: -95, y: 160, z: 1.6}\n51 rotation: {yaw: 0.0}\n52 - blueprint:\n53 name: \"sensor.camera.instance_segmentation\"\n54 attr: {\"image_size_x\": \"800\", \"image_size_y\": \"600\"}\n55 transform:\n56 location: {x: -95, y: 160, z: 1.6}\n57 rotation: {yaw: 0.0}\n58 # Spawn Pedestrian 2\n59 - blueprint:\n60 name: \"walker.pedestrian.*\"\n61 attr: {role_name: \"hero2\", is_invincible: \"false\"}\n62 speed: 2.0\n63 transform:\n64 location: {x: -91, y: 150, z: 0.6}\n65 rotation: {yaw: 90.0}\n66 destination_transform:\n67 location: {x: -91, y: 170, z: 0.6}\nListing 2.An example to show 2 different weather settings used to create the examples shown in Figure 9\n1 # Rainy weather\n2 weather:\n3 cloudiness: 60.0\n4 precipitation: 60.0\n5 precipitation_deposits: 60.0\n6 wind_intensity: 60.0\n7 sun_azimuth_angle: -1.0\n8 sun_altitude_angle: 15.0\n9 fog_density: 3.0\n10 fog_distance: 0.75\n11 wetness: 0.0\n12\n13 # Foggy weather\n14 weather:\n15 cloudiness: 0.0\n16 precipitation: 0.0\n17 precipitation_deposits: 0.0\n18 wind_intensity: 0.0\n19 sun_azimuth_angle: 0.0\n20 sun_altitude_angle: 10.0\n21 fog_density: 100.0\n22 fog_distance: 1.0Synthetic Dataset Generation for Adversarial Machine Learning Research\n23 wetness: 0.0"
    }
}