{
    "title": "A preliminary version of this paper appears in the proceedings of the23rd ACM Conference on Computer and Communications Security",
    "content": {
        "page_content": "A preliminary version of this paper appears in the proceedings of the23rd ACM Conference on Computer and Communications Security\n(CCS 2016). This is a full version.\nDeep Learning with Differential Privacy\nOctober 25, 2016\nMartín Abadi∗ Andy Chu∗ Ian Goodfellow†\nH. Brendan McMahan∗ Ilya Mironov∗ Kunal Talwar∗\nLi Zhang∗\nABSTRACT\nMachine learning techniques based on neural networks are\nachieving remarkable results in a wide variety of domains.\nOften, the training of models requires large, representative\ndatasets, which may be crowdsourced and contain sensitive\ninformation. The models should not expose private informa-\ntion in these datasets. Addressing this goal, we develop new\nalgorithmic techniques for learning and a reﬁned analysis of\nprivacy costs within the framework of diﬀerential privacy.\nOur implementation and experiments demonstrate that we\ncan train deep neural networks with non-convex objectives,\nunder a modest privacy budget, and at a manageable cost in\nsoftware complexity, training eﬃciency, and model quality.\n1. INTRODUCTION\nRecent progress in neural networks has led to impressive\nsuccesses in a wide range of applications, including image\nclassiﬁcation, language representation, move selection for\nGo, and many more (e.g., [54, 28, 56, 38, 51]). These ad-\nvances are enabled, in part, by the availability of large and\nrepresentative datasets for training neural networks. These\ndatasets are often crowdsourced, and may contain sensitive\ninformation. Their use requires techniques that meet the\ndemands of the applications while oﬀering principled and\nrigorous privacy guarantees.\nIn this paper, we combine state-of-the-art machine learn-\ning methods with advanced privacy-preserving mechanisms,\ntraining neural networks within a modest (“single-digit”) pri-\nvacy budget. We treat models with non-convex objectives,\nseveral layers, and tens of thousands to millions of param-\neters. (In contrast, previous work obtains strong results on\nconvex models with smaller numbers of parameters, or treats\ncomplex neural networks but with a large privacy loss.) For\nthis purpose, we develop new algorithmic techniques, a re-\nﬁned analysis of privacy costs within the framework of dif-\nferential privacy, and careful implementation strategies:\n∗Google.\n†OpenAI. Work done while at Google.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCCS’16 October 24-28, 2016, Vienna, Austria\nc⃝2016 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-4139-4/16/10.\nDOI: http://dx.doi.org/10.1145/2976749.2978318\n1. We demonstrate that, by tracking detailed information\n(higher moments) of the privacy loss, we can obtain\nmuch tighter estimates on the overall privacy loss, both\nasymptotically and empirically.\n2. We improve the computational eﬃciency of diﬀeren-\ntially private training by introducing new techniques.\nThese techniques include eﬃcient algorithms for com-\nputing gradients for individual training examples, sub-\ndividing tasks into smaller batches to reduce memory\nfootprint, and applying diﬀerentially private principal\nprojection at the input layer.\n3. We build on the machine learning framework Tensor-\nFlow [3] for training models with diﬀerential privacy.\nWe evaluate our approach on two standard image clas-\nsiﬁcation tasks, MNIST and CIFAR-10. We chose\nthese two tasks because they are based on public data-\nsets and have a long record of serving as benchmarks\nin machine learning. Our experience indicates that\nprivacy protection for deep neural networks can be\nachieved at a modest cost in software complexity, train-\ning eﬃciency, and model quality.\nMachine learning systems often comprise elements that\ncontribute to protecting their training data. In particular,\nregularization techniques, which aim to avoid overﬁtting to\nthe examples used for training, may hide details of those\nexamples. On the other hand, explaining the internal rep-\nresentations in deep neural networks is notoriously diﬃcult,\nand their large capacity entails that these representations\nmay potentially encode ﬁne details of at least some of the\ntraining data. In some cases, a determined adversary may\nbe able to extract parts of the training data. For example,\nFredrikson et al. demonstrated a model-inversion attack that\nrecovers images from a facial recognition system [24].\nWhile the model-inversion attack requires only “black-\nbox” access to a trained model (that is, interaction with the\nmodel via inputs and outputs), we consider adversaries with\nadditional capabilities, much like Shokri and Shmatikov [50].\nOur approach oﬀers protection against a strong adversary\nwith full knowledge of the training mechanism and access\nto the model’s parameters. This protection is attractive,\nin particular, for applications of machine learning on mobile\nphones, tablets, and other devices. Storing models on-device\nenables power-eﬃcient, low-latency inference, and may con-\ntribute to privacy since inference does not require commu-\nnicating user data to a central server; on the other hand,\nwe must assume that the model parameters themselves may\nbe exposed to hostile inspection. Furthermore, when we are\narXiv:1607.00133v2  [stat.ML]  24 Oct 2016concerned with preserving the privacy of one record in the\ntraining data, we allow for the possibility that the adversary\ncontrols some or even all of the rest of the training data. In\npractice, this possibility cannot always be excluded, for ex-\nample when the data is crowdsourced.\nThe next section reviews background on deep learning and\non diﬀerential privacy. Sections 3 and 4 explain our ap-\nproach and implementation. Section 5 describes our exper-\nimental results. Section 6 discusses related work, and Sec-\ntion 7 concludes. Deferred proofs appear in the Appendix.\n2. BACKGROUND\nIn this section we brieﬂy recall the deﬁnition of diﬀerential\nprivacy, introduce the Gaussian mechanism and composition\ntheorems, and overview basic principles of deep learning.\n2.1 Differential Privacy\nDiﬀerential privacy [19, 16, 20] constitutes a strong stan-\ndard for privacy guarantees for algorithms on aggregate data-\nbases. It is deﬁned in terms of the application-speciﬁc con-\ncept of adjacent databases. In our experiments, for instance,\neach training dataset is a set of image-label pairs; we say\nthat two of these sets are adjacent if they diﬀer in a single\nentry, that is, if one image-label pair is present in one set\nand absent in the other.\nDeﬁnition 1. A randomized mechanism M: D→R with\ndomain Dand range Rsatisﬁes (ε,δ)-diﬀerential privacy if\nfor any two adjacent inputs d,d′∈D and for any subset of\noutputs S ⊆R it holds that\nPr[M(d) ∈S] ≤eεPr[M(d′) ∈S] + δ.\nThe original deﬁnition of ε-diﬀerential privacy does not in-\nclude the additive term δ. We use the variant introduced by\nDwork et al. [17], which allows for the possibility that plain\nε-diﬀerential privacy is broken with probability δ (which is\npreferably smaller than 1 /|d|).\nDiﬀerential privacy has several properties that make it\nparticularly useful in applications such as ours: composabil-\nity, group privacy, and robustness to auxiliary information.\nComposability enables modular design of mechanisms: if all\nthe components of a mechanism are diﬀerentially private,\nthen so is their composition. Group privacy implies graceful\ndegradation of privacy guarantees if datasets contain cor-\nrelated inputs, such as the ones contributed by the same\nindividual. Robustness to auxiliary information means that\nprivacy guarantees are not aﬀected by any side information\navailable to the adversary.\nA common paradigm for approximating a deterministic\nreal-valued function f: D→ R with a diﬀerentially private\nmechanism is via additive noise calibrated to f’s sensitivity\nSf, which is deﬁned as the maximum of the absolute distance\n|f(d) −f(d′)|where d and d′ are adjacent inputs. (The\nrestriction to a real-valued function is intended to simplify\nthis review, but is not essential.) For instance, the Gaussian\nnoise mechanism is deﬁned by\nM(d)\n∆\n= f(d) + N(0,S2\nf ·σ2),\nwhere N(0,S2\nf ·σ2) is the normal (Gaussian) distribution\nwith mean 0 and standard deviation Sfσ. A single applica-\ntion of the Gaussian mechanism to function f of sensitivity\nSf satisﬁes (ε,δ)-diﬀerential privacy if δ≥4\n5 exp(−(σε)2/2)\nand ε <1 [20, Theorem 3.22]. Note that this analysis of\nthe mechanism can be applied post hoc, and, in particular,\nthat there are inﬁnitely many ( ε,δ) pairs that satisfy this\ncondition.\nDiﬀerential privacy for repeated applications of additive-\nnoise mechanisms follows from the basic composition theo-\nrem [17, 18], or from advanced composition theorems and\ntheir reﬁnements [22, 32, 21, 10]. The task of keeping track\nof the accumulated privacy loss in the course of execution\nof a composite mechanism, and enforcing the applicable pri-\nvacy policy, can be performed by the privacy accountant,\nintroduced by McSherry [40].\nThe basic blueprint for designing a diﬀerentially private\nadditive-noise mechanism that implements a given function-\nality consists of the following steps: approximating the func-\ntionality by a sequential composition of bounded-sensitivity\nfunctions; choosing parameters of additive noise; and per-\nforming privacy analysis of the resulting mechanism. We\nfollow this approach in Section 3.\n2.2 Deep Learning\nDeep neural networks, which are remarkably eﬀective for\nmany machine learning tasks, deﬁne parameterized func-\ntions from inputs to outputs as compositions of many layers\nof basic building blocks, such as aﬃne transformations and\nsimple nonlinear functions. Commonly used examples of the\nlatter are sigmoids and rectiﬁed linear units (ReLUs). By\nvarying parameters of these blocks, we can “train” such a pa-\nrameterized function with the goal of ﬁtting any given ﬁnite\nset of input/output examples.\nMore precisely, we deﬁne a loss function Lthat represents\nthe penalty for mismatching the training data. The lossL(θ)\non parameters θ is the average of the loss over the training\nexamples {x1,...,x N}, so L(θ) = 1\nN\n∑\niL(θ,xi). Training\nconsists in ﬁnding θ that yields an acceptably small loss,\nhopefully the smallest loss (though in practice we seldom\nexpect to reach an exact global minimum).\nFor complex networks, the loss function Lis usually non-\nconvex and diﬃcult to minimize. In practice, the minimiza-\ntion is often done by the mini-batch stochastic gradient de-\nscent (SGD) algorithm. In this algorithm, at each step,\none forms a batch B of random examples and computes\ngB = 1 /|B|∑\nx∈B∇θL(θ,x) as an estimation to the gra-\ndient ∇θL(θ). Then θ is updated following the gradient\ndirection −gB towards a local minimum.\nSeveral systems have been built to support the deﬁnition\nof neural networks, to enable eﬃcient training, and then\nto perform eﬃcient inference (execution for ﬁxed parame-\nters) [29, 12, 3]. We base our work on TensorFlow, an open-\nsource dataﬂow engine released by Google [3]. TensorFlow\nallows the programmer to deﬁne large computation graphs\nfrom basic operators, and to distribute their execution across\na heterogeneous distributed system. TensorFlow automates\nthe creation of the computation graphs for gradients; it also\nmakes it easy to batch computation.\n3. OUR APPROACH\nThis section describes the main components of our ap-\nproach toward diﬀerentially private training of neural net-\nworks: a diﬀerentially private stochastic gradient descent\n(SGD) algorithm, the moments accountant, and hyperpa-\nrameter tuning.3.1 Differentially Private SGD Algorithm\nOne might attempt to protect the privacy of training data\nby working only on the ﬁnal parameters that result from the\ntraining process, treating this process as a black box. Un-\nfortunately, in general, one may not have a useful, tight\ncharacterization of the dependence of these parameters on\nthe training data; adding overly conservative noise to the pa-\nrameters, where the noise is selected according to the worst-\ncase analysis, would destroy the utility of the learned model.\nTherefore, we prefer a more sophisticated approach in which\nwe aim to control the inﬂuence of the training data during\nthe training process, speciﬁcally in the SGD computation.\nThis approach has been followed in previous works (e.g., [52,\n7]); we make several modiﬁcations and extensions, in par-\nticular in our privacy accounting.\nAlgorithm 1 outlines our basic method for training a model\nwith parameters θby minimizing the empirical loss function\nL(θ). At each step of the SGD, we compute the gradient\n∇θL(θ,xi) for a random subset of examples, clip theℓ2 norm\nof each gradient, compute the average, add noise in order to\nprotect privacy, and take a step in the opposite direction of\nthis average noisy gradient. At the end, in addition to out-\nputting the model, we will also need to compute the privacy\nloss of the mechanism based on the information maintained\nby the privacy accountant. Next we describe in more detail\neach component of this algorithm and our reﬁnements.\nAlgorithm 1 Diﬀerentially private SGD (Outline)\nInput: Examples {x1,...,x N}, loss function L(θ) =\n1\nN\n∑\niL(θ,xi). Parameters: learning rate ηt, noise scale\nσ, group size L, gradient norm bound C.\nInitialize θ0 randomly\nfor t∈[T] do\nTake a random sample Lt with sampling probability\nL/N\nCompute gradient\nFor each i∈Lt, compute gt(xi) ←∇θt L(θt,xi)\nClip gradient\n¯gt(xi) ←gt(xi)/max\n(\n1,∥gt(xi)∥2\nC\n)\nAdd noise\n˜gt ←1\nL\n(∑\ni ¯gt(xi) + N(0,σ2C2I)\n)\nDescent\nθt+1 ←θt −ηt˜gt\nOutput θT and compute the overall privacy cost ( ε,δ)\nusing a privacy accounting method.\nNorm clipping: Proving the diﬀerential privacy guarantee\nof Algorithm 1 requires bounding the inﬂuence of each indi-\nvidual example on ˜gt. Since there is no a priori bound on\nthe size of the gradients, we clip each gradient in ℓ2 norm;\ni.e., the gradient vector g is replaced by g/max\n(\n1,∥g∥2\nC\n)\n,\nfor a clipping threshold C. This clipping ensures that if\n∥g∥2 ≤C, then g is preserved, whereas if ∥g∥2 >C , it gets\nscaled down to be of norm C. We remark that gradient clip-\nping of this form is a popular ingredient of SGD for deep\nnetworks for non-privacy reasons, though in that setting it\nusually suﬃces to clip after averaging.\nPer-layer and time-dependent parameters: The pseu-\ndocode for Algorithm 1 groups all the parameters into a\nsingle input θ of the loss function L(·). For multi-layer neu-\nral networks, we consider each layer separately, which allows\nsetting diﬀerent clipping thresholds C and noise scales σ for\ndiﬀerent layers. Additionally, the clipping and noise param-\neters may vary with the number of training stepst. In results\npresented in Section 5 we use constant settings for C and σ.\nLots: Like the ordinary SGD algorithm, Algorithm 1 esti-\nmates the gradient of Lby computing the gradient of the\nloss on a group of examples and taking the average. This av-\nerage provides an unbiased estimator, the variance of which\ndecreases quickly with the size of the group. We call such a\ngroup a lot, to distinguish it from the computational group-\ning that is commonly called a batch. In order to limit mem-\nory consumption, we may set the batch size much smaller\nthan the lot size L, which is a parameter of the algorithm.\nWe perform the computation in batches, then group several\nbatches into a lot for adding noise. In practice, for eﬃciency,\nthe construction of batches and lots is done by randomly per-\nmuting the examples and then partitioning them into groups\nof the appropriate sizes. For ease of analysis, however, we as-\nsume that each lot is formed by independently picking each\nexample with probability q = L/N, where N is the size of\nthe input dataset.\nAs is common in the literature, we normalize the running\ntime of a training algorithm by expressing it as the number\nof epochs, where each epoch is the (expected) number of\nbatches required to process N examples. In our notation,\nan epoch consists of N/L lots.\nPrivacy accounting: For diﬀerentially private SGD, an\nimportant issue is computing the overall privacy cost of the\ntraining. The composability of diﬀerential privacy allows\nus to implement an “accountant” procedure that computes\nthe privacy cost at each access to the training data, and\naccumulates this cost as the training progresses. Each step\nof training typically requires gradients at multiple layers,\nand the accountant accumulates the cost that corresponds\nto all of them.\nMoments accountant: Much research has been devoted\nto studying the privacy loss for a particular noise distribu-\ntion as well as the composition of privacy losses. For the\nGaussian noise that we use, if we choose σ in Algorithm 1\nto be\n√\n2 log 1.25\nδ /ε, then by standard arguments [20] each\nstep is ( ε,δ)-diﬀerentially private with respect to the lot.\nSince the lot itself is a random sample from the database,\nthe privacy ampliﬁcation theorem [33, 8] implies that each\nstep is ( O(qε),qδ)-diﬀerentially private with respect to the\nfull database where q = L/N is the sampling ratio per lot\nand ε≤1. The result in the literature that yields the best\noverall bound is the strong composition theorem [22].\nHowever, the strong composition theorem can be loose,\nand does not take into account the particular noise distribu-\ntion under consideration. In our work, we invent a stronger\naccounting method, which we call the moments accountant.\nIt allows us to prove that Algorithm 1 is ( O(qε\n√\nT),δ)-\ndiﬀerentially private for appropriately chosen settings of the\nnoise scale and the clipping threshold. Compared to what\none would obtain by the strong composition theorem, our\nbound is tighter in two ways: it saves a\n√\nlog(1/δ) factor in\nthe ε part and a Tq factor in the δ part. Since we expect\nδ to be small and T ≫1/q (i.e., each example is examined\nmultiple times), the saving provided by our bound is quite\nsigniﬁcant. This result is one of our main contributions.Theorem 1. There exist constants c1 and c2 so that given\nthe sampling probability q = L/N and the number of steps\nT, for any ε < c1q2T, Algorithm 1 is (ε,δ)-diﬀerentially\nprivate for any δ >0 if we choose\nσ≥c2\nq\n√\nTlog(1/δ)\nε .\nIf we use the strong composition theorem, we will then\nneed to choose σ= Ω(q\n√\nTlog(1/δ) log(T/δ)/ε). Note that\nwe save a factor of\n√\nlog(T/δ) in our asymptotic bound. The\nmoments accountant is beneﬁcial in theory, as this result\nindicates, and also in practice, as can be seen from Figure 2\nin Section 4. For example, with L = 0 .01N, σ = 4, δ =\n10−5, and T = 10000, we have ε≈1.26 using the moments\naccountant. As a comparison, we would get a much larger\nε≈9.34 using the strong composition theorem.\n3.2 The Moments Accountant: Details\nThe moments accountant keeps track of a bound on the\nmoments of the privacy loss random variable (deﬁned be-\nlow in Eq. (1)). It generalizes the standard approach of\ntracking ( ε,δ) and using the strong composition theorem.\nWhile such an improvement was known previously for com-\nposing Gaussian mechanisms, we show that it applies also\nfor composing Gaussian mechanisms with random sampling\nand can provide much tighter estimate of the privacy loss of\nAlgorithm 1.\nPrivacy loss is a random variable dependent on the ran-\ndom noise added to the algorithm. That a mechanism M\nis ( ε,δ)-diﬀerentially private is equivalent to a certain tail\nbound on M’s privacy loss random variable. While the tail\nbound is very useful information on a distribution, compos-\ning directly from it can result in quite loose bounds. We in-\nstead compute the log moments of the privacy loss random\nvariable, which compose linearly. We then use the moments\nbound, together with the standard Markov inequality, to ob-\ntain the tail bound, that is the privacy loss in the sense of\ndiﬀerential privacy.\nMore speciﬁcally, for neighboring databases d,d′∈Dn, a\nmechanism M, auxiliary input aux, and an outcome o∈R,\ndeﬁne the privacy loss at o as\nc(o; M,aux,d,d ′)\n∆\n= log Pr[M(aux,d) = o]\nPr[M(aux,d′) = o]. (1)\nA common design pattern, which we use extensively in the\npaper, is to update the state by sequentially applying diﬀer-\nentially private mechanisms. This is an instance of adaptive\ncomposition, which we model by letting the auxiliary input\nof the kth mechanism Mk be the output of all the previous\nmechanisms.\nFor a given mechanism M, we deﬁne the λth moment\nαM(λ; aux,d,d ′) as the log of the moment generating func-\ntion evaluated at the value λ:\nαM(λ; aux,d,d ′)\n∆\n=\nlog Eo∼M(aux,d)[exp(λc(o; M,aux,d,d ′))]. (2)\nIn order to prove privacy guarantees of a mechanism, it is\nuseful to bound all possible αM(λ; aux,d,d ′). We deﬁne\nαM(λ)\n∆\n= max\naux,d,d′\nαM(λ; aux,d,d ′) ,\nwhere the maximum is taken over all possible aux and all\nthe neighboring databases d,d′.\nWe state the properties of αthat we use for the moments\naccountant.\nTheorem 2. Let αM(λ) deﬁned as above. Then\n1. [Composability] Suppose that a mechanism Mcon-\nsists of a sequence of adaptive mechanismsM1,..., Mk\nwhere Mi: ∏i−1\nj=1 Rj ×D→R i. Then, for any λ\nαM(λ) ≤\nk∑\ni=1\nαMi (λ) .\n2. [Tail bound] For any ε > 0, the mechanism Mis\n(ε,δ)-diﬀerentially private for\nδ= min\nλ\nexp(αM(λ) −λε) .\nIn particular, Theorem 2.1 holds when the mechanisms\nthemselves are chosen based on the (public) output of the\nprevious mechanisms.\nBy Theorem 2, it suﬃces to compute, or bound,αMi (λ) at\neach step and sum them to bound the moments of the mech-\nanism overall. We can then use the tail bound to convert the\nmoments bound to the ( ε,δ)-diﬀerential privacy guarantee.\nThe main challenge that remains is to bound the value\nαMt (λ) for each step. In the case of a Gaussian mechanism\nwith random sampling, it suﬃces to estimate the following\nmoments. Let µ0 denote the probability density function\n(pdf) of N(0,σ2), and µ1 denote the pdf of N(1,σ2). Let µ\nbe the mixture of two Gaussians µ= (1 −q)µ0 + qµ1. Then\nwe need to compute α(λ) = log max(E1,E2) where\nE1 = Ez∼µ0 [(µ0(z)/µ(z))λ] , (3)\nE2 = Ez∼µ [(µ(z)/µ0(z))λ] . (4)\nIn the implementation of the moments accountant, we\ncarry out numerical integration to compute α(λ). In ad-\ndition, we can show the asymptotic bound\nα(λ) ≤q2λ(λ+ 1)/(1 −q)σ2 + O(q3/σ3) .\nTogether with Theorem 2, the above bound implies our\nmain Theorem 1. The details can be found in the Appendix.\n3.3 Hyperparameter Tuning\nWe identify characteristics of models relevant for privacy\nand, speciﬁcally, hyperparameters that we can tune in order\nto balance privacy, accuracy, and performance. In particu-\nlar, through experiments, we observe that model accuracy\nis more sensitive to training parameters such as batch size\nand noise level than to the structure of a neural network.\nIf we try several settings for the hyperparameters, we can\ntrivially add up the privacy costs of all the settings, possibly\nvia the moments accountant. However, since we care only\nabout the setting that gives us the most accurate model, we\ncan do better, such as applying a version of a result from\nGupta et al. [27] restated as Theorem D.1 in the Appendix.\nWe can use insights from theory to reduce the number of\nhyperparameter settings that need to be tried. While diﬀer-\nentially private optimization of convex objective functions\nis best achieved using batch sizes as small as 1, non-convex\nlearning, which is inherently less stable, beneﬁts from ag-\ngregation into larger batches. At the same time, Theorem 1\nsuggests that making batches too large increases the pri-\nvacy cost, and a reasonable tradeoﬀ is to take the numberof batches per epoch to be of the same order as the desired\nnumber of epochs. The learning rate in non-private train-\ning is commonly adjusted downwards carefully as the model\nconverges to a local optimum. In contrast, we never need\nto decrease the learning rate to a very small value, because\ndiﬀerentially private training never reaches a regime where\nit would be justiﬁed. On the other hand, in our experi-\nments, we do ﬁnd that there is a small beneﬁt to starting\nwith a relatively large learning rate, then linearly decaying\nit to a smaller value in a few epochs, and keeping it constant\nafterwards.\n4. IMPLEMENTATION\nWe have implemented the diﬀerentially private SGD al-\ngorithms in TensorFlow. The source code is available under\nan Apache 2.0 license from github.com/tensorﬂow/models.\nFor privacy protection, we need to “sanitize” the gradient\nbefore using it to update the parameters. In addition, we\nneed to keep track of the “privacy spending” based on how\nthe sanitization is done. Hence our implementation mainly\nconsists of two components: sanitizer, which preprocesses\nthe gradient to protect privacy, and privacy_accountant,\nwhich keeps track of the privacy spending over the course of\ntraining.\nFigure 1 contains the TensorFlow code snippet (in Python)\nof DPSGD_Optimizer, which minimizes a loss function us-\ning a diﬀerentially private SGD, and DPTrain, which itera-\ntively invokes DPSGD_Optimizer using a privacy accountant\nto bound the total privacy loss.\nIn many cases, the neural network model may beneﬁt from\nthe processing of the input by projecting it on the principal\ndirections (PCA) or by feeding it through a convolutional\nlayer. We implement diﬀerentially private PCA and apply\npre-trained convolutional layers (learned on public data).\nSanitizer. In order to achieve privacy protection, the sani-\ntizer needs to perform two operations: (1) limit the sensitiv-\nity of each individual example by clipping the norm of the\ngradient for each example; and (2) add noise to the gradient\nof a batch before updating the network parameters.\nIn TensorFlow, the gradient computation is batched for\nperformance reasons, yielding gB = 1/|B|∑\nx∈B∇θL(θ,x)\nfor a batch B of training examples. To limit the sensitivity\nof updates, we need to access each individual ∇θL(θ,x). To\nthis end, we implemented per_example_gradient operator\nin TensorFlow, as described by Goodfellow [25]. This opera-\ntor can compute a batch of individual ∇θL(θ,x). With this\nimplementation there is only a modest slowdown in train-\ning, even for larger batch size. Our current implementation\nsupports batched computation for the loss functionL, where\neach xi is singly connected to L, allowing us to handle most\nhidden layers but not, for example, convolutional layers.\nOnce we have the access to the per-example gradient, it\nis easy to use TensorFlow operators to clip its norm and to\nadd noise.\nPrivacy accountant. The main component in our imple-\nmentation is PrivacyAccountant which keeps track of pri-\nvacy spending over the course of training. As discussed in\nSection 3, we implemented the moments accountant that ad-\nditively accumulates the log of the moments of the privacy\nloss at each step. Dependent on the noise distribution, one\ncan compute α(λ) by either applying an asymptotic bound,\nevaluating a closed-form expression, or applying numerical\nclass DPSGD_Optimizer():\ndef __init__(self, accountant, sanitizer):\nself._accountant = accountant\nself._sanitizer = sanitizer\ndef Minimize(self, loss, params,\nbatch_size, noise_options):\n# Accumulate privacy spending before computing\n# and using the gradients.\npriv_accum_op =\nself._accountant.AccumulatePrivacySpending(\nbatch_size, noise_options)\nwith tf.control_dependencies(priv_accum_op):\n# Compute per example gradients\npx_grads = per_example_gradients(loss, params)\n# Sanitize gradients\nsanitized_grads = self._sanitizer.Sanitize(\npx_grads, noise_options)\n# Take a gradient descent step\nreturn apply_gradients(params, sanitized_grads)\ndef DPTrain(loss, params, batch_size, noise_options):\naccountant = PrivacyAccountant()\nsanitizer = Sanitizer()\ndp_opt = DPSGD_Optimizer(accountant, sanitizer)\nsgd_op = dp_opt.Minimize(\nloss, params, batch_size, noise_options)\neps, delta = (0, 0)\n# Carry out the training as long as the privacy\n# is within the pre-set limit.\nwhile within_limit(eps, delta):\nsgd_op.run()\neps, delta = accountant.GetSpentPrivacy()\nFigure 1: Code snippet of DPSGD_Optimizer and DP-\nTrain.\nintegration. The ﬁrst option would recover the generic ad-\nvanced composition theorem, and the latter two give a more\naccurate accounting of the privacy loss.\nFor the Gaussian mechanism we use, α(λ) is deﬁned ac-\ncording to Eqs. (3) and (4). In our implementation, we\ncarry out numerical integration to compute both E1 and E2\nin those equations. Also we compute α(λ) for a range of\nλ’s so we can compute the best possible ( ε,δ) values using\nTheorem 2.2. We ﬁnd that for the parameters of interest to\nus, it suﬃces to compute α(λ) for λ≤32.\nAt any point during training, one can query the privacy\nloss in the more interpretable notion of ( ε,δ) privacy using\nTheorem 2.2. Rogers et al. [47] point out risks associated\nwith adaptive choice of privacy parameters. We avoid their\nattacks and negative results by ﬁxing the number of iter-\nations and privacy parameters ahead of time. More gen-\neral implementations of a privacy accountant must correctly\ndistinguish between two modes of operation—as a privacy\nodometer or a privacy ﬁlter (see [47] for more details).\nDiﬀerentially private PCA. Principal component analy-\nsis (PCA) is a useful method for capturing the main features\nof the input data. We implement the diﬀerentially private\nPCA algorithm as described in [23]. More speciﬁcally, we\ntake a random sample of the training examples, treat them\nas vectors, and normalize each vector to unit ℓ2 norm to\nform the matrix A, where each vector is a row in the ma-\ntrix. We then add Gaussian noise to the covariance matrix\nATA and compute the principal directions of the noisy co-\nvariance matrix. Then for each input example we apply theprojection to these principal directions before feeding it into\nthe neural network.\nWe incur a privacy cost due to running a PCA. However,\nwe ﬁnd it useful for both improving the model quality and for\nreducing the training time, as suggested by our experiments\non the MNIST data. See Section 4 for details.\nConvolutional layers. Convolutional layers are useful for\ndeep neural networks. However, an eﬃcient per-example\ngradient computation for convolutional layers remains a chal-\nlenge within the TensorFlow framework, which motivates\ncreating a separate workﬂow. For example, some recent\nwork argues that even random convolutions often suﬃce [46,\n13, 49, 55, 14].\nAlternatively, we explore the idea of learning convolu-\ntional layers on public data, following Jarrett et al. [30].\nSuch convolutional layers can be based on GoogLeNet or\nAlexNet features [54, 35] for image models or on pretrained\nword2vec or GloVe embeddings in language models [41, 44].\n5. EXPERIMENTAL RESULTS\nThis section reports on our evaluation of the moments ac-\ncountant, and results on two popular image datasets: MNIST\nand CIFAR-10.\n5.1 Applying the Moments Accountant\nAs shown by Theorem 1, the moments accountant pro-\nvides a tighter bound on the privacy loss compared to the\ngeneric strong composition theorem. Here we compare them\nusing some concrete values. The overall privacy loss ( ε,δ)\ncan be computed from the noise level σ, the sampling ra-\ntio of each lot q = L/N (so each epoch consists of 1 /q\nbatches), and the number of epochs E (so the number of\nsteps is T = E/q). We ﬁx the target δ = 10−5, the value\nused for our MNIST and CIFAR experiments.\nIn our experiment, we set q= 0.01, σ= 4, and δ= 10−5,\nand compute the value of ε as a function of the training\nepoch E. Figure 2 shows two curves corresponding to, re-\nspectively, using the strong composition theorem and the\nmoments accountant. We can see that we get a much tighter\nestimation of the privacy loss by using the moments accoun-\ntant. For examples, when E = 100, the values are 9 .34\nand 1.26 respectively, and for E = 400, the values are 24.22\nand 2.55 respectively. That is, using the moments bound,\nwe achieve (2 .55,10−5)-diﬀerential privacy, whereas previ-\nous techniques only obtain the signiﬁcantly worse guarantee\nof (24.22,10−5).\n5.2 MNIST\nWe conduct experiments on the standard MNIST dataset\nfor handwritten digit recognition consisting of 60 ,000 train-\ning examples and 10,000 testing examples [36]. Each exam-\nple is a 28 ×28 size gray-level image. We use a simple feed-\nforward neural network with ReLU units and softmax of 10\nclasses (corresponding to the 10 digits) with cross-entropy\nloss and an optional PCA input layer.\nBaseline model.\nOur baseline model uses a 60-dimensional PCA projection\nlayer and a single hidden layer with 1,000 hidden units. Us-\ning the lot size of 600, we can reach accuracy of 98 .30% in\nabout 100 epochs. This result is consistent with what can\nbe achieved with a vanilla neural network [36].\nFigure 2: The ε value as a function of epoch E for\nq= 0.01, σ= 4, δ= 10−5, using the strong composition\ntheorem and the moments accountant respectively.\nDifferentially private model.\nFor the diﬀerentially private version, we experiment with\nthe same architecture with a 60-dimensional PCA projection\nlayer, a single 1,000-unit ReLU hidden layer, and a lot size of\n600. To limit sensitivity, we clip the gradient norm of each\nlayer at 4. We report results for three choices of the noise\nscale, which we call small ( σ = 2 ,σp = 4), medium ( σ =\n4,σp = 7), and large ( σ = 8,σp = 16). Here σ represents\nthe noise level for training the neural network, and σp the\nnoise level for PCA projection. The learning rate is set at 0.1\ninitially and linearly decreased to 0 .052 over 10 epochs and\nthen ﬁxed to 0 .052 thereafter. We have also experimented\nwith multi-hidden-layer networks. For MNIST, we found\nthat one hidden layer combined with PCA works better than\na two-layer network.\nFigure 3 shows the results for diﬀerent noise levels. In\neach plot, we show the evolution of the training and testing\naccuracy as a function of the number of epochs as well as\nthe corresponding δvalue, keeping εﬁxed. We achieve 90%,\n95%, and 97% test set accuracy for (0 .5,10−5), (2 ,10−5),\nand (8,10−5)-diﬀerential privacy respectively.\nOne attractive consequence of applying diﬀerentially pri-\nvate SGD is the small diﬀerence between the model’s ac-\ncuracy on the training and the test sets, which is consis-\ntent with the theoretical argument that diﬀerentially private\ntraining generalizes well [6]. In contrast, the gap between\ntraining and testing accuracy in non-private training, i.e.,\nevidence of overﬁtting, increases with the number of epochs.\nBy using the moments accountant, we can obtain aδvalue\nfor any given ε. We record the accuracy for diﬀerent ( ε,δ)\npairs in Figure 4. In the ﬁgure, each curve corresponds to the\nbest accuracy achieved for a ﬁxedδ, as it varies between 10−5\nand 10−2. For example, we can achieve 90% accuracy for\nε= 0.25 and δ = 0.01. As can be observed from the ﬁgure,\nfor a ﬁxed δ, varying the value of ε can have large impact\non accuracy, but for any ﬁxed ε, there is less diﬀerence with\ndiﬀerent δ values.\nEffect of the parameters.\nClassiﬁcation accuracy is determined by multiple factors(1) Large noise (2) Medium noise (3) Small noise\nFigure 3: Results on the accuracy for diﬀerent noise levels on the MNIST dataset. In all the experiments, the\nnetwork uses 60 dimension PCA projection, 1,000 hidden units, and is trained using lot size 600 and clipping\nthreshold 4. The noise levels (σ,σp) for training the neural network and for PCA projection are set at ( 8, 16),\n(4, 7), and (2, 4), respectively, for the three experiments.\nFigure 4: Accuracy of various (ε,δ) privacy values\non the MNIST dataset. Each curve corresponds to\na diﬀerent δ value.\nthat must be carefully tuned for optimal performance. These\nfactors include the topology of the network, the number of\nPCA dimensions and the number of hidden units, as well as\nparameters of the training procedure such as the lot size and\nthe learning rate. Some parameters are speciﬁc to privacy,\nsuch as the gradient norm clipping bound and the noise level.\nTo demonstrate the eﬀects of these parameters, we manip-\nulate them individually, keeping the rest constant. We set\nthe reference values as follows: 60 PCA dimensions, 1,000\nhidden units, 600 lot size, gradient norm bound of 4, ini-\ntial learning rate of 0.1 decreasing to a ﬁnal learning rate\nof 0.052 in 10 epochs, and noise σ equal to 4 and 7 respec-\ntively for training the neural network parameters and for the\nPCA projection. For each combination of values, we train\nuntil the point at which (2 ,10−5)-diﬀerential privacy would\nbe violated (so, for example, a larger σ allows more epochs\nof training). The results are presented in Figure 5.\nPCA projection. In our experiments, the accuracy is\nfairly stable as a function of the PCA dimension, with the\nbest results achieved for 60. (Not doing PCA reduces ac-\ncuracy by about 2%.) Although in principle the PCA pro-\njection layer can be replaced by an additional hidden layer,\nwe achieve better accuracy by training the PCA layer sep-\narately. By reducing the input size from 784 to 60, PCA\nleads to an almost 10 ×reduction in training time. The re-\nsult is fairly stable over a large range of the noise levels for\nthe PCA projection and consistently better than the accu-\nracy using random projection, which is at about 92 .5% and\nshown as a horizontal line in the plot.\nNumber of hidden units. Including more hidden units\nmakes it easier to ﬁt the training set. For non-private train-\ning, it is often preferable to use more units, as long as we\nemploy techniques to avoid overﬁtting. However, for diﬀer-\nentially private training, it is not a priori clear if more hidden\nunits improve accuracy, as more hidden units increase the\nsensitivity of the gradient, which leads to more noise added\nat each update.\nSomewhat counterintuitively, increasing the number of\nhidden units does not decrease accuracy of the trained model.\nOne possible explanation that calls for further analysis is\nthat larger networks are more tolerant to noise. This prop-\nerty is quite encouraging as it is common in practice to use\nvery large networks.\nLot size. According to Theorem 1, we can run N/Lepochs\nwhile staying within a constant privacy budget. Choosing\nthe lot size must balance two conﬂicting objectives. On the\none hand, smaller lots allow running more epochs, i.e., passes\nover data, improving accuracy. On the other hand, for a\nlarger lot, the added noise has a smaller relative eﬀect.\nOur experiments show that the lot size has a relatively\nlarge impact on accuracy. Empirically, the best lot size is\nroughly\n√\nN where N is the number of training examples.\nLearning rate. Accuracy is stable for a learning rate in\nthe range of [0 .01,0.07] and peaks at 0.05, as shown in Fig-\nure 5(4). However, accuracy decreases signiﬁcantly if the\nlearning rate is too large. Some additional experiments sug-\ngest that, even for large learning rates, we can reach similar\nlevels of accuracy by reducing the noise level and, accord-\ningly, by training less in order to avoid exhausting the pri-\nvacy budget.\nClipping bound. Limiting the gradient norm has two op-\nposing eﬀects: clipping destroys the unbiasedness of the gra-\ndient estimate, and if the clipping parameter is too small,\nthe average clipped gradient may point in a very diﬀerent(1) variable projection dimensions (2) variable hidden units (3) variable lot size\n(4) variable learning rate (5) variable gradient clipping norm (6) variable noise level\nFigure 5: MNIST accuracy when one parameter varies, and the others are ﬁxed at reference values.\ndirection from the true gradient. On the other hand, in-\ncreasing the norm bound C forces us to add more noise to\nthe gradients (and hence the parameters), since we add noise\nbased on σC. In practice, a good way to choose a value for\nC is by taking the median of the norms of the unclipped\ngradients over the course of training.\nNoise level. By adding more noise, the per-step privacy\nloss is proportionally smaller, so we can run more epochs\nwithin a given cumulative privacy budget. In Figure 5(5),\nthe x-axis is the noise level σ. The choice of this value has\na large impact on accuracy.\nFrom the experiments, we observe the following.\n1. The PCA projection improves both model accuracy\nand training performance. Accuracy is quite stable\nover a large range of choices for the projection dimen-\nsions and the noise level used in the PCA stage.\n2. The accuracy is fairly stable over the network size.\nWhen we can only run smaller number of epochs, it is\nmore beneﬁcial to use a larger network.\n3. The training parameters, especially the lot size and\nthe noise scale σ, have a large impact on the model\naccuracy. They both determine the “noise-to-signal”\nratio of the sanitized gradients as well as the number\nof epochs we are able to go through the data before\nreaching the privacy limit.\nOur framework allows for adaptive control of the training\nparameters, such as the lot size, the gradient norm bound\nC, and noise level σ. Our initial experiments with decreas-\ning noise as training progresses did not show a signiﬁcant\nimprovement, but it is interesting to consider more sophis-\nticated schemes for adaptively choosing these parameters.\n5.3 CIFAR\nWe also conduct experiments on the CIFAR-10 dataset,\nwhich consists of color images classiﬁed into 10 classes such\nas ships, cats, and dogs, and partitioned into 50,000 training\nexamples and 10 ,000 test examples [1]. Each example is a\n32 ×32 image with three channels (RGB). For this learning\ntask, nearly all successful networks use convolutional layers.\nThe CIFAR-100 dataset has similar parameters, except that\nimages are classiﬁed into 100 classes; the examples and the\nimage classes are diﬀerent from those of CIFAR-10.\nWe use the network architecture from the TensorFlow con-\nvolutional neural networks tutorial [2]. Each 32 ×32 image\nis ﬁrst cropped to a 24 ×24 one by taking the center patch.\nThe network architecture consists of two convolutional lay-\ners followed by two fully connected layers. The convolutional\nlayers use 5 ×5 convolutions with stride 1, followed by a\nReLU and 2 ×2 max pools, with 64 channels each. Thus\nthe ﬁrst convolution outputs a 12 ×12 ×64 tensor for each\nimage, and the second outputs a 6×6×64 tensor. The latter\nis ﬂattened to a vector that gets fed into a fully connected\nlayer with 384 units, and another one of the same size.\nThis architecture, non-privately, can get to about 86% ac-\ncuracy in 500 epochs. Its simplicity makes it an appealing\nchoice for our work. We should note however that by us-\ning deeper networks with diﬀerent non-linearities and other\nadvanced techniques, one can obtain signiﬁcantly better ac-\ncuracy, with the state-of-the-art being about 96 .5% [26].\nAs is standard for such image datasets, we use data aug-\nmentation during training. For each training image, we gen-\nerate a new distorted image by randomly picking a 24 ×24\npatch from the image, randomly ﬂipping the image along\nthe left-right direction, and randomly distorting the bright-\nness and the contrast of the image. In each epoch, these(1) ε= 2 (2) ε= 4 (3) ε= 8\nFigure 6: Results on accuracy for diﬀerent noise levels on CIFAR-10. With δ set to 10−5, we achieve accuracy\n67%, 70%, and 73%, with ε being 2, 4, and 8, respectively. The ﬁrst graph uses a lot size of 2,000, (2) and (3)\nuse a lot size of 4,000. In all cases, σ is set to 6, and clipping is set to 3.\ndistortions are done independently. We refer the reader to\nthe TensorFlow tutorial [2] for additional details.\nAs the convolutional layers have shared parameters, com-\nputing per-example gradients has a larger computational\noverhead. Previous work has shown that convolutional lay-\ners are often transferable: parameters learned from one data-\nset can be used on another one without retraining [30]. We\ntreat the CIFAR-100 dataset as a public dataset and use it\nto train a network with the same architecture. We use the\nconvolutions learned from training this dataset. Retrain-\ning only the fully connected layers with this architecture for\nabout 250 epochs with a batch size of 120 gives us approxi-\nmately 80% accuracy, which is our non-private baseline.\nDifferentially private version.\nFor the diﬀerentially private version, we use the same ar-\nchitecture. As discussed above, we use pre-trained convolu-\ntional layers. The fully connected layers are initialized from\nthe pre-trained network as well. We train the softmax layer,\nand either the top or both fully connected layers. Based on\nlooking at gradient norms, the softmax layer gradients are\nroughly twice as large as the other two layers, and we keep\nthis ratio when we try clipping at a few diﬀerent values be-\ntween 3 and 10. The lot size is an additional knob that we\ntune: we tried 600, 2 ,000, and 4 ,000. With these settings,\nthe per-epoch training time increases from approximately 40\nseconds to 180 seconds.\nIn Figure 6, we show the evolution of the accuracy and\nthe privacy cost, as a function of the number of epochs, for\na few diﬀerent parameter settings.\nThe various parameters inﬂuence the accuracy one gets, in\nways not too diﬀerent from that in the MNIST experiments.\nA lot size of 600 leads to poor results on this dataset and\nwe need to increase it to 2,000 or more for results reported\nin Figure 6.\nCompared to the MNIST dataset, where the diﬀerence in\naccuracy between a non-private baseline and a private model\nis about 1.3%, the corresponding drop in accuracy in our\nCIFAR-10 experiment is much larger (about 7%). We leave\nclosing this gap as an interesting test for future research in\ndiﬀerentially private machine learning.\n6. RELATED WORK\nThe problem of privacy-preserving data mining, or ma-\nchine learning, has been a focus of active work in several\nresearch communities since the late 90s [5, 37]. The exist-\ning literature can be broadly classiﬁed along several axes:\nthe class of models, the learning algorithm, and the privacy\nguarantees.\nPrivacy guarantees. Early works on privacy-preserving\nlearning were done in the framework of secure function eval-\nuation (SFE) and secure multi-party computations (MPC),\nwhere the input is split between two or more parties, and\nthe focus is on minimizing information leaked during the\njoint computation of some agreed-to functionality. In con-\ntrast, we assume that data is held centrally, and we are\nconcerned with leakage from the functionality’s output (i.e.,\nthe model).\nAnother approach, k-anonymity and closely related no-\ntions [53], seeks to oﬀer a degree of protection to underlying\ndata by generalizing and suppressing certain identifying at-\ntributes. The approach has strong theoretical and empirical\nlimitations [4, 9] that make it all but inapplicable to de-\nanonymization of high-dimensional, diverse input datasets.\nRather than pursue input sanitization, we keep the under-\nlying raw records intact and perturb derived data instead.\nThe theory of diﬀerential privacy, which provides the an-\nalytical framework for our work, has been applied to a large\ncollection of machine learning tasks that diﬀered from ours\neither in the training mechanism or in the target model.\nThe moments accountant is closely related to the notion of\nR´ enyi diﬀerential privacy [42], which proposes (scaled)α(λ)\nas a means of quantifying privacy guarantees. In a concur-\nrent and independent work Bun and Steinke [10] introduce\na relaxation of diﬀerential privacy (generalizing the work\nof Dwork and Rothblum [20]) deﬁned via a linear upper\nbound on α(λ). Taken together, these works demonstrate\nthat the moments accountant is a useful technique for theo-\nretical and empirical analyses of complex privacy-preserving\nalgorithms.\nLearning algorithm. A common target for learning with\nprivacy is a class of convex optimization problems amenable\nto a wide variety of techniques [18, 11, 34]. In concurrent\nwork, Wu et al. achieve 83% accuracy on MNIST via con-\nvex empirical risk minimization [57]. Training multi-layer\nneural networks is non-convex, and typically solved by an\napplication of SGD, whose theoretical guarantees are poorly\nunderstood.For the CIFAR neural network we incorporate diﬀeren-\ntially private training of the PCA projection matrix [23],\nwhich is used to reduce dimensionality of inputs.\nModel class. The ﬁrst end-to-end diﬀerentially private sys-\ntem was evaluated on the Netﬂix Prize dataset [39], a version\nof a collaborative ﬁltering problem. Although the problem\nshared many similarities with ours—high-dimensional in-\nputs, non-convex objective function—the approach taken by\nMcSherry and Mironov diﬀered signiﬁcantly. They identiﬁed\nthe core of the learning task, eﬀectively suﬃcient statistics,\nthat can be computed in a diﬀerentially private manner via\na Gaussian mechanism. In our approach no such suﬃcient\nstatistics exist.\nIn a recent work Shokri and Shmatikov [50] designed and\nevaluated a system for distributed training of a deep neural\nnetwork. Participants, who hold their data closely, commu-\nnicate sanitized updates to a central authority. The sani-\ntization relies on an additive-noise mechanism, based on a\nsensitivity estimate, which could be improved to a hard sen-\nsitivity guarantee. They compute privacy loss per param-\neter (not for an entire model). By our preferred measure,\nthe total privacy loss per participant on the MNIST dataset\nexceeds several thousand.\nA diﬀerent, recent approach towards diﬀerentially private\ndeep learning is explored by Phan et al. [45]. This work\nfocuses on learning autoencoders. Privacy is based on per-\nturbing the objective functions of these autoencoders.\n7. CONCLUSIONS\nWe demonstrate the training of deep neural networks with\ndiﬀerential privacy, incurring a modest total privacy loss,\ncomputed over entire models with many parameters. In\nour experiments for MNIST, we achieve 97% training accu-\nracy and for CIFAR-10 we achieve 73% accuracy, both with\n(8,10−5)-diﬀerential privacy. Our algorithms are based on a\ndiﬀerentially private version of stochastic gradient descent;\nthey run on the TensorFlow software library for machine\nlearning. Since our approach applies directly to gradient\ncomputations, it can be adapted to many other classical\nand more recent ﬁrst-order optimization methods, such as\nNAG [43], Momentum [48], AdaGrad [15], or SVRG [31].\nA new tool, which may be of independent interest, is a\nmechanism for tracking privacy loss, the moments accoun-\ntant. It permits tight automated analysis of the privacy loss\nof complex composite mechanisms that are currently beyond\nthe reach of advanced composition theorems.\nA number of avenues for further work are attractive. In\nparticular, we would like to consider other classes of deep\nnetworks. Our experience with MNIST and CIFAR-10 should\nbe helpful, but we see many opportunities for new research,\nfor example in applying our techniques to LSTMs used for\nlanguage modeling tasks. In addition, we would like to ob-\ntain additional improvements in accuracy. Many training\ndatasets are much larger than those of MNIST and CIFAR-\n10; accuracy should beneﬁt from their size.\n8. ACKNOWLEDGMENTS\nWe are grateful to ´Ulfar Erlingsson and Dan Ramage for\nmany useful discussions, and to Mark Bun and Thomas\nSteinke for sharing a draft of [10].\n9. REFERENCES\n[1] CIFAR-10 and CIFAR-100 datasets.\nwww.cs.toronto.edu/˜kriz/cifar.html.\n[2] TensorFlow convolutional neural networks tutorial.\nwww.tensorﬂow.org/tutorials/deep cnn.\n[3] TensorFlow: Large-scale machine learning on\nheterogeneous systems, 2015. Software available from\ntensorﬂow.org.\n[4] C. C. Aggarwal. On k-anonymity and the curse of\ndimensionality. In VLDB, pages 901–909, 2005.\n[5] R. Agrawal and R. Srikant. Privacy-preserving data\nmining. In SIGMOD, pages 439–450. ACM, 2000.\n[6] R. Bassily, K. Nissim, A. Smith, T. Steinke,\nU. Stemmer, and J. Ullman. Algorithmic stability for\nadaptive data analysis. In STOC, pages 1046–1059.\nACM, 2016.\n[7] R. Bassily, A. D. Smith, and A. Thakurta. Private\nempirical risk minimization: Eﬃcient algorithms and\ntight error bounds. In FOCS, pages 464–473. IEEE,\n2014.\n[8] A. Beimel, H. Brenner, S. P. Kasiviswanathan, and\nK. Nissim. Bounds on the sample complexity for\nprivate learning and private data release. Machine\nLearning, 94(3):401–437, 2014.\n[9] J. Brickell and V. Shmatikov. The cost of privacy:\nDestruction of data-mining utility in anonymized data\npublishing. In KDD, pages 70–78. ACM, 2008.\n[10] M. Bun and T. Steinke. Concentrated diﬀerential\nprivacy: Simpliﬁcations, extensions, and lower bounds.\nIn TCC-B.\n[11] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate.\nDiﬀerentially private empirical risk minimization.\nJ. Machine Learning Research, 12:1069–1109, 2011.\n[12] R. Collobert, K. Kavukcuoglu, and C. Farabet.\nTorch7: A Matlab-like environment for machine\nlearning. In BigLearn, NIPS Workshop, number\nEPFL-CONF-192376, 2011.\n[13] D. D. Cox and N. Pinto. Beyond simple features: A\nlarge-scale feature search approach to unconstrained\nface recognition. In FG 2011, pages 8–15. IEEE, 2011.\n[14] A. Daniely, R. Frostig, and Y. Singer. Toward deeper\nunderstanding of neural networks: The power of\ninitialization and a dual view on expressivity. CoRR,\nabs/1602.05897, 2016.\n[15] J. Duchi, E. Hazan, and Y. Singer. Adaptive\nsubgradient methods for online learning and stochastic\noptimization. J. Machine Learning Research,\n12:2121–2159, July 2011.\n[16] C. Dwork. A ﬁrm foundation for private data analysis.\nCommun. ACM, 54(1):86–95, Jan. 2011.\n[17] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,\nand M. Naor. Our data, ourselves: Privacy via\ndistributed noise generation. In EUROCRYPT, pages\n486–503. Springer, 2006.\n[18] C. Dwork and J. Lei. Diﬀerential privacy and robust\nstatistics. In STOC, pages 371–380. ACM, 2009.\n[19] C. Dwork, F. McSherry, K. Nissim, and A. Smith.\nCalibrating noise to sensitivity in private data\nanalysis. In TCC, pages 265–284. Springer, 2006.\n[20] C. Dwork and A. Roth. The algorithmic foundationsof diﬀerential privacy. Foundations and Trends in\nTheoretical Computer Science, 9(3–4):211–407, 2014.\n[21] C. Dwork and G. N. Rothblum. Concentrated\ndiﬀerential privacy. CoRR, abs/1603.01887, 2016.\n[22] C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting\nand diﬀerential privacy. In FOCS, pages 51–60. IEEE,\n2010.\n[23] C. Dwork, K. Talwar, A. Thakurta, and L. Zhang.\nAnalyze Gauss: Optimal bounds for\nprivacy-preserving principal component analysis. In\nSTOC, pages 11–20. ACM, 2014.\n[24] M. Fredrikson, S. Jha, and T. Ristenpart. Model\ninversion attacks that exploit conﬁdence information\nand basic countermeasures. In CCS, pages 1322–1333.\nACM, 2015.\n[25] I. Goodfellow. Eﬃcient per-example gradient\ncomputations. CoRR, abs/1510.01799v2, 2015.\n[26] B. Graham. Fractional max-pooling. CoRR,\nabs/1412.6071, 2014.\n[27] A. Gupta, K. Ligett, F. McSherry, A. Roth, and\nK. Talwar. Diﬀerentially private combinatorial\noptimization. In SODA, pages 1106–1125, 2010.\n[28] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep\ninto rectiﬁers: Surpassing human-level performance on\nImageNet classiﬁcation. In ICCV, pages 1026–1034.\nIEEE, 2015.\n[29] R. Ierusalimschy, L. H. de Figueiredo, and W. Filho.\nLua—an extensible extension language. Software:\nPractice and Experience, 26(6):635–652, 1996.\n[30] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and\nY. LeCun. What is the best multi-stage architecture\nfor object recognition? In ICCV, pages 2146–2153.\nIEEE, 2009.\n[31] R. Johnson and T. Zhang. Accelerating stochastic\ngradient descent using predictive variance reduction.\nIn NIPS, pages 315–323, 2013.\n[32] P. Kairouz, S. Oh, and P. Viswanath. The composition\ntheorem for diﬀerential privacy. In ICML, pages\n1376–1385. ACM, 2015.\n[33] S. P. Kasiviswanathan, H. K. Lee, K. Nissim,\nS. Raskhodnikova, and A. Smith. What can we learn\nprivately? SIAM J. Comput. , 40(3):793–826, 2011.\n[34] D. Kifer, A. D. Smith, and A. Thakurta. Private\nconvex optimization for empirical risk minimization\nwith applications to high-dimensional regression. In\nCOLT, pages 25.1–25.40, 2012.\n[35] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImageNet classiﬁcation with deep convolutional neural\nnetworks. In NIPS, pages 1097–1105, 2012.\n[36] Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner.\nGradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11), 1998.\n[37] Y. Lindell and B. Pinkas. Privacy preserving data\nmining. In CRYPTO, pages 36–54. Springer, 2000.\n[38] C. J. Maddison, A. Huang, I. Sutskever, and D. Silver.\nMove evaluation in Go using deep convolutional neural\nnetworks. In ICLR, 2015.\n[39] F. McSherry and I. Mironov. Diﬀerentially private\nrecommender systems: Building privacy into the\nNetﬂix Prize contenders. In KDD, pages 627–636.\nACM, 2009.\n[40] F. D. McSherry. Privacy integrated queries: An\nextensible platform for privacy-preserving data\nanalysis. In SIGMOD, pages 19–30. ACM, 2009.\n[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean.\nEﬃcient estimation of word representations in vector\nspace. CoRR, abs/1301.3781, 2013.\n[42] I. Mironov. R´ enyi diﬀerential privacy. Private\ncommunication, 2016.\n[43] Y. Nesterov. Introductory Lectures on Convex\nOptimization. A Basic Course . Springer, 2004.\n[44] J. Pennington, R. Socher, and C. D. Manning. GloVe:\nGlobal vectors for word representation. In EMNLP,\npages 1532–1543, 2014.\n[45] N. Phan, Y. Wang, X. Wu, and D. Dou. Diﬀerential\nprivacy preservation for deep auto-encoders: an\napplication of human behavior prediction. In AAAI,\npages 1309–1316, 2016.\n[46] N. Pinto, Z. Stone, T. E. Zickler, and D. Cox. Scaling\nup biologically-inspired computer vision: A case study\nin unconstrained face recognition on Facebook. In\nCVPR, pages 35–42. IEEE, 2011.\n[47] R. M. Rogers, A. Roth, J. Ullman, and S. P. Vadhan.\nPrivacy odometers and ﬁlters: Pay-as-you-go\ncomposition. In NIPS.\n[48] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nLearning representations by back-propagating errors.\nNature, 323:533–536, Oct. 1986.\n[49] A. Saxe, P. W. Koh, Z. Chen, M. Bhand, B. Suresh,\nand A. Ng. On random weights and unsupervised\nfeature learning. In ICML, pages 1089–1096. ACM,\n2011.\n[50] R. Shokri and V. Shmatikov. Privacy-preserving deep\nlearning. In CCS, pages 1310–1321. ACM, 2015.\n[51] D. Silver, A. Huang, C. J. Maddison, A. Guez,\nL. Sifre, G. van den Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot,\nS. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,\nI. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,\nT. Graepel, and D. Hassabis. Mastering the game of\nGo with deep neural networks and tree search. Nature,\n529(7587):484–489, 2016.\n[52] S. Song, K. Chaudhuri, and A. Sarwate. Stochastic\ngradient descent with diﬀerentially private updates. In\nGlobalSIP Conference, 2013.\n[53] L. Sweeney. k-anonymity: A model for protecting\nprivacy. International J. of Uncertainty, Fuzziness and\nKnowledge-Based Systems, 10(05):557–570, 2002.\n[54] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and\nA. Rabinovich. Going deeper with convolutions. In\nCVPR, pages 1–9. IEEE, 2015.\n[55] S. Tu, R. Roelofs, S. Venkataraman, and B. Recht.\nLarge scale kernel learning using block coordinate\ndescent. CoRR, abs/1602.05310, 2016.\n[56] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever,\nand G. E. Hinton. Grammar as a foreign language. In\nNIPS, pages 2773–2781, 2015.\n[57] X. Wu, A. Kumar, K. Chaudhuri, S. Jha, and J. F.\nNaughton. Diﬀerentially private stochastic gradient\ndescent for in-RDBMS analytics. CoRR,\nabs/1606.04722, 2016.APPENDIX\nA. PROOF OF THEOREM 2\nHere we restate and prove Theorem 2.\nTheorem 2. Let αM(λ) deﬁned as\nαM(λ)\n∆\n= max\naux,d,d′\nαM(λ; aux,d,d ′),\nwhere the maximum is taken over all auxiliary inputs and\nneighboring databases d,d′. Then\n1. [Composability] Suppose that a mechanism Mcon-\nsists of a sequence of adaptive mechanismsM1,..., Mk\nwhere Mi: ∏i−1\nj=1 Rj ×D→R i. Then, for any λ\nαM(λ) ≤\nk∑\ni=1\nαMi (λ) .\n2. [Tail bound] For any ε > 0, the mechanism M is\n(ε,δ)-diﬀerentially private for\nδ= min\nλ\nexp(αM(λ) −λε) .\nProof. Composition of moments. For brevity, let\nM1:i denote ( M1,..., Mi), and similarly let o1:i denote\n(o1,...,o i). For neighboring databases d,d′ ∈Dn, and a\nsequence of outcomes o1,...,o k we write\nc(o1:k; M1:k,o1:(k−1),d,d ′)\n= log Pr[M1:k(d; o1:(k−1)) = o1:k]\nPr[M1:k(d′; o1:(k−1)) = o1:k]\n= log\nk∏\ni=1\nPr[Mi(d) = oi |M1:(i−1)(d) = o1:(i−1)]\nPr[Mi(d′) = oi |M1:(i−1)(d′) = o1:(i−1)]\n=\nk∑\ni=1\nlog Pr[Mi(d) = oi |M1:(i−1)(d) = o1:(i−1)]\nPr[Mi(d′) = oi |M1:(i−1)(d′) = o1:(i−1)]\n=\nk∑\ni=1\nc(oi; Mi,o1:(i−1),d,d ′).\nThus\nEo′\n1:k∼M1:k(d)\n[\nexp(λc(o′\n1:k; M1:k,d,d ′))\n⏐⏐∀i<k : o′\ni = oi\n]\n= Eo′\n1:k∼M1:k(d)\n[\nexp\n(\nλ\nk∑\ni=1\nc(o′\ni; Mi,o1:(i−1),d,d ′)\n)]\n= Eo′\n1:k∼M1:k(d)\n[ k∏\ni=1\nexp\n(\nλc(o′\ni; Mi,o1:(i−1),d,d ′)\n)\n]\n(by independence of noise)\n=\nk∏\ni=1\nEo′\ni∼Mi(d)\n[\nexp(λc(o′\ni; Mi,o1:(i−1),d,d ′))\n]\n=\nk∏\ni=1\nexp\n(\nαMi (λ; o1:(i−1),d,d ′)\n)\n= exp\n( k∑\ni=1\nαi(λ; o1:(i−1),d,d ′)\n)\n.\nThe claim follows.\nTail bound by moments. The proof is based on the stan-\ndard Markov’s inequality argument used in proofs of mea-\nsure concentration. We have\nPr\no∼M(d)\n[c(o) ≥ε]\n= Pr\no∼M(d)\n[exp(λc(o)) ≥exp(λε))]\n≤Eo∼M(d)[exp(λc(o))]\nexp(λε)\n≤exp(α−λε).\nLet B= {o: c(o) ≥ε}. Then for any S,\nPr[M(d) ∈S]\n= Pr[M(d) ∈S∩Bc] + Pr[M(d) ∈S∩B]\n≤exp(ε) Pr[M(d′) ∈S∩Bc] + Pr[M(d) ∈B]\n≤exp(ε) Pr[M(d′) ∈S] + exp(α−λε).\nThe second part follows by an easy calculation.\nThe proof demonstrates a tail bound on the privacy loss,\nmaking it stronger than diﬀerential privacy for a ﬁxed value\nof ε,δ.\nB. PROOF OF LEMMA 3\nThe proof of the main theorem relies on the following mo-\nments bound on Gaussian mechanism with random sam-\npling.\nLemma 3. Suppose that f: D →Rp with ∥f(·)∥2 ≤1.\nLet σ≥1 and let J be a sample from [n] where each i∈[n]\nis chosen independently with probability q < 1\n16σ. Then for\nany positive integer λ ≤σ2 ln 1\nqσ, the mechanism M(d) =∑\ni∈J f(di) + N(0,σ2I) satisﬁes\nαM(λ) ≤q2λ(λ+ 1)\n(1 −q)σ2 + O(q3λ3/σ3).\nProof. Fix d′ and let d = d′∪{dn}. Without loss of\ngenerality, f(dn) = e1 and ∑\ni∈J\\[n] f(di) = 0. Thus M(d)\nand M(d′) are distributed identically except for the ﬁrst\ncoordinate and hence we have a one-dimensional problem.\nLet µ0 denote the pdf of N(0,σ2) and let µ1 denote the pdf\nof N(1,σ2). Thus:\nM(d′) ∼µ0,\nM(d) ∼µ\n∆\n= (1 −q)µ0 + qµ1.\nWe want to show that\nEz∼µ[(µ(z)/µ0(z))λ] ≤α,\nand Ez∼µ0 [(µ0(z)/µ(z))λ] ≤α,\nfor some explicit α to be determined later.\nWe will use the same method to prove both bounds. As-\nsume we have two distributions ν0 and ν1, and we wish to\nbound\nEz∼ν0 [(ν0(z)/ν1(z))λ] = Ez∼ν1 [(ν0(z)/ν1(z))λ+1] .Using binomial expansion, we have\nEz∼ν1 [(ν0(z)/ν1(z))λ+1]\n= Ez∼ν1 [(1 + (ν0(z) −ν1(z))/ν1(z))λ+1]\n= Ez∼ν1 [(1 + (ν0(z) −ν1(z))/ν1(z))λ+1]\n=\nλ+1∑\nt=0\n(\nλ+ 1\nt\n)\nEz∼ν1 [((ν0(z) −ν1(z))/ν1(z))t] . (5)\nThe ﬁrst term in (5) is 1, and the second term is\nEz∼ν1\n[ν0(z) −ν1(z)\nν1(z)\n]\n=\n∫ ∞\n−∞\nν1(z)ν0(z) −ν1(z)\nν1(z) dz\n=\n∫ ∞\n−∞\nν0(z) dz−\n∫ ∞\n−∞\nν1(z) dz\n= 1 −1 = 0.\nTo prove the lemma it suﬃces to show show that for both\nν0 = µ,ν1 = µ0 and ν0 = µ0,ν1 = µ, the third term is\nbounded by q2λ(λ+ 1)/(1 −q)σ2 and that this bound dom-\ninates the sum of the remaining terms. We will prove the\nmore diﬃcult second case (ν0 = µ0,ν1 = µ); the proof of the\nother case is similar.\nTo upper bound the third term in (5), we note thatµ(z) ≥\n(1 −q)µ0(z), and write\nEz∼µ\n[(µ0(z) −µ(z)\nµ(z)\n)2]\n= q2Ez∼µ\n[(µ0(z) −µ1(z)\nµ(z)\n)2]\n= q2\n∫ ∞\n−∞\n(µ0(z) −µ1(z))2\nµ(z) dz\n≤ q2\n1 −q\n∫ ∞\n−∞\n(µ0(z) −µ1(z))2\nµ0(z) dz\n= q2\n1 −qEz∼µ0\n[(µ0(z) −µ1(z)\nµ0(z)\n)2]\n.\nAn easy fact is that for any a ∈R, Ez∼µ0 exp(2az/2σ2) =\nexp(a2/2σ2). Thus,\nEz∼µ0\n[(µ0(z) −µ1(z)\nµ0(z)\n)2]\n= Ez∼µ0\n[(\n1 −exp(2z−1\n2σ2 )\n)2]\n= 1 −2Ez∼µ0\n[\nexp(2z−1\n2σ2 )\n]\n+ Ez∼µ0\n[\nexp(4z−2\n2σ2 )\n]\n= 1 −2 exp\n( 1\n2σ2\n)\n·exp\n(−1\n2σ2\n)\n+ exp\n( 4\n2σ2\n)\n·exp\n(−2\n2σ2\n)\n= exp(1/σ2) −1.\nThus the third term in the binomial expansion (5)\n(\n1 + λ\n2\n)\nEz∈µ\n[(µ0(z) −µ(z)\nµ(z)\n)2]\n≤λ(λ+ 1)q2\n(1 −q)σ2 .\nTo bound the remaining terms, we ﬁrst note that by stan-\ndard calculus, we get:\n∀z≤0 : |µ0(z) −µ1(z)|≤− (z−1)µ0(z)/σ2,\n∀z≥1 : |µ0(z) −µ1(z)|≤ zµ1(z)/σ2,\n∀0 ≤z≤1 : |µ0(z) −µ1(z)|≤ µ0(z)(exp(1/2σ2) −1)\n≤µ0(z)/σ2.\nWe can then write\nEz∼µ\n[(µ0(z) −µ(z)\nµ(z)\n)t]\n≤\n∫ 0\n−∞\nµ(z)\n⏐⏐⏐⏐⏐\n(µ0(z) −µ(z)\nµ(z)\n)t\n⏐⏐⏐⏐⏐dz\n+\n∫ 1\n0\nµ(z)\n⏐⏐⏐⏐⏐\n(µ0(z) −µ(z)\nµ(z)\n)t\n⏐⏐⏐⏐⏐dz\n+\n∫ ∞\n1\nµ(z)\n⏐⏐⏐⏐⏐\n(µ0(z) −µ(z)\nµ(z)\n)t\n⏐⏐⏐⏐⏐dz.\nWe consider these terms individually. We repeatedly make\nuse of three observations: (1) µ0 −µ = q(µ0 −µ1), (2)\nµ≥(1 −q)µ0, and (3) Eµ0 [|z|t] ≤σt(t−1)!!. The ﬁrst term\ncan then be bounded by\nqt\n(1 −q)t−1σ2t\n∫ 0\n−∞\nµ0(z)|z−1|tdz\n≤ (2q)t(t−1)!!\n2(1 −q)t−1σt.\nThe second term is at most\nqt\n(1 −q)t\n∫ 1\n0\nµ(z)\n⏐⏐⏐⏐⏐\n(µ0(z) −µ1(z)\nµ0(z)\n)t\n⏐⏐⏐⏐⏐dz\n≤ qt\n(1 −q)t\n∫ 1\n0\nµ(z) 1\nσ2t dz\n≤ qt\n(1 −q)tσ2t.\nSimilarly, the third term is at most\nqt\n(1 −q)t−1σ2t\n∫ ∞\n1\nµ0(z)\n(zµ1(z)\nµ0(z)\n)t\ndz\n≤ qt\n(1 −q)t−1σ2t\n∫ ∞\n1\nµ0(z) exp((2tz−t)/2σ2)ztdz\n≤qtexp((t2 −t)/2σ2)\n(1 −q)t−1σ2t\n∫ ∞\n0\nµ0(z−t)ztdz\n≤(2q)texp((t2 −t)/2σ2)(σt(t−1)!! + tt)\n2(1 −q)t−1σ2t .\nUnder the assumptions on q, σ, and λ, it is easy to check\nthat the three terms, and their sum, drop oﬀ geometrically\nfast in t for t > 3. Hence the binomial expansion (5) is\ndominated by the t = 3 term, which is O(q3λ3/σ3). The\nclaim follows.To derive Theorem 1, we use the above moments bound\nalong with the tail bound from Theorem 2, optimizing over\nthe choice of λ.\nTheorem 1. There exist constants c1 and c2 so that given\nthe sampling probability q = L/N and the number of steps\nT, for any ε < c1q2T, Algorithm 1 is (ε,δ)-diﬀerentially\nprivate for any δ >0 if we choose\nσ≥c2\nq\n√\nTlog(1/δ)\nε .\nProof. Assume for now that σ,λ satisfy the conditions\nin Lemma 3. By Theorem 2.1 and Lemma 3, the log mo-\nment of Algorithm 1 can be bounded as follows α(λ) ≤\nTq2λ2/σ2. By Theorem 2, to guarantee Algorithm 1 to be\n(ε,δ)-diﬀerentially private, it suﬃces that\nTq2λ2/σ2 ≤λε/2 ,\nexp(−λε/2) ≤δ.\nIn addition, we need\nλ≤σ2 log(1/qσ) .\nIt is now easy to verify that when ε= c1q2T, we can satisfy\nall these conditions by setting\nσ= c2\nq\n√\nTlog(1/δ)\nε\nfor some explicit constants c1 and c2.\nC. FROM DIFFERENTIAL PRIV ACY TO MO-\nMENTS BOUNDS\nOne can also translate a diﬀerential privacy guarantee into\na moment bound.\nLemma C.1. Let Mbe ε-diﬀerentially private. Then for\nany λ> 0, Msatisﬁes\nαλ ≤λε(eε −1) + λ2ε2e2ε/2.\nProof. Let Zdenote the random variablec(M(d)). Then\ndiﬀerential privacy implies that\n•µ\n∆\n= E[Z] ≤ε(exp(ε) −1).\n• |Z|≤ ε, so that |Z−µ|≤ εexp(ε).\nThen E[exp(λZ)] = exp(λµ)·E[exp(λ(Z−µ))]. Since Z is in\na bounded range [−εexp(ε),ε exp(ε)] and f(x) = exp(λx) is\nconvex, we can boundf(x) by a linear interpolation between\nthe values at the two endpoints of the range. Basic calculus\nthen implies that\nE[f(Z)] ≤f(E[Z]) ·exp(λ2ε2 exp(2ε)/2),\nwhich concludes the proof.\nLemma C.1 and Theorem 2 give a way of getting a compo-\nsition theorem for diﬀerentially private mechanisms, which\nis roughly equivalent to unrolling the proof of the strong\ncomposition theorem of [22]. The power of the moments ac-\ncountant comes from the fact that, for many mechanisms of\nchoice, directly bounding in the moments gives a stronger\nguarantee than one would get by establishing diﬀerential\nprivacy and applying Lemma C.1.\nD. HYPERPARAMETER SEARCH\nHere we state Theorem 10.2 from [27] that we use to ac-\ncount for the cost of hyperparameter search.\nTheorem D.1 (Gupta et al. [27]).Let M be an ε -\ndiﬀerentially private mechanism such that for a query func-\ntion q with sensitivity 1, and a parameter Q, it holds that\nPrr∼M(d)[q(d,r) ≥Q] ≥pfor some p∈(0,1). Then for any\nδ >0 and any ε′ ∈(0,1\n2 ), there is a mechanism M′ which\nsatisﬁes the following properties:\n•Prr∼M′(d)\n[\nq(d,r) ≥Q−4\nε′ log( 1\nε′δp)\n]\n≥1 −δ.\n•M′ makes ( 1\nε′δp)2 log( 1\nε′δp) calls to M.\n•M′ is (ε+ 8ε′)-diﬀerentially private.\nSuppose that we have a diﬀerentially private mechanism\nMi for each of K choices of hyperparameters. Let ˜M be the\nmechanism that picks a random choice of hyperparameters,\nand runs the corresponding Mi. Let q(d,r) denote the num-\nber of examples from the validation set therlabels correctly,\nand let Q be a target accuracy. Assuming that one of the\nhyperparameter settings gets accuracy at least Q, ˜M satis-\nﬁes the pre-conditions of the theorem for p= 1\nK. Then with\nhigh probability, the mechanism implied by the theorem gets\naccuracy close to Q. We remark that the proof of Theo-\nrem D.1 actually implies a stronger max( ε,8ε′)-diﬀerential\nprivacy for the setting of interest here.\nPutting in some numbers, for a target accuracy of 95% on\na validation set of size 10,000, we get Q= 9,500. Thus, if,\nfor instance, we allow ε′= 0.5, and δ= 0.05, we lose at most\n1% in accuracy as long as 100 >8 ln 40\np . This is satisﬁed as\nlong as p≥ 1\n6700 . In other words, one can try 6,700 diﬀerent\nparameter settings at privacy cost ε = 4 for the validation\nset. In our experiments, we tried no more than a hundred\nsettings, so that this bound is easily satisﬁed. In practice,\nas our graphs show, p for our hyperparameter search is sig-\nniﬁcantly larger than 1\nK, so that a slightly smaller ε′should\nsuﬃce."
    }
}