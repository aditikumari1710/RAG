{
    "title": "arXiv:1409.3215v3  [cs.CL]  14 Dec 2014",
    "content": {
        "page_content": "arXiv:1409.3215v3  [cs.CL]  14 Dec 2014\nSequence to Sequence Learning\nwith Neural Networks\nIlya Sutskever\nGoogle\nilyasu@google.com\nOriol Vinyals\nGoogle\nvinyals@google.com\nQuoc V . Le\nGoogle\nqvl@google.com\nAbstract\nDeep Neural Networks (DNNs) are powerful models that have achieved excel-\nlent performance on difﬁcult learning tasks. Although DNNswork well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map theinput sequence\nto a vector of a ﬁxed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on anEnglish to French\ntranslation task from the WMT’14 dataset, the translationsproduced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not\nhave difﬁculty on long sentences. For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset. When we usedthe LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask. The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the pas-\nsive voice. Finally, we found that reversing the order of thewords in all source\nsentences (but not target sentences) improved the LSTM’s performance markedly,\nbecause doing so introduced many short term dependencies between the source\nand the target sentence which made the optimization problemeasier.\n1 Introduction\nDeep Neural Networks (DNNs) are extremely powerful machinelearning models that achieve ex-\ncellent performance on difﬁcult problems such as speech recognition [13, 7] and visual object recog-\nnition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\nN N -bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough\ninformation to specify the network’s parameters. Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humanscan solve the task very rapidly),\nsupervised backpropagation will ﬁnd these parameters and solve the problem.\nDespite their ﬂexibility and power, DNNs can only be appliedto problems whose inputs and targets\ncan be sensibly encoded with vectors of ﬁxed dimensionality. It is a signiﬁcant limitation, since\nmany important problems are best expressed with sequences whose lengths are not known a-priori.\nFor example, speech recognition and machine translation are sequential problems. Likewise, ques-\ntion answering can also be seen as mapping a sequence of wordsrepresenting the question to a\n1sequence of words representing the answer. It is therefore clear that a domain-independent method\nthat learns to map sequences to sequences would be useful.\nSequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\noutputs is known and ﬁxed. In this paper, we show that a straightforward application of the Long\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large ﬁxed-\ndimensional vector representation, and then to use anotherLSTM to extract the output sequence\nfrom that vector (ﬁg. 1). The second LSTM is essentially a recurrent neural network language model\n[28, 23, 30] except that it is conditioned on the input sequence. The LSTM’s ability to successfully\nlearn on data with long range temporal dependencies makes ita natural choice for this application\ndue to the considerable time lag between the inputs and theircorresponding outputs (ﬁg. 1).\nThere have been a number of related attempts to address the general sequence to sequence learning\nproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\nwho were the ﬁrst to map the entire input sentence to vector, and is related to Cho et al. [5] although\nthe latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\nferent parts of their input, and an elegant variant of this idea was successfully applied to machine\ntranslation by Bahdanau et al. [2]. The Connectionist Sequence Classiﬁcation is another popular\ntechnique for mapping sequences to sequences with neural networks, but it assumes a monotonic\nalignment between the inputs and the outputs [11].\nFigure 1:Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence. The\nmodel stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the\ninput sentence in reverse, because doing so introduces manyshort term dependencies in the data that make the\noptimization problem much easier.\nThe main result of this work is the following. On the WMT’14 English to French translation task,\nwe obtained a BLEU score of34.81by directly extracting translations from an ensemble of 5 deep\nLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-\nsearch decoder. This is by far the best result achieved by direct translation with large neural net-\nworks. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\nBLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\nwhenever the reference translation contained a word not covered by these 80k. This result shows\nthat a relatively unoptimized small-vocabulary neural network architecture which has much room\nfor improvement outperforms a phrase-based SMT system.\nFinally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\nthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).\nSurprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\nresearchers with related architectures [26]. We were able to do well on long sentences because we\nreversed the order of words in the source sentence but not thetarget sentences in the training and test\nset. By doing so, we introduced many short term dependenciesthat made the optimization problem\nmuch simpler (see sec. 2 and 3.3). As a result, SGD could learnLSTMs that had no trouble with\nlong sentences. The simple trick of reversing the words in the source sentence is one of the key\ntechnical contributions of this work.\nA useful property of the LSTM is that it learns to map an input sentence of variable length into\na ﬁxed-dimensional vector representation. Given that translations tend to be paraphrases of the\nsource sentences, the translation objective encourages the LSTM to ﬁnd sentence representations\nthat capture their meaning, as sentences with similar meanings are close to each other while different\n2sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\nis aware of word order and is fairly invariant to the active and passive voice.\n2 The model\nThe Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural\nnetworks to sequences. Given a sequence of inputs(x1, . . . , x T ), a standard RNN computes a\nsequence of outputs(y1, . . . , y T ) by iterating the following equation:\nht = sigm\n(\nW hxxt + W hhht−1\n)\nyt = W yhht\nThe RNN can easily map sequences to sequences whenever the alignment between the inputs the\noutputs is known ahead of time. However, it is not clear how toapply an RNN to problems whose\ninput and the output sequences have different lengths with complicated and non-monotonic relation-\nships.\nThe simplest strategy for general sequence learning is to map the input sequence to a ﬁxed-sized\nvector using one RNN, and then to map the vector to the target sequence with another RNN (this\napproach has also been taken by Cho et al. [5]). While it couldwork in principle since the RNN is\nprovided with all the relevant information, it would be difﬁcult to train the RNNs due to the resulting\nlong term dependencies (ﬁgure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)\n[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed\nin this setting.\nThe goal of the LSTM is to estimate the conditional probabilityp(y1, . . . , y T ′ |x1, . . . , x T ) where\n(x1, . . . , x T ) is an input sequence andy1, . . . , y T ′ is its corresponding output sequence whose length\nT ′ may differ fromT . The LSTM computes this conditional probability by ﬁrst obtaining the ﬁxed-\ndimensional representationv of the input sequence(x1, . . . , x T ) given by the last hidden state of the\nLSTM, and then computing the probability ofy1, . . . , y T ′ with a standard LSTM-LM formulation\nwhose initial hidden state is set to the representationv ofx1, . . . , x T :\np(y1, . . . , y T ′ |x1, . . . , x T ) =\nT ′\n∏\nt=1\np(yt|v, y 1, . . . , y t−1) (1)\nIn this equation, eachp(yt|v, y 1, . . . , y t−1) distribution is represented with a softmax over all the\nwords in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that\neach sentence ends with a special end-of-sentence symbol “<EOS >”, which enables the model to\ndeﬁne a distribution over sequences of all possible lengths. The overall scheme is outlined in ﬁgure\n1, where the shown LSTM computes the representation of “A”, “B”, “C”, “<EOS >” and then uses\nthis representation to compute the probability of “W”, “X”,“Y”, “Z”, “<EOS >”.\nOur actual models differ from the above description in threeimportant ways. First, we used two\ndifferent LSTMs: one for the input sequence and another for the output sequence, because doing\nso increases the number model parameters at negligible computational cost and makes it natural to\ntrain the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs\nsigniﬁcantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found\nit extremely valuable to reverse the order of the words of theinput sentence. So for example, instead\nof mapping the sentencea, b, c to the sentenceα, β, γ , the LSTM is asked to mapc, b, a toα, β, γ ,\nwhere α, β, γ is the translation ofa, b, c . This way,a is in close proximity toα ,b is fairly close toβ,\nand so on, a fact that makes it easy for SGD to “establish communication” between the input and the\noutput. We found this simple data transformation to greatlyimprove the performance of the LSTM.\n3 Experiments\nWe applied our method to the WMT’14 English to French MT task in two ways. We used it to\ndirectly translate the input sentence without using a reference SMT system and we it to rescore the\nn-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample\ntranslations, and visualize the resulting sentence representation.\n33.1 Dataset details\nWe used the WMT’14 English to French dataset. We trained our models on a subset of 12M sen-\ntences consisting of 348M French words and 304M English words, which is a clean “selected”\nsubset from [29]. We chose this translation task and this speciﬁc training set subset because of the\npublic availability of a tokenized training and test set together with 1000-best lists from the baseline\nSMT [29].\nAs typical neural language models rely on a vector representation for each word, we used a ﬁxed\nvocabulary for both languages. We used 160,000 of the most frequent words for the source language\nand 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was\nreplaced with a special “UNK” token.\n3.2 Decoding and Rescoring\nThe core of our experiments involved training a large deep LSTM on many sentence pairs. We\ntrained it by maximizing the log probability of a correct translationT given the source sentenceS,\nso the training objective is\n1/ |S|\n∑\n(T,S)∈S\nlog p(T |S)\nwhere S is the training set. Once training is complete, we produce translations by ﬁnding the most\nlikely translation according to the LSTM:\nˆT = arg max\nT\np(T |S) (2)\nWe search for the most likely translation using a simple left-to-right beam search decoder which\nmaintains a small numberB of partial hypotheses, where a partial hypothesis is a preﬁxof some\ntranslation. At each timestep we extend each partial hypothesis in the beam with every possible\nword in the vocabulary. This greatly increases the number ofthe hypotheses so we discard all but\ntheB most likely hypotheses according to the model’s log probability. As soon as the “<EOS >”\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses. While this decoder is approximate, it is simpleto implement. Interestingly, our system\nperforms well even with a beam size of 1, and a beam of size 2 provides most of the beneﬁts of beam\nsearch (Table 1).\nWe also used the LSTM to rescore the 1000-best lists producedby the baseline system [29]. To\nrescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took\nan even average with their score and the LSTM’s score.\n3.3 Reversing the Source Sentences\nWhile the LSTM is capable of solving problems with long term dependencies, we discovered that\nthe LSTM learns much better when the source sentences are reversed (the target sentences are not\nreversed). By doing so, the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU\nscores of its decoded translations increased from 25.9 to 30.6.\nWhile we do not have a complete explanation to this phenomenon, we believe that it is caused by\nthe introduction of many short term dependencies to the dataset. Normally, when we concatenate a\nsource sentence with a target sentence, each word in the source sentence is far from its corresponding\nword in the target sentence. As a result, the problem has a large “minimal time lag” [17]. By\nreversing the words in the source sentence, the average distance between corresponding words in\nthe source and target language is unchanged. However, the ﬁrst few words in the source language\nare now very close to the ﬁrst few words in the target language, so the problem’s minimal time lag is\ngreatly reduced. Thus, backpropagation has an easier time “establishing communication” between\nthe source sentence and the target sentence, which in turn results in substantially improved overall\nperformance.\nInitially, we believed that reversing the input sentences would only lead to more conﬁdent predic-\ntions in the early parts of the target sentence and to less conﬁdent predictions in the later parts. How-\never, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\n4trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences\nresults in LSTMs with better memory utilization.\n3.4 Training details\nWe found that the LSTM models are fairly easy to train. We useddeep LSTMs with 4 layers,\nwith 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary\nof 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to\nrepresent a sentence. We found deep LSTMs to signiﬁcantly outperform shallow LSTMs, where\neach additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden\nstate. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M\nparameters of which 64M are pure recurrent connections (32Mfor the “encoder” LSTM and 32M\nfor the “decoder” LSTM). The complete training details are given below:\n• We initialized all of the LSTM’s parameters with the uniformdistribution between -0.08\nand 0.08\n• We used stochastic gradient descent without momentum, witha ﬁxed learning rate of 0.7.\nAfter 5 epochs, we begun halving the learning rate every halfepoch. We trained our models\nfor a total of 7.5 epochs.\n• We used batches of 128 sequences for the gradient and dividedit the size of the batch\n(namely, 128).\n• Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\nexploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n25] by scaling it when its norm exceeded a threshold. For eachtraining batch, we compute\ns = ∥g∥2, whereg is the gradient divided by 128. Ifs > 5, we setg = 5g\ns .\n• Different sentences have different lengths. Most sentences are short (e.g., length 20-30)\nbut some sentences are long (e.g., length> 100), so a minibatch of 128 randomly chosen\ntraining sentences will have many short sentences and few long sentences, and as a result,\nmuch of the computation in the minibatch is wasted. To address this problem, we made sure\nthat all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.\n3.5 Parallelization\nA C++ implementation of deep LSTM with the conﬁguration fromthe previous section on a sin-\ngle GPU processes a speed of approximately 1,700 words per second. This was too slow for our\npurposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was\nexecuted on a different GPU and communicated its activations to the next GPU / layer as soon as\nthey were computed. Our models have 4 layers of LSTMs, each ofwhich resides on a separate\nGPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible\nfor multiplying by a1000 × 20000 matrix. The resulting implementation achieved a speed of 6,300\n(both English and French) words per second with a minibatch size of 128. Training took about a ten\ndays with this implementation.\n3.6 Experimental Results\nWe used the cased BLEU score [24] to evaluate the quality of our translations. We computed our\nBLEU scores usingmulti-bleu.pl1 on thetokenizedpredictions and ground truth. This way\nof evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].\nHowever, if we evaluate the best WMT’14 system [9] (whose predictions can be downloaded from\nstatmt.org\\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by\nstatmt.org\\matrix.\nThe results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs\nthat differ in their random initializations and in the random order of minibatches. While the decoded\ntranslations of the LSTM ensemble do not outperform the bestWMT’14 system, it is the ﬁrst time\nthat a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT\n1There several variants of the BLEU score, and each variant isdeﬁned with a perl script.\n5Method test BLEU score (ntst14)\nBahdanau et al. [2] 28.45\nBaseline System [29] 33.30\nSingle forward LSTM, beam size 12 26.17\nSingle reversed LSTM, beam size 12 30.59\nEnsemble of 5 reversed LSTMs, beam size 1 33.00\nEnsemble of 2 reversed LSTMs, beam size 12 33.27\nEnsemble of 5 reversed LSTMs, beam size 2 34.50\nEnsemble of 5 reversed LSTMs, beam size 12 34.81\nTable 1: The performance of the LSTM on WMT’14 English to French test set (ntst14). Note that\nan ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of\nsize 12.\nMethod test BLEU score (ntst14)\nBaseline System [29] 33.30\nCho et al. [5] 34.54\nBest WMT’14 result [9] 37.0\nRescoring the baseline 1000-best with a single forward LSTM 35.61\nRescoring the baseline 1000-best with a single reversed LSTM 35.85\nRescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs 36.5\nOracle Rescoring of the Baseline 1000-best lists ∼ 45\nTable 2: Methods that use neural networks together with an SMT system on the WMT’14 English\nto French test set (ntst14).\ntask by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is\nwithin 0.5 BLEU points of the best WMT’14 result if it is used to rescore the 1000-best list of the\nbaseline system.\n3.7 Performance on long sentences\nWe were surprised to discover that the LSTM did well on long sentences, which is shown quantita-\ntively in ﬁgure 3. Table 3 presents several examples of long sentences and their translations.\n3.8 Model Analysis\n−8 −6 −4 −2 0 2 4 6 8 10\n−6\n−5\n−4\n−3\n−2\n−1\n0\n1\n2\n3\n4\nJohn respects Mary\nMary respects John\nJohn admires Mary\nMary admires John\nMary is in love with John\nJohn is in love with Mary\n−15 −10 −5 0 5 10 15 20\n−20\n−15\n−10\n−5\n0\n5\n10\n15\nI gave her a card in the garden\nIn the garden , I gave her a card\nShe was given a card by me in the garden\nShe gave me a card in the garden\nIn the garden , she gave me a card\nI was given a card by her in the garden\nFigure 2:The ﬁgure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained\nafter processing the phrases in the ﬁgures. The phrases are clustered by meaning, which in these examples is\nprimarily a function of word order, which would be difﬁcult to capture with a bag-of-words model. Notice that\nboth clusters have similar internal structure.\nOne of the attractive features of our model is its ability to turn a sequence of words into a vector\nof ﬁxed dimensionality. Figure 2 visualizes some of the learned representations. The ﬁgure clearly\nshows that the representations are sensitive to the order ofwords, while being fairly insensitive to the\n6Type Sentence\nOur model Ulrich UNK , membre du conseil d’ administration du constructeur automobile Audi ,\nafﬁrme qu’ il s’ agit d’ une pratique courante depuis des ann´ees pour que les t´ el´ ephones\nportables puissent ˆ etre collect´ es avant les r´ eunions duconseil d’ administration aﬁn qu’ ils\nne soient pas utilis´ es comme appareils d’ ´ ecoute ` a distance .\nTruth Ulrich Hackenberg , membre du conseil d’ administration du constructeur automobile Audi ,\nd´ eclare que la collecte des t´ el´ ephones portables avant les r´ eunions du conseil , aﬁn qu’ ils\nne puissent pas ˆ etre utilis´ es comme appareils d’ ´ ecoute `a distance , est une pratique courante\ndepuis des ann´ ees .\nOur model “ Les t´ el´ ephones cellulaires , qui sont vraiment une question , non seulement parce qu’ ils\npourraient potentiellement causer des interf´ erences avec les appareils de navigation , mais\nnous savons , selon la FCC , qu’ ils pourraient interf´ erer avec les tours de t´ el´ ephone cellulaire\nlorsqu’ ils sont dans l’ air ” , dit UNK .\nTruth “ Les t´ el´ ephones portables sont v´ eritablement un probl`eme , non seulement parce qu’ ils\npourraient ´ eventuellement cr´ eer des interf´ erences avec les instruments de navigation , mais\nparce que nous savons , d’ apr` es la FCC , qu’ ils pourraient perturber les antennes-relais de\nt´ el´ ephonie mobile s’ ils sont utilis´ es ` a bord ” , a d´ eclar´ e Rosenker .\nOur model Avec la cr´ emation , il y a un “ sentiment de violence contre lecorps d’ un ˆ etre cher ” ,\nqui sera “ r´ eduit ` a une pile de cendres ” en tr` es peu de tempsau lieu d’ un processus de\nd´ ecomposition “ qui accompagnera les ´ etapes du deuil ” .\nTruth Il y a , avec la cr´ emation , “ une violence faite au corps aim´ e” ,\nqui va ˆ etre “ r´ eduit ` a un tas de cendres ” en tr` es peu de temps , et non apr` es un processus de\nd´ ecomposition , qui “ accompagnerait les phases du deuil ” .\nTable 3: A few examples of long translations produced by the LSTM alongside the ground truth\ntranslations. The reader can verify that the translations are sensible using Google translate.\n4 7 8 12 17 22 28 35 79\ntest sentences sorted by their length\n20\n25\n30\n35\n40BLEU score\nLSTM  (34.8)\nbaseline (33.3)\n0 500 1000 1500 2000 2500 3000 3500\ntest sentences sorted by average word frequency rank\n20\n25\n30\n35\n40BLEU score\nLSTM  (34.8)\nbaseline (33.3)\nFigure 3:The left plot shows the performance of our system as a function of sentence length, where the\nx-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths.\nThere is no degradation on sentences with less than 35 words,there is only a minor degradation on the longest\nsentences. The right plot shows the LSTM’s performance on sentences with progressively more rare words,\nwhere the x-axis corresponds to the test sentences sorted bytheir “average word frequency rank”.\nreplacement of an active voice with a passive voice. The two-dimensional projections are obtained\nusing PCA.\n4 Related work\nThere is a large body of work on applications of neural networks to machine translation. So far,\nthe simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a\n7Feedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the n-\nbest lists of a strong MT baseline [22], which reliably improves translation quality.\nMore recently, researchers have begun to look into ways of including information about the source\nlanguage into the NNLM. Examples of this work include Auli etal. [1], who combine an NNLM\nwith a topic model of the input sentence, which improves rescoring performance. Devlin et al. [8]\nfollowed a similar approach, but they incorporated their NNLM into the decoder of an MT system\nand used the decoder’s alignment information to provide theNNLM with the most useful words in\nthe input sentence. Their approach was highly successful and it achieved large improvements over\ntheir baseline.\nOur work is closely related to Kalchbrenner and Blunsom [18], who were the ﬁrst to map the input\nsentence into a vector and then back to a sentence, although they map sentences to vectors using\nconvolutional neural networks, which lose the ordering of the words. Similarly to this work, Cho et\nal. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their\nprimary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also\nattempted direct translations with a neural network that used an attention mechanism to overcome\nthe poor performance on long sentences experienced by Cho etal. [5] and achieved encouraging\nresults. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et\nal. [5] by translating pieces of the source sentence in way that produces smooth translations, which\nis similar to a phrase-based approach. We suspect that they could achieve similar improvements by\nsimply training their networks on reversed source sentences.\nEnd-to-end training is also the focus of Hermann et al. [12],whose model represents the inputs and\noutputs by feedforward networks, and map them to similar points in space. However, their approach\ncannot generate translations directly: to get a translation, they need to do a look up for closest vector\nin the pre-computed database of sentences, or to rescore a sentence.\n5 Conclusion\nIn this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes\nalmost no assumption about problem structure can outperform a standard SMT-based system whose\nvocabulary is unlimited on a large-scale MT task. The success of our simple LSTM-based approach\non MT suggests that it should do well on many other sequence learning problems, provided they\nhave enough training data.\nWe were surprised by the extent of the improvement obtained by reversing the words in the source\nsentences. We conclude that it is important to ﬁnd a problem encoding that has the greatest number\nof short term dependencies, as they make the learning problem much simpler. In particular, while\nwe were unable to train a standard RNN on the non-reversed translation problem (shown in ﬁg. 1),\nwe believe that a standard RNN should be easily trainable when the source sentences are reversed\n(although we did not verify it experimentally).\nWe were also surprised by the ability of the LSTM to correctlytranslate very long sentences. We\nwere initially convinced that the LSTM would fail on long sentences due to its limited memory,\nand other researchers reported poor performance on long sentences with a model similar to ours\n[5, 2, 26]. And yet, LSTMs trained on the reversed dataset hadlittle difﬁculty translating long\nsentences.\nMost importantly, we demonstrated that a simple, straightforward and a relatively unoptimized ap-\nproach can outperform an SMT system, so further work will likely lead to even greater translation\naccuracies. These results suggest that our approach will likely do well on other challenging sequence\nto sequence problems.\n6 Acknowledgments\nWe thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolf-\ngang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba, and the Google Brain team\nfor useful comments and discussions.\n8References\n[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint languageand translation modeling with recurrent\nneural networks. InEMNLP , 2013.\n[2] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473, 2014.\n[3] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. InJournal of\nMachine Learning Research, pages 1137–1155, 2003.\n[4] Y . Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.\nIEEE Transactions on Neural Networks, 5(2):157–166, 1994.\n[5] K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y . Bengio. Learning phrase represen-\ntations using RNN encoder-decoder for statistical machinetranslation. InArxiv preprint arXiv:1406.1078,\n2014.\n[6] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.\nInCVPR , 2012.\n[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large\nvocabulary speech recognition.IEEE Transactions on Audio, Speech, and Language Processing - Special\nIssue on Deep Learning for Speech and Language Processing, 2012.\n[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J.Makhoul. Fast and robust neural network\njoint models for statistical machine translation. InACL , 2014.\n[9] Nadir Durrani, Barry Haddow, Philipp Koehn, and KennethHeaﬁeld. Edinburgh’s phrase-based machine\ntranslation systems for wmt-14. InWMT , 2014.\n[10] A. Graves. Generating sequences with recurrent neuralnetworks. InArxiv preprint arXiv:1308.0850,\n2013.\n[11] A. Graves, S. Fern´ andez, F. Gomez, and J. Schmidhuber.Connectionist temporal classiﬁcation: labelling\nunsegmented sequence data with recurrent neural networks.InICML , 2006.\n[12] K. M. Hermann and P. Blunsom. Multilingual distributedrepresentations without word alignment. In\nICLR , 2014.\n[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly,A. Senior, V . Vanhoucke, P. Nguyen,\nT. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition.IEEE\nSignal Processing Magazine, 2012.\n[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen.Master's thesis, Institut fur Infor-\nmatik, Technische Universitat, Munchen, 1991.\n[15] S. Hochreiter, Y . Bengio, P. Frasconi, and J. Schmidhuber. Gradient ﬂow in recurrent nets: the difﬁculty\nof learning long-term dependencies, 2001.\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.Neural Computation, 1997.\n[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997.\n[18] N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. InEMNLP , 2013.\n[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNetclassiﬁcation with deep convolutional neural\nnetworks. InNIPS, 2012.\n[20] Q.V . Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y . Ng. Building\nhigh-level features using large scale unsupervised learning. InICML , 2012.\n[21] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 1998.\n[22] T. Mikolov.Statistical Language Models based on Neural Networks. PhD thesis, Brno University of\nTechnology, 2012.\n[23] T. Mikolov, M. Karaﬁ´ at, L. Burget, J. Cernock` y, and S.Khudanpur. Recurrent neural network based\nlanguage model. InINTERSPEECH , pages 1045–1048, 2010.\n[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machine\ntranslation. InACL , 2002.\n[25] R. Pascanu, T. Mikolov, and Y . Bengio. On the difﬁculty of training recurrent neural networks.arXiv\npreprint arXiv:1211.5063, 2012.\n[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y . Bengio. Overcoming the\ncurse of sentence length for neural machine translation using automatic segmentation.arXiv preprint\narXiv:1409.1257, 2014.\n[27] A. Razborov. On small depth threshold circuits. InProc. 3rd Scandinavian Workshop on Algorithm\nTheory, 1992.\n[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learningrepresentations by back-propagating errors.\nNature, 323(6088):533–536, 1986.\n[29] H. Schwenk. University le mans.http://www-lium.univ-lemans.fr/˜schwenk/cslm_\njoint_paper/, 2014. [Online; accessed 03-September-2014].\n[30] M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. InINTER-\nSPEECH , 2010.\n[31] P. Werbos. Backpropagation through time: what it does and how to do it.Proceedings of IEEE, 1990.\n9"
    }
}