{
    "title": "Deep Learning Recommendation Model for",
    "content": {
        "page_content": "Deep Learning Recommendation Model for\nPersonalization and Recommendation Systems\nMaxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi∗, Jianyu Huang,\nNarayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta†, Carole-Jean Wu,\nAlisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu,\nRaghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira,\nXianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong and Misha Smelyanskiy\nFacebook, 1 Hacker Way, Menlo Park, CA 94065\n{mnaumov,dheevatsa}@fb.com\nAbstract\nWith the advent of deep learning, neural network-based recommendation models\nhave emerged as an important tool for tackling personalization and recommendation\ntasks. These networks differ signiﬁcantly from other deep learning networks due\nto their need to handle categorical features and are not well studied or understood.\nIn this paper, we develop a state-of-the-art deep learning recommendation model\n(DLRM) and provide its implementation in both PyTorch and Caffe2 frameworks.\nIn addition, we design a specialized parallelization scheme utilizing model paral-\nlelism on the embedding tables to mitigate memory constraints while exploiting\ndata parallelism to scale-out compute from the fully-connected layers. We compare\nDLRM against existing recommendation models and characterize its performance\non the Big Basin AI platform, demonstrating its usefulness as a benchmark for\nfuture algorithmic experimentation and system co-design.\n1 Introduction\nPersonalization and recommendation systems are currently deployed for a variety of tasks at large\ninternet companies, including ad click-through rate (CTR) prediction and rankings. Although these\nmethods have had long histories, these approaches have only recently embraced neural networks.\nTwo primary perspectives contributed towards the architectural design of deep learning models for\npersonalization and recommendation.\nThe ﬁrst comes from the view of recommendation systems. These systems initially employed content\nﬁltering where a set of experts classiﬁed products into categories, while users selected their preferred\ncategories and were matched based on their preferences [22]. The ﬁeld subsequently evolved to use\ncollaborative ﬁltering, where recommendations are based on past user behaviors, such as prior ratings\ngiven to products. Neighborhood methods [21] that provide recommendations by grouping users and\nproducts together and latent factor methods that characterize users and products by certain implicit\nfactors via matrix factorization techniques [9, 17] were later deployed with success.\nThe second view comes from predictive analytics, which relies on statistical models to classify or\npredict the probability of events based on the given data [5]. Predictive models shifted from using\nsimple models such as linear and logistic regression [26] to models that incorporate deep networks.\nIn order to process categorical data, these models adopted the use of embeddings, which transform\nthe one- and multi-hot vectors into dense representations in an abstract space [ 20]. This abstract\nspace may be interpreted as the space of the latent factors found by recommendation systems.\n∗Northwestern University, †Harvard University, work done while at Facebook.\nPreprint. Under review.\narXiv:1906.00091v1  [cs.IR]  31 May 2019In this paper, we introduce a personalization model that was conceived by the union of the two\nperspectives described above. The model uses embeddings to process sparse features that represent\ncategorical data and a multilayer perceptron (MLP) to process dense features, then interacts these\nfeatures explicitly using the statistical techniques proposed in [ 24]. Finally, it ﬁnds the event\nprobability by post-processing the interactions with another MLP. We refer to this model as a deep\nlearning recommendation model (DLRM); see Fig. 1. A PyTorch and Caffe2 implementation of this\nmodel will be released for testing and experimentation with the publication of this manuscript.\n2 Model Design and Architecture\nIn this section, we will describe the design of DLRM. We will begin with the high level components\nof the network and explain how and why they have been assembled together in a particular way, with\nimplications for future model design, then characterize the low level operators and primitives that\nmake up the model, with implications for future hardware and system design.\n2.1 Components of DLRM\nFigure 1: A deep learning recommendation model\nThe high-level components of the DLRM can be\nmore easily understood by reviewing early mod-\nels. We will avoid the full scientiﬁc literature\nreview and focus instead on the four techniques\nused in early models that can be interpreted as\nsalient high-level components of the DLRM.\n2.1.1 Embeddings\nIn order to handle categorical data, embeddings\nmap each category to a dense representation in\nan abstract space. In particular, each embedding\nlookup may be interpreted as using a one-hot\nvector ei (with the i-th position being 1 while\nothers are 0, where index icorresponds to i-th\ncategory) to obtain the corresponding row vector\nof the embedding table W ∈Rm×d as follows\nwT\ni = eT\ni W. (1)\nIn more complex scenarios, an embedding can also represent a weighted combination of multiple\nitems, with a multi-hot vector of weights aT = [0,...,a i1 ,...,a ik ,..., 0], with elements ai ̸= 0for\ni = i1,...,i k and 0 everywhere else, where i1,...,i k index the corresponding items. Note that a\nmini-batch of tembedding lookups can hence be written as\nS = AT W (2)\nwhere sparse matrix A= [a1,..., at] [20].\nDLRMs will utilize embedding tables for mapping categorical features to dense representations.\nHowever, even after these embeddings are meaningfully devised, how are they to be exploited to\nproduce accurate predictions? To answer this, we return to latent factor methods.\n2.1.2 Matrix Factorization\nRecall that in the typical formulation of the recommendation problem, we are given a set Sof users\nthat have rated some products. We would like to represent the i-th product by a vector wi ∈Rd for\ni= 1,...,n and j-th user by a vector vj ∈Rd for j = 1,...,m to ﬁnd all the ratings, where nand m\ndenote the total number of products and users, respectively. More rigorously, the setSconsists of\ntuples (i,j) indexing when the i-th product has been rated by the j-th user.\nThe matrix factorization approach solves this problem by minimizing\nmin\n∑\n(i,j)∈S\nrij −wT\ni vj (3)\n2where rij ∈R is the rating of the i-th product by the j-th user for i = 1,...,m and j = 1,...,n .\nThen, letting WT = [w1,..., wm] and VT = [v1,..., vn], we may approximate the full matrix of\nratings R= [rij] as the matrix product R≈WV T . Note that W and V may be interpreted as two\nembedding tables, where each row represents a user/product in a latent factor space2 [17]. The dot\nproduct of these embedding vectors yields a meaningful prediction of the subsequent rating, a key\nobservation to the design of factorization machines and DLRM.\n2.1.3 Factorization Machine\nIn classiﬁcation problems, we want to deﬁne a prediction function φ : Rn →T from an input\ndatapoint x ∈Rn to a target label y∈T. As an example, we can predict the click-through rate by\ndeﬁning T = {+1,−1}with +1 denoting the presence of a click and −1 as the absence of a click.\nFactorization machines (FM) incorporate second-order interactions into a linear model with categori-\ncal data by deﬁning a model of the form\nˆy= b+ wT x + xT upper(VV T )x (4)\nwhere V ∈Rn×d, w ∈Rn, and b∈R are the parameters with d≪n, and upper selects the strictly\nupper triangular part of the matrix [24].\nFMs are notably distinct from support vector machines (SVMs) with polynomial kernels [4] because\nthey factorize the second-order interaction matrix into its latent factors (or embedding vectors) as\nin matrix factorization, which more effectively handles sparse data. This signiﬁcantly reduces the\ncomplexity of the second-order interactions by only capturing interactions between pairs of distinct\nembedding vectors, yielding linear computational complexity.\n2.1.4 Multilayer Perceptrons\nSimultaneously, much recent success in machine learning has been due to the rise of deep learning.\nThe most fundamental model of these is the multilayer perceptron (MLP), a prediction function\ncomposed of an interleaving sequence of fully connected (FC) layers and an activation function\nσ: R →R applied componentwise as shown below\nˆy= Wkσ(Wk−1σ(...σ(W1x + b1)...) +bk−1) +bk (5)\nwhere weight matrix Wl ∈Rnl×nl−1 , bias bl ∈Rnl for layer l= 1,...,k .\nThese methods have been used to capture more complex interactions. It has been shown, for example,\nthat given enough parameters, MLPs with sufﬁcient depth and width can ﬁt data to arbitrary precision\n[1]. Variations of these methods have been widely used in various applications including computer\nvision and natural language processing. One speciﬁc case, Neural Collaborative Filtering (NCF)\n[15, 25] used as part of the MLPerf benchmark [19], uses an MLP rather than dot product to compute\ninteractions between embeddings in matrix factorization.\n2.2 DLRM Architecture\nSo far, we have described different models used in recommendation systems and predictive analytics.\nLet us now combine their intuitions to build a state-of-the-art personalization model.\nLet the users and products be described by many continuous and categorical features. To process\nthe categorical features, each categorical feature will be represented by an embedding vector of the\nsame dimension, generalizing the concept of latent factors used in matrix factorization (3). To handle\nthe continuous features, the continuous features will be transformed by an MLP (which we call the\nbottom or dense MLP) which will yield a dense representation of the same length as the embedding\nvectors (5).\nWe will compute second-order interaction of different features explicitly, following the intuition for\nhandling sparse data provided in FMs (4), optionally passing them through MLPs. This is done by\ntaking the dot product between all pairs of embedding vectors and processed dense features. These\ndot products are concatenated with the original processed dense features and post-processed with\nanother MLP (the top or output MLP) (5), and fed into a sigmoid function to give a probability.\n2This problem is different from low-rank approximation, which can be solved by SVD [11], because not all entries of matrixR are known.\n3We refer to the resulting model as DLRM, shown in Fig. 1. We show some of the operators used in\nDLRM in PyTorch [23] and Caffe2 [8] frameworks in Table 1.\nEmbedding MLP Interactions Loss\nPyTorch nn.EmbeddingBag nn.Linear/addmm matmul/bmm nn.CrossEntropyLoss\nCaffe2 SparseLengthSum FC BatchMatMul CrossEntropy\nTable 1: DLRM operators by framework\n2.3 Comparison with Prior Models\nMany deep learning-based recommendation models [3, 13, 27, 18, 28, 29] use similar underlying\nideas to generate higher-order terms to handle sparse features. Wide and Deep, Deep and Cross,\nDeepFM, and xDeepFM networks, for example, design specialized networks to systematically\nconstruct higher-order interactions. These networks then sum the results from both their specialized\nmodel and an MLP, passing this through a linear layer and sigmoid activation to yield a ﬁnal\nprobability. DLRM speciﬁcally interacts embeddings in a structured way that mimics factorization\nmachines to signiﬁcantly reduce the dimensionality of the model by only considering cross-terms\nproduced by the dot-product between pairs of embeddings in the ﬁnal MLP. We argue that higher-\norder interactions beyond second-order found in other networks may not necessarily be worth the\nadditional computational/memory cost.\nA key difference between DLRM and other networks is in how these networks treat embedded feature\nvectors and their cross-terms. In particular, DLRM (and xDeepFM [18]) interpret each feature vector\nas a single unit representing a single category, whereas networks like Deep and Cross treat each\nelement in the feature vector as a new unit that should yield different cross-terms. Hence, Deep and\nCross networks will produce cross-terms not only between elements from different feature vectors\nas in DLRM via the dot product, but also produce cross-terms between elements within the same\nfeature vector, resulting in higher dimensionality.\n3 Parallelism\nModern personalization and recommendation systems require large and complex models to capitalize\non vast amounts of data. DLRMs particularly contain a very large number of parameters, up to\nmultiple orders of magnitude more than other common deep learning models like convolutional\nneural networks (CNN), transformer and recurrent networks (RNN), and generative networks (GAN).\nThis results in training times up to several weeks or more. Hence, it is important to parallelize these\nmodels efﬁciently in order to solve these problems at practical scales.\nAs described in the previous section, DLRMs process both categorical features (with embeddings)\nand continuous features (with the bottom MLP) in a coupled manner. Embeddings contribute the\nmajority of the parameters, with several tables each requiring in excess of multiple GBs of memory,\nmaking DLRM memory-capacity and bandwidth intensive. The size of the embeddings makes it\nprohibitive to use data parallelism since it requires replicating large embeddings on every device. In\nmany cases, this memory constraint necessitates the distribution of the model across multiple devices\nto be able satisfy memory capacity requirements.\nOn the other hand, the MLP parameters are smaller in memory but translate into sizeable amounts of\ncompute. Hence, data-parallelism is preferred for MLPs since this enables concurrent processing\nof the samples on different devices and only requires communication when accumulating updates.\nOur parallelized DLRM will use a combination of model parallelism for the embeddings and data\nparallelism for the MLPs to mitigate the memory bottleneck produced by the embeddings while\nparallelizing the forward and backward propagations over the MLPs. Combined model and data\nparallelism is a unique requirement of DLRM as a result of its architecture and large model sizes.\nSuch combined parallelism is not supported in either Caffe2 or PyTorch (as well as other popular\ndeep learning frameworks), therefore we design a custom implementation. We plan to provide its\ndetailed performance study in forthcoming work.\nIn our setup, the top MLP and the interaction operator require access to part of the mini-batch from\nthe bottom MLP and all of the embeddings. Since model parallelism has been used to distribute the\nembeddings across devices, this requires a personalized all-to-all communication [12]. At the end of\nthe embedding lookup, each device has a vector for the embedding tables resident on those devices\nfor all the samples in the mini-batch, which needs to be split along the mini-batch dimension and\n4Figure 2: Butterﬂy shufﬂe for the all-to-all (personalized) communication\ncommunicated to the appropriate devices, as shown in Fig. 2. Neither PyTorch nor Caffe2 provide\nnative support for model parallelism; therefore, we have implemented it by explicitly mapping the\nembedding operators (nn.EmbeddingBag for PyTorch, SparseLengthSum for Caffe2) to different\ndevices. Then personalized all-to-all communication is implemented using the butterﬂy shufﬂe\noperator, which appropriately slices the resulting embedding vectors and transfers them to the target\ndevices. In the current version, these transfers are explicit copies, but we intend to further optimize\nthis using the available communication primitives (such as all-gather and send-recv).\nWe note that for the data parallel MLPs, the parameter updates in the backward pass are accu-\nmulated with an allreduce3 and applied to the replicated parameters on each device [ 12] in a\nsynchronous fashion, ensuring the updated parameters on each device are consistent before every\niteration. In PyTorch, data parallelism is enabled through the nn.DistributedDataParallel and\nnn.DataParallel modules that replicate the model on each device and insert allreduce with the\nnecessary dependencies. In Caffe2, we manually insert allreduce before the gradient update.\n4 Data\nIn order to measure the accuracy of the model, test its overall performance, and characterize the\nindividual operators, we need to create or obtain a data set for our implementation. Our current\nimplementation of the model supplies three types of data sets: random, synthetic and public data sets.\nThe former two data sets are useful in experimenting with the model from the systems perspective.\nIn particular, it permits us to exercise different hardware properties and bottlenecks by generating\ndata on the ﬂy while removing dependencies on data storage systems. The latter allows us to perform\nexperiments on real data and measure the accuracy of the model.\n4.1 Random\nRecall that DLRM accepts continuous and categorical features as inputs. The former can be modeled\nby generating a vector of random numbers using either a uniform or normal (Gaussian) distributions\nwith the numpy.random package rand or randn calls with default parameters. Then a mini-batch\nof inputs can be obtained by generating a matrix where each row corresponds to an element in the\nmini-batch.\nTo generate categorical features, we need to determine how many non-zero elements we would like\nhave in a given multi-hot vector. The benchmark allows this number to be either ﬁxed or random\nwithin a range4 [1,k]. Then, we generate the corresponding number of integer indices, within a\nrange [1,m], where mis the number of rows in the embedding W in (2). Finally, in order to create a\nmini-batch of lookups, we concatenate the above indices and delineate each individual lookup with\nlengths (SparseLengthsSum) or offsets (nn.EmbeddingBag)5.\n3Optimized implementations for the allreduce op. include Nvidia’s NCCL [16] and Facebook’s gloo [7].\n4see options --num-indices-per-lookup=k and --num-indices-per-lookup-fixed\n5For instance, in order to represent three embedding lookups, with indices {0, 2}, {0, 1, 5}and {3}we use\nlengths/offsets = {2, 3, 1}/{0, 2, 5}\nindices = {0, 2, 0, 1, 5, 3}\nNote that this format resembles Compressed-Sparse Row (CSR) often used for sparse matrices in linear algebra.\n54.2 Synthetic\nThere are many reasons to support custom generation of indices corresponding to categorical features.\nFor instance, if our application uses a particular data set, but we would not like to share it for privacy\npurposes, then we may choose to express the categorical features through distributions. This could\npotentially serve as an alternative to the privacy preserving techniques used in applications such as\nfederated learning [2, 10]. Also, if we would like to exercise system components, such as studying\nmemory behavior, we may want to capture fundamental locality of accesses of original trace within\nsynthetic trace.\nLet us now illustrate how we can use a synthetic data set. Assume that we have a trace of indices\nthat correspond to embedding lookups for a single categorical feature (and repeat the process for all\nfeatures). We can record the unique accesses and frequency of distances between repeated accesses\nin this trace (Alg. 1) and then generate a synthetic trace (Alg. 2) as proposed in [14].\nAlgorithm 1Proﬁle (Original) Trace\n1: Let tr be input sequence, s stack of distances, u list of unique accesses and p probability\ndistribution\n2: Let s.position_from_the_top return d= 0if the index is not found, and d> 0 otherwise.\n3: for i=0; i<length(tr); i++ do\n4: a = tr[i]\n5: d = s.position_from_the_top(a)\n6: if d == 0 then\n7: u.append(a)\n8: else\n9: s.remove_from_the_top_at_position(d)\n10: end if\n11: p[d] += 1.0/length(tr)\n12: s.push_to_the_top(a)\n13: end for\nAlgorithm 2Generate (Synthetic) Trace\n1: Let u be input list of unique accesses and p probability distribution of distances, while tr output\ntrace.\n2: for s=0, i=0; i<length; i++ do\n3: d = p.sample_from_distribution_with_support(0,s)\n4: if d == 0 then\n5: a = u.remove_from_front()\n6: s++\n7: else\n8: a = u.remove_from_the_back_at_position(d)\n9: end if\n10: u.append(a)\n11: tr[i] = a\n12: end for\nNote that we can only generate a stack distance up to s number of unique accesses we have seen so\nfar, therefore s is used to control the support of the distribution p in Alg. 2. Given a ﬁxed number of\nunique accesses, the longer input trace will result in lower probability being assigned to them in Alg.\n1, which will lead to longer time to achieve full distribution support in Alg. 2. In order to address\nthis problem, we increase the probability for the unique accesses up to a minimum threshold and\nadjust support to remove unique accesses from it once all have been seen. A visual comparison of\nprobability distribution p based on original and synthetic traces is shown in Fig. 3. In our experiments\noriginal and adjusted synthetic traces produce similar cache hit/miss rates.\nAlg. 1 and 2 were designed for more accurate cache simulations, but they illustrate a general idea of\nhow probability distributions can be used to generate synthetic traces with desired properties.\n6(a) original\n (b) synthetic trace\n (c) adjusted synthetic trace\nFigure 3: Probability distribution p based on a sample trace tr = random.uniform(1,100,100K)\n4.3 Public\nFew public data sets are available for recommendation and personalization systems. The Criteo AI\nLabs Ad Kaggle6 and Terabyte7 data sets are open-sourced data sets consisting of click logs for\nad CTR prediction. Each data set contains 13 continuous and 26 categorical features. Typically\nthe continuous features are pre-processed with a simple log transform log(1 +x). The categorical\nfeature are mapped to its corresponding embedding index, with unlabeled categorical features or\nlabels mapped to 0 or NULL.\nThe Criteo Ad Kaggle data set contains approximately 45 million samples over 7 days. In experiments,\ntypically the 7th day is split into a validation and test set while the ﬁrst 6 days are used as the training\nset. The Criteo Ad Terabyte data set is sampled over 24 days, where the 24th day is split into\na validation and test set and the ﬁrst 23 days is used as a training set. Note that there are an\napproximately equal number of samples from each day.\n5 Experiments\nFigure 4: Big Basin AI platform\nLet us now illustrate the performance and accuracy of DLRM.\nThe model is implemented in PyTorch and Caffe2 frameworks\nand is available on GitHub8. It uses fp32 ﬂoating point and\nint32(Caffe2)/int64(PyTorch) types for model parameters\nand indices, respectively. The experiments are performed on\nthe Big Basin platform with Dual Socket Intel Xeon 6138 CPU\n@ 2.00GHz and eight Nvidia Tesla V100 16GB GPUs, publicly\navailable through the Open Compute Project9, shown in Fig. 4.\n5.1 Model Accuracy on Public Data Sets\nWe evaluate the accuracy of the model on Criteo Ad Kaggle data set and compare the performance of\nDLRM against a Deep and Cross network (DCN) as-is without extensive tuning [27]. We compare\nwith DCN because it is one of the few models that has comprehensive results on the same data set.\nNotice that in this case the models are sized to accommodate the number of features present in the\ndata set. In particular, DLRM consists of both a bottom MLP for processing dense features consisting\nof three hidden layers with 512, 256 and 64 nodes, respectively, and a top MLP consisting of two\nhidden layers with 512 and 256 nodes. On the other hand DCN consists of six cross layers and a\ndeep network with 512 and 256 nodes. An embedding dimension of 16 is used. Note that this yields\na DLRM and DCN both with approximately 540M parameters.\nWe plot both the training (solid) and validation (dashed) accuracies over a full single epoch of training\nfor both models with SGD and Adagrad optimizers [6]. No regularization is used. In this experiment,\nDLRM obtains slightly higher training and validation accuracy, as shown in Fig. 5. We emphasize\nthat this is without extensive tuning of model hyperparameters.\n6https://www.kaggle.com/c/criteo-display-ad-challenge\n7https://labs.criteo.com/2013/12/download-terabyte-click-logs/\n8https://github.com/facebookresearch/dlrm\n9https://www.opencompute.org\n70.0 0.5 1.0 1.5 2.0 2.5 3.0\nIterations 1e5\n0.75\n0.76\n0.77\n0.78\n0.79Accuracy\nKaggle DLRM\nDLRM\nDCN\n(a) SGD\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nIterations 1e5\n0.770\n0.775\n0.780\n0.785\n0.790\n0.795Accuracy\nKaggle DLRM\nDLRM\nDCN (b) Adagrad\nFigure 5: Comparison of training (solid) and validation (dashed) accuracies of DLRM and DCN\n5.2 Model Performance on a Single Socket/Device\nTo proﬁle the performance of our model on a single socket device, we consider a sample model with\n8 categorical features and 512 continuous features. Each categorical feature is processed through\nan embedding table with 1M vectors, with vector dimension 64, while the continuous features are\nassembled into a vector of dimension 512. Let the bottom MLP have two layers, while the top\nMLP has four layers. We proﬁle this model on a data set with 2048Krandomly generated samples\norganized into 1Kmini-batches10.\n(a) Caffe2\n (b) PyTorch\nFigure 6: Proﬁling of a sample DLRM on a single socket/device\nThis model implementation in Caffe2 runs in around 256 seconds on the CPU and 62 seconds on the\nGPU, with proﬁling of individual operators shown in Fig. 6. As expected, the majority of time is\nspent performing embedding lookups and fully connected layers. On the CPU, fully connected layers\ntake a signiﬁcant portion of the computation, while on the GPU they are almost negligible.\n6 Conclusion\nIn this paper, we have proposed and open-sourced a novel deep learning-based recommendation\nmodel that exploits categorical data. Although recommendation and personalization systems still\ndrive much practical success of deep learning within industry today, these networks continue to\nreceive little attention in the academic community. By providing a detailed description of a state-of-\nthe-art recommendation system and its open-source implementation, we hope to draw attention to the\nunique challenges that this class of networks present in an accessible way for the purpose of further\nalgorithmic experimentation, modeling, system co-design, and benchmarking.\n10 For instance, this conﬁguration can be achieved with the following command line arguments\n--arch-embedding-size=1000000-1000000-1000000-1000000-1000000-1000000-1000000-1000000\n--arch-sparse-feature-size=64 --arch-mlp-bot=512-512-64 --arch-mlp-top=1024-1024-1024-1\n--data-generation=random --mini-batch-size=2048 --num-batches=1000 --num-indices-per-lookup=100 [--use-gpu]\n[--enable-profiling]\n8Acknowledgments\nThe authors would like to acknowledge AI Systems Co-Design, Caffe2, PyTorch and AML team\nmembers for their help in reviewing this document.\nReferences\n[1] Christopher M. Bishop. Neural Networks for Pattern Recognition. The Oxford University Press, 1st edition,\n1995.\n[2] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov,\nChloé Kiddon, Jakub Koneˇcný, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David\nPetrou, Daniel Ramage, and Jason Roselander. Towards federated learning at scale: System design. In\nProc. 2nd Conference on Systems and Machine Learning (SysML), 2019.\n[3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen\nAnderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain,\nXiaobing Liu, and Hemal Shah. Wide & deep learning for recommender systems. In Proc. 1st Workshop\non Deep Learning for Recommender Systems, pages 7–10, 2016.\n[4] Corinna Cortes and Vladimir N. Vapnik. Support-vector networks. Machine Learning, 2:273–297, 1995.\n[5] Luc Devroye, Laszlo Gyorﬁ, and Gabor Lugosi. A Probabilistic Theory of Pattern Recognition. New York,\nSpringer-Verlag, 1996.\n[6] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011.\n[7] Facebook. Collective communications library with various primitives for multi-machine training (gloo),\nhttps://github.com/facebookincubator/gloo.\n[8] Facebook. Caffe2, https://caffe2.ai, 2016.\n[9] Evgeny Frolov and Ivan Oseledets. Tensor methods and recommender systems. Wiley Interdisciplinary\nReviews: Data Mining and Knowledge Discovery, 7(3):e1201, 2017.\n[10] Craig Gentry. A fully homomorphic encryption scheme. PhD thesis, Stanford University, 2009.\n[11] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The John Hopkins University Press, 3rd\nedition, 1996.\n[12] Ananth Grama, Vipin Kumar, Anshul Gupta, and George Karypis. Introduction to parallel computing.\nPearson Education, 2003.\n[13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. DeepFM: a factorization-\nmachine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247, 2017.\n[14] Rahman Hassan, Antony Harris, Nigel Topham, and Aris Efthymiou. Synthetic trace-driven simulation\nof cache memory. In Proc. 21st International Conference on Advanced Information Networking and\nApplications Workshops (AINAW’07), 2007.\n[15] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative\nﬁltering. In Proc. 26th Int. Conf. World Wide Web, pages 173–182, 2017.\n[16] Sylvain Jeaugey. Nccl 2.0, 2017.\n[17] Yehuda Koren, Robert Bell, and Chris V olinsky. Matrix factorization techniques for recommender systems.\nComputer, (8):30–37, 2009.\n[18] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. xDeepFM:\nCombining explicit and implicit feature interactions for recommender systems. In Proc. of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1754–1763. ACM,\n2018.\n[19] MLPerf. https://mlperf.org/.\n[20] Maxim Naumov. On the dimensionality of embeddings for sparse features and data. In arXiv preprint\narXiv:1901.02103, 2019.\n[21] Xia Ning, Christian Desrosiers, and George Karypis. A comprehensive survey of neighborhood-based\nrecommendation methods. In Recommender Systems Handbook, 2015.\n[22] Pandora. Music genome project https://www.pandora.com/about/mgp.\n[23] Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan. PyTorch: Tensors and dynamic neural\nnetworks in python with strong GPU acceleration https://pytorch.org/, 2017.\n[24] Steffen Rendle. Factorization machines. In Proc. 2010 IEEE International Conference on Data Mining,\npages 995–1000, 2010.\n9[25] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. Autorec: Autoencoders meet\ncollaborative ﬁltering. In Proc. 24th Int. Conf. World Wide Web, pages 111–112, 2015.\n[26] Strother H. Walker and David B. Duncan. Estimation of the probability of an event as a function of several\nindependent variables. Biometrika, 54:167–178, 1967.\n[27] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predictions. In\nProc. ADKDD, page 12, 2017.\n[28] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. Deep\ninterest evolution network for click-through rate prediction. arXiv preprint arXiv:1809.03672, 2018.\n[29] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li,\nand Kun Gai. Deep interest network for click-through rate prediction. In Proc. of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining, pages 1059–1068. ACM, 2018.\n10"
    }
}