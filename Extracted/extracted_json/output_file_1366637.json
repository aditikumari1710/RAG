{
    "title": "Tensor2Tensor for Neural Machine Translation",
    "content": {
        "page_content": "Tensor2Tensor for Neural Machine Translation\nAshish Vaswani1, Samy Bengio1, Eugene Brevdo1, Francois Chollet1, Aidan N. Gomez1,\nStephan Gouws1, Llion Jones1, Łukasz Kaiser 1, 3, Nal Kalchbrenner2, Niki Parmar1,\nRyan Sepassi1, 4, Noam Shazeer1, and Jakob Uszkoreit1\n1Google Brain\n2DeepMind\n3Corresponding author: lukaszkaiser@google.com\n4Corresponding author: rsepassi@google.com\nAbstract\nTensor2Tensor is a library for deep learning models that is well-suited for neural machine trans-\nlation and includes the reference implementation of the state-of-the-art Transformer model.\n1 Neural Machine Translation Background\nMachine translation using deep neural networks achieved great success with sequence-to-\nsequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014) that used recur-\nrent neural networks (RNNs) with LSTM cells (Hochreiter and Schmidhuber, 1997). The basic\nsequence-to-sequence architecture is composed of an RNN encoder which reads the source sen-\ntence one token at a time and transforms it into a ﬁxed-sized state vector. This is followed by an\nRNN decoder, which generates the target sentence, one token at a time, from the state vector.\nWhile a pure sequence-to-sequence recurrent neural network can already obtain good\ntranslation results (Sutskever et al., 2014; Cho et al., 2014), it suffers from the fact that the\nwhole input sentence needs to be encoded into a single ﬁxed-size vector. This clearly manifests\nitself in the degradation of translation quality on longer sentences and was partially overcome\nin Bahdanau et al. (2014) by using a neural model of attention.\nConvolutional architectures have been used to obtain good results in word-level neural\nmachine translation starting from Kalchbrenner and Blunsom (2013) and later in Meng et al.\n(2015). These early models used a standard RNN on top of the convolution to generate the\noutput, which creates a bottleneck and hurts performance.\nFully convolutional neural machine translation without this bottleneck was ﬁrst achieved\nin Kaiser and Bengio (2016) and Kalchbrenner et al. (2016). The Extended Neural GPU\nmodel (Kaiser and Bengio, 2016) used a recurrent stack of gated convolutional layers, while\nthe ByteNet model (Kalchbrenner et al., 2016) did away with recursion and used left-padded\nconvolutions in the decoder. This idea, introduced in WaveNet (van den Oord et al., 2016),\nsigniﬁcantly improves efﬁciency of the model. The same technique was improved in a number\nof neural translation models recently, including Gehring et al. (2017) and Kaiser et al. (2017).\n2 Self-Attention\nInstead of convolutions, one can use stacked self-attention layers. This was introduced in the\nTransformer model (Vaswani et al., 2017) and has signiﬁcantly improved state-of-the-art in ma-\nchine translation and language modeling while also improving the speed of training. Research\narXiv:1803.07416v1  [cs.LG]  16 Mar 2018Figure 1: The Transformer model architecture.\ncontinues in applying the model in more domains and exploring the space of self-attention\nmechanisms. It is clear that self-attention is a powerful tool in general-purpose sequence mod-\neling.\nWhile RNNs represent sequence history in their hidden state, the Transformer has no such\nﬁxed-size bottleneck. Instead, each timestep has full direct access to the history through the\ndot-product attention mechanism. This has the effect of both enabling the model to learn more\ndistant temporal relationships, as well as speeding up training because there is no need to wait\nfor a hidden state to propagate across time. This comes at the cost of memory usage, as the\nattention mechanism scales with t2, where t is the length the sequence. Future work may\nreduce this scaling factor.\nThe Transformer model is illustrated in Figure 1. It uses stacked self-attention and point-\nwise, fully connected layers for both the encoder and decoder, shown in the left and right halves\nof Figure 1 respectively.\nEncoder: The encoder is composed of a stack of identical layers. Each layer has two\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple,\npositionwise fully connected feed-forward network.\nDecoder: The decoder is also composed of a stack of identical layers. In addition to the\ntwo sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs\nmulti-head attention over the output of the encoder stack.Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential\noperations for different layer types. nis the sequence length, dis the representation dimension,\nkis the kernel size of convolutions andrthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n· d2) O(n) O(n)\nConvolutional O(k· n· d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r· n· d) O(1) O(n/r)\nMore details about multi-head attention and overall architecture can be found in Vaswani\net al. (2017).\n2.1 Computational Performance\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of\nsequentially executed operations, whereas a recurrent layer requiresO(n) sequential operations.\nIn terms of computational complexity, self-attention layers are faster than recurrent layers when\nthe sequence length nis smaller than the representation dimensionality d, which is most often\nthe case with sentence representations used by state-of-the-art models in machine translations,\nsuch as word-piece (Wu et al., 2016) and byte-pair (Sennrich et al., 2015) representations.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of in-\nput and output positions. Doing so requires a stack of O(n/k) convolutional layers in the\ncase of contiguous kernels, or O(logk(n)) in the case of dilated convolutions (Kalchbrenner\net al., 2016), increasing the length of the longest paths between any two positions in the net-\nwork. Convolutional layers are generally more expensive than recurrent layers, by a factor of\nk. Separable convolutions (Chollet, 2016), however, decrease the complexity considerably, to\nO(k· n· d+ n· d2). Even with k = n, however, the complexity of a separable convolution\nis equal to the combination of a self-attention layer and a point-wise feed-forward layer, the\napproach we take in our model.\nSelf-attention can also yield more interpretable models. In Tensor2Tensor, we can visual-\nize attention distributions from our models for each individual layer and head. Observing them\nclosely, we see that the models learn to perform different tasks, many appear to exhibit behavior\nrelated to the syntactic and semantic structure of the sentences.\n2.2 Machine Translation\nWe trained our models on the WMT Translation task.\nOn the WMT 2014 English-to-German translation task, the big transformer model (Trans-\nformer (big) in Table 2) outperforms the best previously reported models (including ensembles)\nby more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. Training took\n3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and\nensembles, at a fraction of the training cost of any of the competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU\nscore of 41.8, outperforming all of the previously published single models, at less than 1/4 the\ntraining cost of the previous state-of-the-art model.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints,\nwhich were written at 10-minute intervals. For the big models, we averaged the last 20 check-\npoints. We used beam search with a beam size of 4 and length penalty α = 0.6 (Wu et al.,Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models\non the English-to-German and English-to-French newstest2014 tests at a fraction of the training\ncost.\nModel\nBLEU Training Cost\n(in FLOPS * 1018)\nEN-DE EN-FR EN-DE EN-FR\nByteNet (Kalchbrenner et al., 2016) 23.75\nDeep-Att + PosUnk (Zhou et al., 2016) 39.2 100\nGNMT + RL (Wu et al., 2016) 24.6 39.92 23 140\nConvS2S (Gehring et al., 2017) 25.16 40.46 9.6 150\nMoE (Shazeer et al., 2017) 26.03 40.56 20 120\nGNMT + RL Ensemble (Wu et al., 2016) 26.30 41.16 180 1100\nConvS2S Ensemble (Gehring et al., 2017) 26.36 41.29 77 1200\nTransformer (base model) 27.3 38.1 3.3\nTransformer (big) 28.4 41.8 23\n2016). These hyperparameters were chosen after experimentation on the development set. We\nset the maximum output length during inference to input length + 50, but terminate early when\npossible (Wu et al., 2016).\n3 Tensor2Tensor\nTensor2Tensor (T2T) is a library of deep learning models and datasets designed to make deep\nlearning research faster and more accessible. T2T uses TensorFlow (Abadi et al., 2016) through-\nout and there is a strong focus on performance as well as usability. Through its use of Tensor-\nFlow and various T2T-speciﬁc abstractions, researchers can train models on CPU, GPU (single\nor multiple), and TPU, locally and in the cloud, usually with no or minimal device-speciﬁc code\nor conﬁguration.\nDevelopment began focused on neural machine translation and so Tensor2Tensor includes\nmany of the most successful NMT models and standard datasets. It has since added support for\nother task types as well across multiple media (text, images, video, audio). Both the number of\nmodels and datasets has grown signiﬁcantly.\nUsage is standardized across models and problems which makes it easy to try a new model\non multiple problems or try multiple models on a single problem. See Example Usage (appendix\nB) to see some of the usability beneﬁts of standardization of commands and uniﬁcation of\ndatasets, models, and training, evaluation, decoding procedures.\nDevelopment is done in the open on GitHub (http://github.com/tensorﬂow/tensor2tensor)\nwith many contributors inside and outside Google.\n4 System Overview\nThere are ﬁve key components that specify a training run in Tensor2Tensor:\n1. Datasets: The Problem class encapsulate everything about a particular dataset. A\nProblem can generate the dataset from scratch, usually downloading data from a pub-\nlic source, building a vocabulary, and writing encoded samples to disk. Problems also\nproduce input pipelines for training and evaluation as well as any necessary additional\ninformation per feature (for example, its type, vocabulary size, and an encoder able to\nconvert samples to and from human and machine-readable representations).2. Device conﬁguration: the type, number, and location of devices. TensorFlow and Ten-\nsor2Tensor currently support CPU, GPU, and TPU in single and multi-device conﬁgu-\nrations. Tensor2Tensor also supports both synchronous and asynchronous data-parallel\ntraining.\n3. Hyperparameters: parameters that control the instantiation of the model and training pro-\ncedure (for example, the number of hidden layers or the optimizer’s learning rate). These\nare speciﬁed in code and named so they can be easily shared and reproduced.\n4. Model: the model ties together the preceding components to instantiate the parameter-\nized transformation from inputs to targets, compute the loss and evaluation metrics, and\nconstruct the optimization procedure.\n5. Estimator and Experiment: These classes that are part of TensorFlow handle in-\nstantiating the runtime, running the training loop, and executing basic support services like\nmodel checkpointing, logging, and alternation between training and evaluation.\nThese abstractions enable users to focus their attention only on the component they’re\ninterested in experimenting with. Users that wish to try models on a new problem usually only\nhave to deﬁne a new problem. Users that wish to create or modify models only have to create\na model or edit hyperparameters. The other components remain untouched, out of the way, and\navailable for use, all of which reduces mental load and allows users to more quickly iterate on\ntheir ideas at scale.\nAppendix A contains an outline of the code and appendix B contains example usage.\n5 Library of research components\nTensor2Tensor provides a vehicle for research ideas to be quickly tried out and shared. Compo-\nnents that prove to be very useful can be committed to more widely-used libraries like Tensor-\nFlow, which contains many standard layers, optimizers, and other higher-level components.\nTensor2Tensor supports library usage as well as script usage so that users can reuse speciﬁc\ncomponents in their own model or system. For example, multiple researchers are continuing\nwork on extensions and variations of the attention-based Transformer model and the availability\nof the attention building blocks enables that work.\nSome examples:\n• The Image Transformer (Parmar et al., 2018) extends the Transformer model to images. It\nrelies heavily on many of the attention building blocks in Tensor2Tensor and adds many\nof its own.\n• tf.contrib.layers.rev block, implementing a memory-efﬁcient block of re-\nversible layers as presented in Gomez et al. (2017), was ﬁrst implemented and exercised\nin Tensor2Tensor.\n• The Adafactor optimizer (pending publication), which signiﬁcantly reduces memory re-\nquirements for second-moment estimates, was developed within Tensor2Tensor and tried\non various models and problems.\n• tf.contrib.data.bucket by sequence length enables efﬁcient processing\nof sequence inputs on GPUs in the new tf.data.Dataset input pipeline API. It was\nﬁrst implemented and exercised in Tensor2Tensor.6 Reproducibility and Continuing Development\nContinuing development on a machine learning codebase while maintaining the quality of mod-\nels is a difﬁcult task because of the expense and randomness of model training. Freezing a\ncodebase to maintain a certain conﬁguration, or moving to an append-only process has enor-\nmous usability and development costs.\nWe attempt to mitigate the impact of ongoing development on historical reproducibility\nthrough 3 mechanisms:\n1. Named and versioned hyperparameter sets in code\n2. End-to-end regression tests that run on a regular basis for important model-problem pairs\nand verify that certain quality metrics are achieved.\n3. Setting random seeds on multiple levels (Python, numpy, and TensorFlow) to mitigate\nthe effects of randomness (though this is effectively impossible to achieve in full in a\nmultithreaded, distributed, ﬂoating-point system).\nIf necessary, because the code is under version control on GitHub\n(http://github.com/tensorﬂow/tensor2tensor), we can always recover the exact code that\nproduced certain experiment results.\nReferences\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard,\nM., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G., Steiner, B., Tucker, P., Vasudevan,\nV ., Warden, P., Wicke, M., Yu, Y ., and Zheng, X. (2016). Tensorﬂow: A system for large-scale machine\nlearning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16),\npages 265–283.\nBahdanau, D., Cho, K., and Bengio, Y . (2014). Neural machine translation by jointly learning to align and\ntranslate. CoRR, abs/1409.0473.\nCho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y . (2014). Learn-\ning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR,\nabs/1406.1078.\nChollet, F. (2016). Xception: Deep learning with depthwise separable convolutions. arXiv preprint\narXiv:1610.02357.\nGehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. (2017). Convolutional sequence to\nsequence learning. CoRR, abs/1705.03122.\nGomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. (2017). The reversible residual network: Back-\npropagation without storing activations. CoRR, abs/1707.04585.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8):1735–\n1780.\nKaiser, Ł. and Bengio, S. (2016). Can active memory replace attention? InAdvances in Neural Information\nProcessing Systems, pages 3781–3789.\nKaiser, L., Gomez, A. N., and Chollet, F. (2017). Depthwise separable convolutions for neural machine\ntranslation. CoRR, abs/1706.03059.Kalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In Proceedings\nEMNLP 2013, pages 1700–1709.\nKalchbrenner, N., Espeholt, L., Simonyan, K., van den Oord, A., Graves, A., and Kavukcuoglu, K. (2016).\nNeural machine translation in linear time. CoRR, abs/1610.10099.\nMeng, F., Lu, Z., Wang, M., Li, H., Jiang, W., and Liu, Q. (2015). Encoding source language with\nconvolutional neural network for machine translation. In ACL, pages 20–30.\nParmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer, N., and Ku, A. (2018). Image Transformer.\nArXiv e-prints.\nSennrich, R., Haddow, B., and Birch, A. (2015). Neural machine translation of rare words with subword\nunits. arXiv preprint arXiv:1508.07909.\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. (2017). Outrageously\nlarge neural networks: The sparsely-gated mixture-of-experts layer. CoRR, abs/1701.06538.\nSutskever, I., Vinyals, O., and Le, Q. V . (2014). Sequence to sequence learning with neural networks. In\nAdvances in Neural Information Processing Systems, pages 3104–3112.\nvan den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior,\nA., and Kavukcuoglu, K. (2016). WaveNet: A generative model for raw audio.CoRR, abs/1609.03499.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin,\nI. (2017). Attention is all you need. CoRR.\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M., Macherey, W., Krikun, M., Cao, Y ., Gao, Q.,\nMacherey, K., et al. (2016). Google’s neural machine translation system: Bridging the gap between\nhuman and machine translation. CoRR, abs/1609.08144.\nZhou, J., Cao, Y ., Wang, X., Li, P., and Xu, W. (2016). Deep recurrent models with fast-forward connec-\ntions for neural machine translation. CoRR, abs/1606.04199.\nA Tensor2Tensor Code Outline\n• Create HParams\n• Create RunConfig specifying devices\n– Create and include the Parallelism object in the RunConfig which enables data-parallel\nduplication of the model on multiple devices (for example, for multi-GPU synchronous train-\ning).\n• Create Experiment, including training and evaluation hooks which control support services like\nlogging and checkpointing\n• Create Estimator encapsulating the model function\n– T2TModel.estimator model fn\n∗ model(features)\n· model.bottom: This uses feature type information from the Problem to transform\nthe input features into a form consumable by the model body (for example, embedding\ninteger token ids into a dense ﬂoat space).· model.body: The core of the model.\n· model.top: Transforming the output of the model body into the target space using\ninformation from the Problem\n· model.loss\n∗ When training: model.optimize\n∗ When evaluating: create evaluation metrics\n• Create input functions\n– Problem.input fn: produce an input pipeline for a given mode. Uses TensorFlow’s\ntf.data.Dataset API.\n∗ Problem.dataset which creates a stream of individual examples\n∗ Pad and batch the examples into a form ready for efﬁcient processing\n• Run the Experiment\n– estimator.train\n∗ train op = model fn(input fn(mode=TRAIN))\n∗ Run the train op for the number of training steps speciﬁed\n– estimator.evaluate\n∗ metrics = model fn(input fn(mode=EVAL))\n∗ Accumulate the metrics across the number of evaluation steps speciﬁed\nB Example Usage\nTensor2Tensor usage is standardized across problems and models. Below you’ll ﬁnd a set of commands\nthat generates a dataset, trains and evaluates a model, and produces decodes from that trained model.\nExperiments can typically be reproduced with the (problem, model, hyperparameter set) triple.\nThe following train the attention-based Transformer model on WMT data translating from English\nto German:\npip install tensor2tensor\nPROBLEM=translate_ende_wmt32k\nMODEL=transformer\nHPARAMS=transformer_base\n# Generate data\nt2t-datagen \\\n--problem=$PROBLEM \\\n--data_dir=$DATA_DIR \\\n--tmp_dir=$TMP_DIR\n# Train and evaluate\nt2t-trainer \\\n--problems=$PROBLEM \\\n--model=$MODEL \\\n--hparams_set=$HPARAMS \\\n--data_dir=$DATA_DIR \\\n--output_dir=$OUTPUT_DIR \\--train_steps=250000\n# Translate lines from a file\nt2t-decoder \\\n--data_dir=$DATA_DIR \\\n--problems=$PROBLEM \\\n--model=$MODEL \\\n--hparams_set=$HPARAMS \\\n--output_dir=$OUTPUT_DIR \\\n--decode_from_file=$DECODE_FILE \\\n--decode_to_file=translation.en"
    }
}