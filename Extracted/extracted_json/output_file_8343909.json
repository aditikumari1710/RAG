{
    "title": "DDT: Decoupled Diffusion Transformer",
    "content": {
        "page_content": "DDT: Decoupled Diffusion Transformer\nShuai Wang1 Zhi Tian2 Weilin Huang2 Limin Wang 1, /envelâŒ¢pe\n1Nanjing University 2ByteDance Seed Vision\nhttps://github.com/MCG-NJU/DDT\nConditionEncoder\nVelocityDecoderğ‘¥! ğ‘£!\nğ‘¥! ğ‘¦ğ‘¡\nğ‘¡\nPretrained Visual Encoder\nğ‘¥\"#$\nREPAlignDiffusionTransfomer\nğ‘¥! ğ‘¦ğ‘¡\nğ‘£!(a) Our Decoupled Diffusion Transformer(b) Conventional Diffusion Transformer(c) FID compared with Other Diffusion Models\nPretrained Visual Encoder\nğ‘¥\"#$\nREPAlign\nFigure 1. Our deoupled diffusion transformer (DDT-XL/2) achieves a SoTA 1.31 FID under 256 epochs. Our decoupled diffusion\ntransformer models incorporate a condition encoder to extract semantic self-conditions and a velocity decoder to decode velocity.\nAbstract\nDiffusion transformers have demonstrated remarkable gen-\neration quality, albeit requiring longer training iterations\nand numerous inference steps. In each denoising step,\ndiffusion transformers encode the noisy inputs to extract\nthe lower-frequency semantic component and then decode\nthe higher frequency with identical modules. This scheme\ncreates an inherent optimization dilemma: encoding low-\nfrequency semantics necessitates reducing high-frequency\ncomponents, creating tension between semantic encoding\nand high-frequency decoding. To resolve this challenge, we\npropose a new Decoupled Diffusion Transformer (DDT),\nwith a decoupled design of a dedicated condition encoder\nfor semantic extraction alongside a specialized velocity de-\ncoder. Our experiments reveal that a more substantial en-\ncoder yields performance improvements as model size in-\ncreases. For ImageNet 256 Ã— 256, Our DDT-XL/2 achieves\na new state-of-the-art performance of 1.31 FID (nearly\n4Ã— faster training convergence compared to previous dif-\nfusion transformers). For ImageNet 512 Ã— 512, Our DDT-\nXL/2 achieves a new state-of-the-art FID of 1.28. Addi-\ntionally, as a beneficial by-product, our decoupled architec-\nture enhances inference speed by enabling the sharing self-\ncondition between adjacent denoising steps. To minimize\nperformance degradation, we propose a novel statistical dy-\nnamic programming approach to identify optimal sharing\nstrategies.\n1. Introduction\nImage generation is a fundamental task in computer vision\nresearch, which aims at capturing the inherent data distribu-\ntion of original image datasets and generating high-quality\nsynthetic images through distribution sampling. Diffusion\nmodels [19, 21, 29, 30, 41] have recently emerged as highly\npromising solutions to learn the underlying data distribution\nin image generation, outperforming the GAN-based mod-\nels [3, 40] and Auto-Regressive models [5, 43, 51].\nThe diffusion forward process gradually adds Gaussian\nnoise to the pristine data following an SDE forward sched-\nule [19, 21, 41]. The denoising process learns the score es-\ntimation from this corruption process. Once the score func-\ntion is accurately learned, data samples can be synthesized\nby numerically solving the reverse SDE [21, 29, 30, 41].\n/envelâŒ¢pe: Corresponding author (lmwang@nju.edu.cn).\n1\narXiv:2504.05741v2  [cs.CV]  9 Apr 2025Diffusion Transformers [32, 36] introduce the trans-\nformer architecture into diffusion models to replace the tra-\nditionally dominant UNet-based model [2, 10]. Empirical\nevidence suggests that, given sufficient training iterations,\ndiffusion transformers outperform conventional approaches\neven without relying on long residual connections [36].\nNevertheless, their slow convergence rate still poses great\nchallenge for developing new models due to the high cost.\nIn this paper, we want to tackle the aforementioned ma-\njor disadvantages from a model design perspective. Clas-\nsic computer vision algorithms [4, 17, 23] strategically em-\nploy encoder-decoder architectures, prioritizing large en-\ncoders for rich feature extraction and lightweight decoders\nfor efficient inference, while contemporary diffusion mod-\nels predominantly rely on conventional decoder-only struc-\ntures. We systematically investigate the underexplored po-\ntential of decoupled encoder-decoder designs in diffusion\ntransformers, by answering the question of can decoupled\nencoder-decoder transformer unlock the capability of ac-\ncelerated convergence and enhanced sample quality?\nThrough investigation experiments, we conclude that the\nplain diffusion transformer has an optimization dilemma be-\ntween abstract structure information extraction and detailed\nappearance information recovery. Further, the diffusion\ntransformer is limited in extracting semantic representation\ndue to the raw pixel supervision [28, 52, 53]. To address this\nissue, we propose a new architecture to explicitly decouple\nlow-frequency semantic encoding and high-frequency de-\ntailed decoding through a customized encoder-decoder de-\nsign. We call this encoder-decoder diffusion transformer\nmodel as DDT (Decoupled Diffusion Transformer). DDT\nincorporates a condition encoder to extract semantic self-\ncondition features. The extracted self-condition is fed into\na velocity decoder along with the noisy latent to regress the\nvelocity field. To maintain the local consistency of self-\ncondition features of adjacent steps, we employ direct su-\npervision of representation alignment and indirect supervi-\nsion from the velocity regression loss of the decoder.\nIn the ImageNet 256 Ã— 256 dataset, using the tradi-\ntional off-shelf V AE [38], our decoupled diffusion trans-\nformer (DDT-XL/2) model achieves the state-of-the-art per-\nformance of 1.31 FID with interval guidance under only\n256 epochs, approximately 4Ã— training acceleration com-\npared to REPA [52]. In the ImageNet512 Ã—512 dataset, our\nDDT-XL/2 model achieves 1.28 FID within 500K finetun-\ning steps.\nFurthermore, our DDT achieves strong local consistency\non its self-condition feature from the encoder. This prop-\nerty can significantly boost the inference speed by sharing\nthe self-condition between adjacent steps. We formulate the\noptimal encoder sharing strategy solving as a classic mini-\nmal sum path problem by minimizing the performance drop\nof sharing self-condition among adjacent steps. We propose\na statistic dynamic programming approach to find the opti-\nmal encoder sharing strategy with negligible second-level\ntime cost. Compared with the naive uniform sharing, our\ndynamic programming delivers a minimal FID drop. Our\ncontributions are summarized as follows.\nâ€¢ We propose a new decoupled diffusion transformer\nmodel, which consists of a condition encoder and a ve-\nlocity decoder.\nâ€¢ We propose statistic dynamic programming to find the\noptimal self-condition sharing strategy to boost infer-\nence speed while keeping minimal performance down-\ngradation.\nâ€¢ In the ImageNet 256Ã—256 dataset, using tradition SDf8d4\nV AE, our decoupled diffusion transformer (DDT-XL/2)\nmodel achieves the SoTA 1.31 FID with interval guid-\nance under only 256 epochs, approximately 4Ã— training\nacceleration compared to REPA [52].\nâ€¢ In the ImageNet 512 Ã— 512 dataset, our DDT-XL/2 model\nachieves the SoTA 1.28 FID, outperforming all previous\nmethods with a significant margin.\n2. Related Work\nDiffusion Transformers. The pioneering work of\nDiT [36] introduced transformers into diffusion models\nto replace the traditionally dominant UNet architec-\nture [2, 10]. Empirical evidence demonstrates that given\nsufficient training iterations, diffusion transformers out-\nperform conventional approaches even without relying\non long residual connections. SiT [32] further validated\nthe transformer architecture with linear flow diffusion.\nFollowing the simplicity and scalability of the diffu-\nsion transformer [32, 36], SD3 [12], Lumina [54], and\nPixArt [6, 7] introduced the diffusion transformer to more\nadvanced text-to-image areas. Moreover, recently, diffusion\ntransformers have dominated the text-to-video area with\nsubstantiated visual and motion quality [1, 20, 24]. Our\ndecoupled diffusion transformer (DDT) presents a new\nvariant within the diffusion transformer family. It achieves\nfaster convergence by decoupling the low-frequency\nencoding and the high-frequency decoding.\nFast Diffusion Training. To accelerate the training ef-\nficiency of diffusion transformers, recent advances have\npursued multi-faceted optimizations. Operator-centric ap-\nproaches [13, 45, 48, 49] leverage efficient attention mech-\nanisms: linear-attention variants [13, 45, 49] reduced\nquadratic complexity to speed up training, while sparse-\nattention architectures [48] prioritized sparsely relevant to-\nken interactions. Resampling approaches [12, 16] proposed\nlognorm sampling [12] or loss reweighting [16] techniques\nto stabilize training dynamics. Representation learning en-\nhancement approaches integrate external inductive biases:\n2Figure 2. Selected 256 Ã— 256 and 512 Ã— 512 resolution samples. Generated from DDT-XL/2 trained on ImageNet256 Ã— 256 resolution\nand ImageNet 512 Ã— 512 resolution with CFG = 4.0.\nDiffusion Process in ğ’™\tspace ğ’•=ğŸ.ğŸğ’•=ğŸ.ğŸ–ğ’•=ğŸ.ğŸ”\nğ’•=ğŸ.ğŸ ğ’•=ğŸ.ğŸ’\nFigure 3. The reverse-SDE process (generation) of SiT-XL/2 in\nx space. There is a clear generation process from low frequency\nto high frequency. Most of the time is spent on generating high-\nfrequency details (from t = 0.4 to t = 1.0).\nREPA [52], RCG [27] and DoD [53] borrowed vision-\nspecific priors into diffusion training, while masked mod-\neling techniques [14, 15] strengthened spatial reasoning by\nenforcing structured feature completion during denoising.\nCollectively, these strategies address computational, sam-\npling, and representational bottlenecks.\n3. Preliminary Analysis\nLinear-based flow matching [29, 30, 32] represents a spe-\ncialized family of diffusion models that we focus on as\nour primary analytical subject due to its simplicity and effi-\nciency. For the convenience of discussion, in certain situa-\ntions, diffusion and flow-matching will be used interchange-\n0.0 0.2 0.4 0.6 0.8 1.0\nUniform Timesteps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Respaced Timesteps\nshift 1.0\nshift 1.5\nshift 2\nshift 3\n5 6 7 8 9 10 11 12\nNum of inference steps\n2\n3\n4\n5\n6\n7\n8\n9FID50K\nFigure 4. The FID50K metric of SiT-XL/2 for different\ntimeshift values. We employ a 2-nd order Adams-like solver to\ncollect the performance. Allocating more computation at noisy\nsteps significantly improves the performance.\nably. In this framework,t = 0 corresponds to the pure noise\ntimestep.\nAs illustrated in Fig. 3, diffusion models perform au-\ntoregressive refinement on spectral components [11, 37].\nThe diffusion transformer encodes the noisy latent to cap-\nture lower-frequency semantics before decoding higher-\nfrequency details. However, this semantics encoding pro-\ncess inevitably attenuates high-frequency information, cre-\nating an optimization dilemma. This observation motivates\nour proposal to decouple the conventional decode-only dif-\nfusion transformer into an explicit encoder-decoder archi-\ntecture.\nLemma 1. For a linear flow-matching noise scheduler at\ntimestep t, let us denote Kfreq as the maximum frequency\nof the clean data xdata. The maximum retained frequency\n3in the noisy latent satisfies:\nfmax(t) > min\n \u0012 t\n1 âˆ’ t\n\u00132\n, Kfreq\n!\n. (1)\nLemma 1 is directly borrowed from [11, 37], we place\nthe proof of Lemma 1 in Appendix. According to Lemma 1,\nas t increases to less noisy timesteps, semantic encoding be-\ncomes easier (due to noise reduction) while decoding com-\nplexity increases (as residual frequencies grow). Consider\nthe worst-case scenario at denoising step t, the diffusion\ntransformer encodes frequencies up to fmax(t), to progress\nto step s, it must decode a residual frequency of at least\nfmax(s) âˆ’ fmax(t). Failure to decode these residual fre-\nquencies at step t creates a critical bottleneck for progres-\nsion to subsequent steps. From this perspective, if allocating\nmore of the calculations to more noisy timesteps can lead\nto an improvement, it means that diffusion transformers\nstruggle with encoding lower frequency to provide seman-\ntics. Otherwise, if allocating more of the calculations to less\nnoisy timesteps can lead to an improvement, it means that\nflow-matching transformers struggle with decoding higher\nfrequency to provide fine details.\nTo figure out the bottom-necks of current diffusion mod-\nels, we conducted a targeted experiment using SiT-XL/2\nwith a second-order Adams-like linear multistep solver.\nAs shown in Fig. 4, by varying the time-shift values,\nwe demonstrate that allocating more computation to early\ntimesteps improves final performance compared to uniform\nscheduling. This reveals that diffusion models face chal-\nlenges in more noisy steps. This leads to a key conclusion:\nCurrent diffusion transformers are fundamentally con-\nstrained by their low-frequency semantic encoding ca-\npacity. This insight motivates the exploration of encoder-\ndecoder architectures with strategic encoder parameter allo-\ncation.\nPrior researches further support this perspective. While\nlightweight diffusion MLP heads demonstrate limited de-\ncoding capacity, MAR [28] overcomes this limitation\nthrough semantic latents produced by its masked back-\nbones, enabling high-quality image generation. Simi-\nlarly, REPA [52] enhances low-frequency encoding through\nalignment with pre-trained vision foundations [35].\n4. Method\nOur decoupled diffusion transformer architecture comprises\na condition encoder and a velocity decoder. The condi-\ntion encoder extracted the low-frequency component from\nnoisy input, class label, and timestep to serve as a self-\ncondition for the velocity decoder; the velocity decoder pro-\ncessed the noisy latent with the self-condition to regress\nthe high-frequency velocity. We train this model using the\nestablished linear flow diffusion framework. For brevity,\nwe designate our model as DDT (Decoupled Diffusion\nTransformer).\n4.1. Condition Encoder\nThe condition encoder mirrors the architectural design and\ninput structure of DiT/SiT with improved micro-design. It\nis built with interleaved Attention and FFN blocks, with-\nout long residual connections. The encoder processes three\ninputs, the noisy latent xt, timestep t, and class label y,\nto extract the self-condition feature zt through a series of\nstacked Attention and FFN blocks:\nzt = Encoder (xt, t, y). (2)\nSpecifically, the noisy latent xt are patchfied into continu-\nous tokens and then fed to extract the self-conditionzt with\naforementioned encoder blocks. The timestep t and class\nlabel y serve as external-conditioning information projected\ninto embedding. These external-condition embeddings are\nprogressively injected into the encoded features of xt using\nAdaLN-Zero[36] within each encoder block.\nTo maintain local consistency of zt across adjacent\ntimesteps, we adopt the representation alignment technique\nfrom REPA [52]. Shown in Eq. (3), this method aligns\nthe intermediate feature hi from the i-th layer in the self-\nmapping encoder with the DINOv2 representation râˆ—. Con-\nsistent to REPA [52], the hÏ• is the learnable projection\nMLP:\nLenc = 1 âˆ’ cos(râˆ—, hÏ•(hi)). (3)\nThis simple regularization accelerates training convergence,\nas shown in REPA [52], and facilitates local consistency\nof zt between adjacent steps. It allows sharing the self-\ncondition zt produced by the encoder between adjacent\nsteps. Our experiments demonstrate that this encoder-\nsharing strategy significantly enhances inference efficiency\nwith only negligible performance degradation.\nAdditionally, the encoder also receives indirect supervi-\nsion from the decoder, which we elaborate on later.\n4.2. Velocity Decoder\nThe velocity decoder adopts the same architectural design\nas the condition encoder and consists of several stacked\ninterleaved Attention and FFN blocks, akin to DiT/SiT. It\ntakes the noisy latent xt, timestep t, and self-conditioning\nzt as inputs to estimate the velocity vt. Unlike the encoder,\nwe assume that class label information is already embedded\nwithin zt. Thus, only the external-condition timestep t and\nself-condition feature zt are used as condition inputs for the\ndecoder blocks:\nvt = Decoder (xt, t,zt). (4)\nAs demonstrated previously, to further improve consistency\nof self-condition zt between adjacent steps, we employ\n4AdaLN-Zero [36] to inject zt into the decoder feature. The\ndecoder is trained with the flow matching loss as shown in\nEq. (5):\nLdec = E[\nZ 1\n0\n||(xdata âˆ’ Ïµ) âˆ’ vt||2dt]. (5)\n4.3. Sampling acceleration\nBy incorporating explicit representation alignment into the\nencoder and implicit self-conditioning injection into the de-\ncoder, we achieve local consistency of zt across adjacent\nsteps during training (shown in Fig. 5). This enables us to\nshare zt within a suitable local range, reducing the compu-\ntational burden on the self-mapping encoder.\nFormally, given total inference steps N and encoder\ncomputation bugets K, thus the sharing ratio is 1 âˆ’ K\nN , we\ndefine Î¦ with |Î¦| = K as the set of timesteps where the\nself-condition is recalculated, as shown in Equation 6. If\nthe current timestep t is not in Î¦, we reuse the previously\ncomputed ztâˆ’âˆ†t as zt. Otherwise, we recompute zt using\nthe encoder and the current noisy latent xt:\nzt =\n(\nztâˆ’âˆ†t, if t /âˆˆ Î¦\nEncoder (xt, t, y), if t âˆˆ Î¦ (6)\nUniform Encoder Sharing. This naive approach recalu-\nculate self-condition zt every N\nK steps. Previous work, such\nas DeepCache [33], uses this naive handcrafted uniform Î¦\nset to accelerate UNet models. However, UNet models,\ntrained solely with a denoising loss and lacking robust rep-\nresentation alignment, exhibit less regularized local consis-\ntency in deeper features across adjacent steps compared to\nour DDT model. Also, we will propose a simple and ele-\ngant statistic dynamic programming algorithm to construct\nÎ¦. Our statistic dynamic programming can exploit the opti-\nmal Î¦ set optimally compared to the naive approaches [33].\nStatistic Dynamic Programming. We construct the\nstatistic similarity matrix of zt among different steps S âˆˆ\nRNÃ—N using cosine distance. The optimal Î¦ set would\nguarantee the total similarity cost âˆ’PK\nk\nPÎ¦k+1\ni=Î¦k S[Î¦k, i]\nachieves global minimal. This question is a well-formed\nclassic minimal sum path problem, it can be solved by dy-\nnamic programming. As shown in Eq. (8), we donateCk\ni as\ncost and Pk\ni as traced path whenÎ¦k = i. the state transition\nfunction from Ckâˆ’1\nj to Ck\ni follows:\nCk\ni =\ni\nmin\nj=0\n{Ckâˆ’1\nj âˆ’ Î£i\nl=jS[j, l]}. (7)\nPk\ni = argmini\nj=0{Ckâˆ’1\ni âˆ’ Î£i\nl=jS[j, l]}. (8)\nAfter obtaining the cost matrix C and tracked path P, the\noptimal Î¦ can be solved by backtracking P from PK\nN .\n5. Experiment\nWe conduct experiments on 256x256 ImageNet datasets.\nThe total training batch size is set to 256. Consistent with\nmethodological approaches such as SiT [32], DiT [36], and\nREPA [52], we employed the Adam optimizer with a con-\nstant learning rate of 0.0001 throughout the entire training\nprocess. To ensure a fair comparative analysis, we did not\nuse gradient clipping and learning rate warm-up techniques.\nOur default training infrastructure consisted of 16Ã— or 8Ã—\nA100 GPUs. For sampling, we take the Euler solver with\n250 steps as the default choice. As for the V AE, we take the\noff-shelf V AE-ft-EMA with a downsample factor of 8 from\nHuggingface1. We report FID [18], sFID [34], IS [39], Pre-\ncision and Recall [25].\n5.1. Improved baselines\nRecent architectural improvements such as SwiGLU [46,\n47], RoPE [42], and RMSNorm [46, 47] have been ex-\ntensively validated in the research community [8, 31, 50].\nAdditionally, lognorm sampling [12] has demonstrated sig-\nnificant benefits for training convergence. Consequently,\nwe developed improved baseline models by incorporating\nthese advanced techniques, drawing inspiration from recent\nworks in the field. The performance of these improved base-\nlines is comprehensively provided in Tab. 2. To validate the\nreliability of our implementation, we also reproduced the\nresults for REPA-B/2, achieving metrics that marginally ex-\nceed those originally reported in the REPA[52]. These re-\nproduction results provide additional confidence in the ro-\nbustness of our approach.\nThe improved baselines in our Tab. 2 consistently out-\nperform their predecessors without REPA. However, upon\nimplementing REPA, performance rapidly approaches a sat-\nuration point. This is particularly evident in the XL model\nsize, where incremental technique improvements yield di-\nminishingly small gains.\n5.2. Metric comparison with baselines\nWe present the performances of different-size models at\n400K training steps in Tab. 2. Our diffusion encoder-\ndecoder transformer(DDT) family demonstrates consistent\nand significant improvements across various model sizes.\nOur DDT-B/2(8En4De) model exceeds Improved-REPA-\nB/2 by 2.8 FID gains. Our DDT-XL/2(22En6De) exceeds\nREPA-XL/2 by 1.3 FID gains. While the decoder-only dif-\nfusion transformers approach performance saturation with\nREPA[52], our DDT models continue to deliver superior\nresults. The incremental technique improvements show di-\nminishing gains, particularly in larger model sizes. How-\never, our DDT models maintain a significant performance\nadvantage, underscoring the effectiveness of our approach.\n1https://huggingface.co/stabilityai/sd-vae-ft-\nema\n5256Ã—256, w/o CFG 256 Ã—256, w/ CFG\nParams Epochs FIDâ†“ ISâ†‘ Pre.â†‘ Rec.â†‘ FIDâ†“ ISâ†‘ Pre.â†‘ Rec.â†‘\nMAR-B [28] 208M 800 3.48 192.4 0.78 0.58 2.31 281.7 0.82 0.57\nCausalFusion [9] 368M 800 5.12 166.1 0.73 0.66 1.94 264.4 0.82 0.59\nLDM-4 [38] 400M 170 10.56 103.5 0.71 0.62 3.6 247.7 0.87 0.48\nDDT-L (Ours) 458M 80 7.98 128.1 0.68 0.67 1.64 310.5 0.81 0.61\nMAR-L [28] 479M 800 2.6 221.4 0.79 0.60 1.78 296.0 0.81 0.60\nV A V AE [50] 675M 800 2.17 205.6 0.77 0.65 1.35 295.3 0.79 0.65\nCausalFusion [9] 676M 800 3.61 180.9 0.75 0.66 1.77 282.3 0.82 0.61\nADM [10] 554M 400 10.94 - 0.69 0.63 4.59 186.7 0.82 0.52\nDiT-XL [36] 675M 1400 9.62 121.5 0.67 0.67 2.27 278.2 0.83 0.57\nSiT-XL [32] 675M 1400 8.3 - - - 2.06 270.3 0.82 0.59\nViT-XL [16] 451M 400 8.10 - - - 2.06 - - -\nU-ViT-H/2 [2] 501M 400 6.58 - - - 2.29 263.9 0.82 0.57\nMaskDiT [14] 675M 1600 5.69 178.0 0.74 0.60 2.28 276.6 0.80 0.61\nFlowDCN [48] 618M 400 8.36 122.5 0.69 0.65 2.00 263.1 0.82 0.58\nRDM [44] 553M / 5.27 153.4 0.75 0.62 1.99 260.4 0.81 0.58\nREPA [52] 675M 800 5.9 157.8 0.70 0.69 1.42 305.7 0.80 0.64\nDDT-XL (Ours) 675M 80 6.62 135.2 0.69 0.67 1.52 263.7 0.78 0.63\nDDT-XL (Ours) 675M 256 6.30 146.7 0.68 0.68 1.31 308.1 0.78 0.62\nDDT-XL (Ours) 675M 400 6.27 154.7 0.68 0.69 1.26 310.6 0.79 0.65\nTable 1. System performance comparison on ImageNet 256 Ã— 256 class-conditioned generation. Gray blocks mean the algorithm uses\nV AE trained or fine-tuned on ImageNet instead of the off-shelf SD-V AE-f8d4-ft-ema.\nModel FIDâ†“ sFIDâ†“ ISâ†‘ Prec.â†‘ Rec.â†‘\nSiT-B/2 [32] 33.0 6.46 43.7 0.53 0.63\nREPA-B/2 [52] 24.4 6.40 59.9 0.59 0.65\nREPA-B/2(Reproduced) 22.2 7.50 69.1 0.59 0.65\nDDT-B/2â€  (8En4De) 21.1 7.81 73.0 0.60 0.65\nImproved-SiT-B/2 25.1 6.54 58.8 0.57 0.64\nImproved-REPA-B/2 19.1 6.88 76.49 0.60 0.66\nDDT-B/2 (8En4De) 16.32 6.63 86.0 0.62 0.66\nSiT-L/2 [32] 18.8 5.29 72.0 0.64 0.64\nREPA-L/2 [52] 10.0 5.20 109.2 0.69 0.65\nImproved-SiT-L/2 12.7 5.48 95.7 0.65 0.65\nImproved-REPA-L/2 9.3 5.44 116.6 0.67 0.66\nDDT-L/2 (20En4De) 7.98 5.50 128.1 0.68 0.67\nSiT-XL/2 [32] 17.2 5.07 76.52 0.65 0.63\nREPA-XL/2 [52] 7.9 5.06 122.6 0.70 0.65\nImproved-SiT-XL/2 10.9 5.3 103.4 0.66 0.65\nImproved-REPA-XL/2 8.14 5.34 124.9 0.68 0.67\nDDT-XL/2 (22En6De) 6.62 4.86 135.1 0.69 0.67\nTable 2. Metrics of 400K training steps with different model\nsizes. All results are reported without classifier-free guidance.\ngray means metrics are copied from the original paper, otherwise\nit is produced by our codebase. By default, our DDT models are\nbuilt on improved baselines. DDT â€  means model built on naive\nbaseline without architecture improvement and lognorm sampling,\nconsistent to REPA. Our DDT models consistently outperformed\ntheir counterparts.\n5.3. System level comparision\nImageNet 256Ã—256. We report the final metrics of DDT-\nXL/2 (22En6De) and DDT-L/2 (20En4De) at Tab. 1. Our\nDDT models demonstrate exceptional efficiency, achiev-\ning convergence in approximately 1\n4 of the total epochs\ncompared to REPA [52] and other diffusion transformer\nmodels. In order to maintain methodological consistency\nwith REPA, we employed the classifier-free guidance with\n2.0 in the interval [0.3, 1], Our models delivered impres-\nsive results: DDT-L/2 achieved 1.64 FID, and DDT-XL/2\ngot 1.52 FID within just 80 epochs. By extending train-\ning to 256 epochsâ€”still significantly more efficient than\ntraditional 800-epoch approachesâ€”our DDT-XL/2 estab-\nlished a new state-of-the-art benchmark of 1.31 FID on\nImageNet 256Ã—256, decisively outperforming previous dif-\nfusion transformer methodologies. To extend training to\n400 epochs, our DDT-XL/2(22En6De) achieves 1.26 FID,\nnearly reaching the upper limit of SD-V AE-ft-EMA-f8d4,\nwhich has a 1.20 rFID on ImageNet256.\nImageNet 512Ã—512 We provide the final metrics of DDT-\nXL/2 at Tab. 3. To validate the superiority of our DDT\nmodel, we take our DDT-XL/2 trained on ImageNet 256 Ã—\n256 under 256 epochs as the initialization, fine-tune out\nDDT-XL/2 on ImageNet 512 Ã— 512 for 100K steps. We\nadopt the aforementioned interval guidance [26] and we\nachieved a remarkable state-of-the-art performance of 1.90\nFID, decisively outperforming REPA by a significant 0.28\n6ImageNet 512 Ã— 512\nModel FIDâ†“ sFIDâ†“ ISâ†‘ Pre.â†‘ Rec.â†‘\nBigGAN-deep [3] 8.43 8.13 177.90 0.88 0.29\nStyleGAN-XL [40] 2.41 4.06 267.75 0.77 0.52\nADM-G [10] 7.72 6.57 172.71 0.87 0.42\nADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53\nDiT-XL/2 [36] 3.04 5.02 240.82 0.84 0.54\nSiT-XL/2 [32] 2.62 4.18 252.21 0.84 0.57\nREPA-XL/2 [52] 2.08 4.19 274.6 0.83 0.58\nFlowDCN-XL/2 [48] 2.44 4.53 252.8 0.84 0.54\nDDT-XL/2 (500K) 1.28 4.22 305.1 0.80 0.63\nTable 3. Benchmarking class-conditional image generation on\nImageNet 512 Ã—512. Our DDT-XL/2(512 Ã— 512) is fine-tuned\nfrom the same model trained on 256 Ã— 256 resolution setting of\n1.28M steps. We adopt the interval guidance with interval [0.3, 1]\nand CFG of 3.0\nperformance margin. In Tab. 3, some metrics exhibit sub-\ntle degradation, we attribute this to potentially insufficient\nfine-tuning. When allocating more training iterations to\nDDT-XL/2, it achieves1.28 FID at 500K steps with CFG3.0\nwithin the time interval [0.3, 1.0].\n5.4. Acceleration by Encoder sharing\nAs illustrated in Fig. 5, there is a strong local consistency of\nthe self-condition in our condition encoder. Even zt=0 has\na strong similarity above 0.8 with zt=1. This consistency\nprovides an opportunity to speed up inference by sharing\nthe encoder between adjacent steps.\nWe employed the simple uniform encoder sharing strat-\negy and the new novel statistics dynamic programming\nstrategy. Specifically, for the uniform strategy, we only re-\ncalculate the self-condition zt every K steps. For statistics\ndynamic programming, we solve the aforementioned mini-\nmal sum path on the similarity matrix by dynamic program-\nming and recalculate zt according to the solved strategy.\nAs shown in Fig. 6, there is a significant inference speedup\nnearly without visual quality loss when K is smaller than\n6. As shown in Tab. 4, the metrics loss is still marginal,\nwhile the inference speedup is significant. The novel statis-\ntics dynamic programming slightly outperformed the naive\nuniform strategy with less FID drop.\n5.5. Ablations\nWe conduct ablation studies on ImageNet 256 Ã— 256 with\nDDT-B/2 and DDT-L/2. For sampling, we take the Eu-\nler solver with 250 steps as the default choice without\nclassifier-free guidance. For training, we train each model\nwith 80 epochs(400k steps), and the batch size is set to 256.\nEncoder-Decoder Ratio we systematically explored ra-\ntios ranging from 2 : 1 to 5 : 1 across different model sizes.\nSharRatio Acc Î¦ FIDâ†“ sFIDâ†“ ISâ†‘ Prec.â†‘ Rec.â†‘\n0.00 1.0Ã— Uniform 1.31 4.62 308.1 0.78 0.66\n0.50 1.6Ã— Uniform 1.31 4.48 300.5 0.78 0.65\n0.66 1.9Ã— Uniform 1.32 4.46 301.2 0.78 0.65\n0.75 2.3Ã— Uniform 1.34 4.43 302.7 0.78 0.65\n0.80 2.6Ã— Uniform 1.36 4.40 303.3 0.78 0.64\nStatisticDP 1.33 4.37 301.7 0.78 0.64\n0.83 2.7Ã— Uniform 1.37 4.41 302.8 0.78 0.64\nStatisticDP 1.36 4.35 300.3 0.78 0.64\n0.87 3.0Ã— Uniform 1.42 4.43 302.8 0.78 0.64\nStatisticDP 1.40 4.35 302.4 0.78 0.64\nTable 4. Metrics of 400K training steps with different model\nsizes. All results are reported without classifier-free guidance.\ngray means metrics are copied from the original paper, otherwise\nit is produced by our codebase. Our DDT models consistently out-\nperformed its counterparts\nTimsteps:   0                                     1Timsteps:   0                                     1\nFigure 5. The cosine similarity of self-condition featurezt from\nencoder between different timesteps. There is a strong correla-\ntion between adjacent steps, indicating the redundancy.\nSharRatio75%  (Acc 2.3x)83%  (Acc 2.7x)87%  (Acc 3.0x)50%(Acc 1.6x)\nFigure 6. Sharing the self-condition zt in adjacent steps signif-\nicant speedup the inference. We tried various sharing frequency\nconfigurations. There is marginal visual quality down-gradation\nwhen the sharing frequency is reasonable.\nin Fig. 7 and Fig. 8. Our notationmEnnDe represents mod-\nels with m encoder layers and n decoder layers. The inves-\n7100 150 200 250 300 350 400\nTraining iterations(K steps)\n16\n18\n20\n22\n24\n26\n28\n30\n32FID50K\n4En8De\n6En6De\n8En4De\n10En2De\n100 150 200 250 300 350 400\nTraining iterations(K steps)\n45\n50\n55\n60\n65\n70\n75\n80\n85InceptionScore\n4En8De\n6En6De\n8En4De\n10En2De\n100 150 200 250 300 350 400\nTraining iterations(K steps)\n0.635\n0.640\n0.645\n0.650\n0.655\n0.660Recall\n4En8De\n6En6De\n8En4De\n10En2De\nFigure 7. The DDT-B/2 built upon Improved-baselines under various Encoder and Decoder layer ratio.DDT-B/2(8En4De) achieves\nmuch faster convergence speed and better performance.\n100 150 200 250 300 350 400\nTraining iterations(K steps)\n8\n9\n10\n11\n12\n13\n14\n15FID50K\n10En14De\n12En12De\n16En8De\n20En4De\n100 150 200 250 300 350 400\nTraining iterations(K steps)\n80\n90\n100\n110\n120\n130InceptionScore\n10En14De\n12En12De\n16En8De\n20En4De\n100 150 200 250 300 350 400\nTraining iterations(K steps)\n0.645\n0.650\n0.655\n0.660\n0.665\n0.670\n0.675Recall\n10En14De\n12En12De\n16En8De\n20En4De\nFigure 8. The DDT-L/2 built upon Improved-baselines under various Encoder and Decoder layer ratio. DDT-L/2 prefers an unex-\npected aggressive encoder-deocder ratio DDT-L/2(20En4De) achieves much faster convergence speed and better performance.\ntigation experiments in Fig. 7 and Fig. 8 revealed critical\ninsights into architectural optimization. We observed that\na larger encoder is beneficial for further improving the\nperformance as the model size increases . For the Base\nmodel in Fig. 7, the optimal configuration emerged as 8 en-\ncoder layers and 4 decoder layers, delivering superior per-\nformance and convergence speed. Notably, the Large model\nin Fig. 8 exhibited a distinct preference, achieving peak per-\nformance with 20 encoder layers and 4 decoder layers, an\nunexpectedly aggressive encoder-decoder ratio. This un-\nexpected discovery motivates us to scale the layer ratio in\nDDT-XL/2 to 22 encoder layers and 6 decoders to explore\nthe performance upper limits of diffusion transformers.\nDecoder Block types. In our investigation of decoder\nblock types and their impact on high-frequency decoding\nperformance, we systematically evaluated multiple archi-\ntectural configurations. Our comprehensive assessment in-\ncluded alternative approaches such as simple 3Ã—3 convolu-\ntion blocks and naive MLP blocks. As shown in Tab. 5,\nthe default (Attention with the MLP) setting achieves better\nresults. Thanks to the encoder-decoder design, naive Conv\nblocks even achieve comparable results.\n6. Conclusion\nIn this paper, we have introduced a novel Decoupled Diffu-\nsion Transformer, which rethinks the optimization dilemma\nDecoderBlock FIDâ†“ sFIDâ†“ ISâ†‘ Prec.â†‘ Rec.â†‘\nConv+MLP 16.96 7.33 85.1 0.62 0.65\nMLP+MLP 24.13 7.89 65.0 0.57 0.65\nAttn+MLP 16.32 6.63 86.0 0.62 0.66\nTable 5. Metrics of 400K training steps on DDT-B/2(8En4De)\nwith different decoder blocks. All results are reported without\nclassifier-free guidance. The Default Attention + MLP configura-\ntion achieves best performance.\nof the traditional diffusion transformer. By decoupling\nthe low-frequency encoding and high-frequency decoding\ninto dedicated components, we effectively resolved the\noptimization dilemma that has constrained diffusion trans-\nformer. Furthermore, we discovered that increasing the\nencoder capacity relative to the decoder yields increasingly\nbeneficial results as the overall model scale grows. This\ninsight provides valuable guidance for future model scaling\nefforts. Our experiments demonstrate that our DDT-XL/2\n(22En6De) with an unexpected aggressive encoder-decoder\nlayer ratio achieves great performance while requiring\nonly 256 training epochs. This significant improvement\nin efficiency addresses one of the primary limitations of\ndiffusion models: their lengthy training requirements.\nThe decoupled architecture also presents opportunities\nfor inference optimization through our proposed encoder\nresult sharing mechanism. Our statistical dynamic pro-\ngramming approach for determining optimal sharing\nstrategies enables faster inference while minimizing quality\n8degradation, demonstrating that architectural innovations\ncan yield benefits beyond their primary design objectives.\nReferences\n[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji,\nErik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin\nChen, Yin Cui, Yifan Ding, et al. Cosmos world foun-\ndation model platform for physical ai. arXiv preprint\narXiv:2501.03575, 2025. 2\n[2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,\nHang Su, and Jun Zhu. All are worth words: A vit backbone\nfor diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22669â€“22679, 2023. 2, 6\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv preprint arXiv:1809.11096, 2018. 1, 7\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213â€“229. Springer, 2020. 2\n[5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11315â€“11325, 2022.\n1\n[6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, et al. Pixart- \\alpha: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis. arXiv\npreprint arXiv:2310.00426, 2023. 2\n[7] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei\nYao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu,\nand Zhenguo Li. Pixart- \\sigma: Weak-to-strong training of\ndiffusion transformer for 4k text-to-image generation. arXiv\npreprint arXiv:2403.04692, 2024. 2\n[8] Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen.\nVisionllama: A unified llama interface for vision tasks.arXiv\npreprint arXiv:2403.00522, 2024. 5\n[9] Chaorui Deng, Deyao Zh, Kunchang Li, Shi Guan, and\nHaoqi Fan. Causal diffusion transformers for generative\nmodeling. arXiv preprint arXiv:2412.12095, 2024. 6, 12\n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780â€“8794, 2021. 2, 6, 7, 11\n[11] Sander Dieleman. Diffusion is spectral autoregression, 2024.\n3, 4\n[12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M Â¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified\nflow transformers for high-resolution image synthesis. arXiv\npreprint arXiv:2403.03206, 2024. 2, 5\n[13] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang\nLi, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-\nlike architectures for diffusion models. arXiv preprint\narXiv:2404.04478, 2024. 2\n[14] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and\nShuicheng Yan. Masked diffusion transformer is a strong\nimage synthesizer. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision , pages 23164â€“23173,\n2023. 3, 6\n[15] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and\nShuicheng Yan. Masked diffusion transformer is a strong\nimage synthesizer. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 23164â€“23173,\n2023. 3\n[16] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong\nChen, Han Hu, Xin Geng, and Baining Guo. Efficient diffu-\nsion training via min-snr weighting strategy. In Proceedings\nof the IEEE/CVF international conference on computer vi-\nsion, pages 7441â€“7451, 2023. 2, 6\n[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDollÂ´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 16000â€“\n16009, 2022. 2\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems ,\n30, 2017. 5\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840â€“6851, 2020. 1\n[20] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang. Cogvideo: Large-scale pretraining for\ntext-to-video generation via transformers. arXiv preprint\narXiv:2205.15868, 2022. 2\n[21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in Neural Information Processing Sys-\ntems, 35:26565â€“26577, 2022. 1\n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 11\n[23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In Proceedings of the IEEE/CVF international confer-\nence on computer vision, pages 4015â€“4026, 2023. 2\n[24] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024.\n2\n[25] Tuomas Kynk Â¨aÂ¨anniemi, Tero Karras, Samuli Laine, Jaakko\nLehtinen, and Timo Aila. Improved precision and recall met-\nric for assessing generative models. Advances in neural in-\nformation processing systems, 32, 2019. 5\n[26] Tuomas Kynk Â¨aÂ¨anniemi, Miika Aittala, Tero Karras, Samuli\nLaine, Timo Aila, and Jaakko Lehtinen. Applying guidance\nin a limited interval improves sample and distribution quality\nin diffusion models. arXiv preprint arXiv:2404.07724, 2024.\n6\n9[27] Tianhong Li, Dina Katabi, and Kaiming He. Return of\nunconditional generation: A self-supervised representation\ngeneration method. Advances in Neural Information Pro-\ncessing Systems, 37:125441â€“125468, 2024. 3\n[28] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and\nKaiming He. Autoregressive image generation without vec-\ntor quantization. Advances in Neural Information Processing\nSystems, 37:56424â€“56445, 2025. 2, 4, 6\n[29] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximil-\nian Nickel, and Matt Le. Flow matching for generative mod-\neling. arXiv preprint arXiv:2210.02747, 2022. 1, 3\n[30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow\nstraight and fast: Learning to generate and transfer data with\nrectified flow. arXiv preprint arXiv:2209.03003, 2022. 1, 3\n[31] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xi-\nhui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible\nvision transformer for diffusion model. arXiv preprint\narXiv:2402.12376, 2024. 5\n[32] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M\nBoffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Explor-\ning flow and diffusion-based generative models with scalable\ninterpolant transformers. arXiv preprint arXiv:2401.08740,\n2024. 2, 3, 5, 6, 7\n[33] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache:\nAccelerating diffusion models for free. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 15762â€“15772, 2024. 5\n[34] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W\nBattaglia. Generating images with sparse representations.\narXiv preprint arXiv:2103.03841, 2021. 5\n[35] Maxime Oquab, Timoth Â´ee Darcet, Th Â´eo Moutakanni, Huy\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 4\n[36] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195â€“4205,\n2023. 2, 4, 5, 6, 7\n[37] Severi Rissanen, Markus Heinonen, and Arno Solin. Gener-\native modelling with inverse heat dissipation. arXiv preprint\narXiv:2206.13397, 2022. 3, 4\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj Â¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684â€“10695, 2022. 2, 6\n[39] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 5\n[40] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-\nxl: Scaling stylegan to large diverse datasets. In ACM SIG-\nGRAPH 2022 conference proceedings, pages 1â€“10, 2022. 1,\n7\n[41] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 1\n[42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063,\n2024. 5\n[43] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 1\n[44] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jian-\nqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion:\nUnifying diffusion process across resolutions for image syn-\nthesis. arXiv preprint arXiv:2309.03350, 2023. 6\n[45] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu\nWang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba\nfor efficient high-resolution image synthesis. arXiv preprint\narXiv:2405.14224, 2024. 2\n[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, TimothÂ´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 5\n[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models.arXiv\npreprint arXiv:2307.09288, 2023. 5\n[48] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng\nGe, Bo Zheng, and Limin Wang. Flowdcn: Exploring dcn-\nlike architectures for fast image generation with arbitrary res-\nolution. arXiv preprint arXiv:2410.22655, 2024. 2, 6, 7\n[49] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush.\nDiffusion models without attention. arXiv preprint\narXiv:2311.18257, 2023. 2\n[50] Jingfeng Yao and Xinggang Wang. Reconstruction vs. gener-\nation: Taming optimization dilemma in latent diffusion mod-\nels. arXiv preprint arXiv:2501.01423, 2025. 5, 6\n[51] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-\nChieh Chen. Randomized autoregressive visual generation.\narXiv preprint arXiv:2411.00776, 2024. 1\n[52] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon\nJeong, Jonathan Huang, Jinwoo Shin, and Saining Xie.\nRepresentation alignment for generation: Training diffu-\nsion transformers is easier than you think. arXiv preprint\narXiv:2410.06940, 2024. 2, 3, 4, 5, 6, 7, 12\n[53] Xiaoyu Yue, Zidong Wang, Zeyu Lu, Shuyang Sun, Meng\nWei, Wanli Ouyang, Lei Bai, and Luping Zhou. Diffu-\nsion models need visual priors for image generation. arXiv\npreprint arXiv:2410.08531, 2024. 2, 3\n[54] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang\nLiu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang,\nZhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger\nand faster with next-dit. arXiv preprint arXiv:2406.18583 ,\n2024. 2\n10A. Model Specs\nConfig #Layers Hidden dim #Heads\nB/2 12 768 12\nL/2 24 1024 16\nXL/2 28 1152 16\nB. Hyper-parameters\nV AE SD-V AE-f8d4-ft-ema\nV AE donwsample 8\nlatent channel 4\noptimizer AdamW [22]\nbase learning rate 1e-4\nweight decay 0.0\nbatch size 256\nlearning rate schedule constant\naugmentation center crop\ndiffusion sampler Euler-ODE\ndiffusion steps 250\nevaluation suite ADM [10]\nC. Linear flow and Diffusion\nGiven the SDE forward and reverse process:\ndxt = f(t)xtdt + g(t)dw (9)\ndxt = [f(t)xt âˆ’ g(t)2âˆ‡x log p(xt)]dt + g(t)dw (10)\nA corresponding deterministic process exists with trajecto-\nries sharing the same marginal probability densities of re-\nverse SDE.\ndxt = [f(t)xt âˆ’ 1\n2g(t)2âˆ‡xt log p(xt)]dt (11)\nGiven xt = Î±txdata + ÏƒÏµ. The traditional diffusion model\nlearns:\nâˆ‡xt log p(xt) = âˆ’ Ïµ\nÏƒ(t) (12)\nThe flow-matching framework actually learns the follow-\ning:\nvt = Ë™Î±x + Ë™ÏƒÏµ (13)\n= x âˆ’ Ïµ (14)\nHere we will demonstrate in flow-matching, the vt pre-\ndiction is actually as same as the reverse ode:\nË™Î±x + Ë™ÏƒÏµ (15)\n=f(t)xt âˆ’ 1\n2g(t)2âˆ‡xt log p(xt) (16)\nLet us start by expanding the reverse ode first.\nf(t)xt âˆ’ 1\n2g(t)2âˆ‡xt log p(xt) (17)\n=f(t)(Î±(t)xdata + Ïƒ(t)Ïµ) âˆ’ 1\n2g(t)2[âˆ’ Ïµ\nÏƒ(t)] (18)\n=f(t)Î±(t)xdata + (f(t)Ïƒ(t) + 1\n2\ng(t)2\nÏƒ(t) )Ïµ (19)\nTo prove Eq. (16), we needs to demonstrate that:\nË™Î±(t) = ftÎ±(t) (20)\nË™Ïƒ(t) = ftÏƒ(t) + 1\n2\ng2\nt\nÏƒ(t). (21)\nHere, let us derive the relation betweenft and Î±(t), Ë™Î±(t).\nWe donate xdata(t) = Î±(t)xdata is the remain component\nof xdata in xt, it is easy to find that:\ndxdata(t) = ftxdata(t)dt (22)\nd(Î±(t)xdata) = ftÎ±(t)xdatadt (23)\ndÎ±(t) = ftÎ±(t)dt (24)\nSo, Eq. (20) is right.\nBased on the above equation, we will demonstrate the\nrelation of gt, ft with Ïƒ(t). Note that Gaussian noise has\nnice additive properties.\naÏµ1 + bÏµ2 âˆˆ N(0,\np\na2 + b2) (25)\nLet us start with the gaussian noise component Ïµ(t) calcu-\nlation, reaching at t, every noise addition at s âˆˆ [0, t] while\nbeen decayed by a factor of Î±(t)\nÎ±(s) . Thus, the mixed Gaussian\nnoise will have a std variance Ïƒ(t) of:\nÏƒ(t) =\ns\n(\nZ t\n0\n[( Î±(t)\nÎ±(s))2g2s]ds) (26)\nÏƒ(t) = Î±(t)\ns\n(\nZ t\n0\n[( gs\nÎ±(s))2]ds) (27)\nAfter obtaining the relation of ft, gt and Î±(t), Ïƒ(t), we de-\nrive Ë™Î±(t) and Ë™Ïƒ(t) with above conditions:\nË™Î±(t) = ft exp[\nZ t\n0\nfsds] (28)\nË™Î±(t) = ftÎ±(t) (29)\n11As for Ë™Ïƒ(t), it is quit complex but not hard:\nË™Ïƒ(t) = Ë™Î±(t)\ns\n(\nZ t\n0\n[( gt\nÎ±(s))2]ds) + Î±(t)\n1\n2\ng2\nt\nÎ±(t)q\n(\nRt\n0 [( gt\nÎ±(s) )2g2s]ds)\n(30)\nË™Ïƒ(t) = (ftÎ±(t))\ns\n(\nZ t\n0\n[( gt\nÎ±(s))2]ds) + Î±(t)\n1\n2\ng2\nt\nÎ±2(t)q\n(\nRt\n0 [( gt\nÎ±(s) )2]ds)\n(31)\nË™Ïƒ(t) = ftÎ±(t)\ns\n(\nZ t\n0\n[( gt\nÎ±(s))2]ds) +\n1\n2 g2\nt\nÎ±(t)\nq\n(\nRt\n0 [( gt\nÎ±(s) )2]ds)\n(32)\nË™Ïƒ(t) = ftÏƒ(t) + 1\n2\ngt\nÏƒ(t) (33)\nSo, Eq. (21) is right.\nD. Proof of Spectrum Autoregressive\nGiven the noise scheduler{Î±t, Ïƒt}, the clean data xdata and\nGaussian noise Ïµ. Denote Kfreq as the maximum frequency\nof the clean data xdata The noisy latent xt at timestep t has\nbeen defined as:\nxt = Î±txdata + ÏƒtÏµ (34)\nThe spectrum magnitude ciof xt on DCT basics ui fol-\nlows:\nci = EÏµ[uT\ni xt]2\nci = EÏµ[uT\ni (Î±txdata + ÏƒtÏµ)]2\nRecall that the spectrum magnitude of Gaussian noise Ïµ\nis uniformly distributed.\nci = [Î±tuT\ni xdata]2 + 2Î±tÏƒtEÏµ[uT\ni xdatauT\ni Ïµ] + Ïƒ2\nt EÏµ[uT\ni Ïµ]2\nci = [Î±tuT\ni xdata]2 + Ïƒ2\nt EÏµ[uT\ni Ïµ]2\nci = Î±2\nt [uT\ni xdata]2 + Ïƒ2\nt Î»\nif Ïƒ2\nt Î» has bigger value than[Î±tuT\ni xdata]2, the spectrum\nmagnitude ci on DCT basics ui will be canceled, thus the\nmaximal remaining frequency fmax(t) of original data in\nxt follows:\nfmax(t) > min\n \u0012Î±tuT\ni xdata\nÏƒtÎ»\n\u00132\n, Kfreq\n!\n(35)\nThough Î±tuT\ni xdata\nÏƒtÎ»\n2\ndepends on the dataset. Here, we\ndirectly suppose it as a constant 1. And replace Î± = t and\nÏƒ = 1 âˆ’ t in above equation:\nfmax(t) > min\n \u0012 t\n1 âˆ’ t\n\u00132\n, Kfreq\n!\n(36)\nE. Linear multisteps method\nWe conduct targeted experiment on SiT-XL/2 with\nAdamsâ€“Bashforth like linear multistep solver; To clarify,\nwe did not employ this powerful solver for our DDT mod-\nels in all tables across the main paper.\nThe reverse ode of the diffusion models tackles the fol-\nlowing integral:\nxi+1 = xi +\nZ ti+1\nti\nvÎ¸(xt, t)dt (37)\nThe classic Euler method employs vÎ¸(xi, ti) as an estimate\nof vÎ¸(xt, t) throughout the interval [ti, ti+1]\nxi+1 = xi + (ti+1 âˆ’ ti)vÎ¸(xi, ti). (38)\nThe most classic multi-step solver Adamsâ€“Bashforth\nmethod (deemed as Adams for brevity) incorporates the La-\ngrange polynomial to improve the estimation accuracy with\nprevious predictions.\nvÎ¸(xt, t) =\niX\nj=0\n(\niY\nk=0,kÌ¸=j\nt âˆ’ tk\ntj âˆ’ tk\n)vÎ¸(xj, tj)\nxi+1 â‰ˆ xi +\nZ ti+1\nti\niX\nj=0\n(\niY\nk=0,kÌ¸=j\nt âˆ’ tk\ntj âˆ’ tk\n)vÎ¸(xj, tj)dt\nxi+1 â‰ˆ xi +\niX\nj=0\nvÎ¸(xj, tj)\nZ ti+1\nti\n(\niY\nk=0,kÌ¸=j\nt âˆ’ tk\ntj âˆ’ tk\n)dt\nNote that\nRti+1\nti\n(Qi\nk=0,kÌ¸=j\ntâˆ’tk\ntjâˆ’tk\n)dt of the Lagrange poly-\nnomial can be pre-integrated into a constant coefficient, re-\nsulting in only naive summation being required for ODE\nsolving.\nF. Classifier free guidance.\nAs classifier-free guidance significantly impacts the per-\nformance of diffusion models. Traditional classifier-free\nguidance improves performance at the cost of decreased\ndiversity. Interval guidance is recently been adopted by\nREPA[52] and Causalfusion[9], It applies classifier-free\nguidance only to the high-frequency generation phase to\npreserve the diversity. We sweep different classifier-free\nguidance strength with selected intervals. Our DDT-XL/2\nachieves the best performance with interval [0.3, 1] with a\nclassifer-free guidance of 2. Recall that we donate t = 0 as\nthe pure noise timestep while REPA[52] uset = 1, thus this\nexactly correspond to the [0, 0.7] interval in REPA[52]\n121.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6\nCFG values\n4\n5\n6\n7\n8\n9FID10K\nClassifer-free guidance with intervals\nInterval: [0,      1]\nInterval: [0.2,   1]\nInterval: [0.3,   1]\nInterval: [0.35, 1]\nFigure 9. FID10K of DDT-XL/2 with different Classifer free\nguidance strength and guidance intervals. We sweep different\nclassifier-free guidance strength with selected intervals. Our DDT-\nXL/2 achieves the best performance with interval [0.3, 1] with a\nclassifer-free guidance of 2.\n13"
    }
}