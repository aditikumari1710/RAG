{
    "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
    "content": {
        "page_content": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\nLantao Yu†, Weinan Zhang†∗, Jun Wang‡, Yong Yu†\n†Shanghai Jiao Tong University,‡University College London\n{yulantao,wnzhang,yyu}@apex.sjtu.edu.cn, j.wang@cs.ucl.ac.uk\nAbstract\nAs a new way of training generative models, Generative Ad-\nversarial Net (GAN) that uses a discriminative model to guide\nthe training of the generative model has enjoyed considerable\nsuccess in generating real-valued data. However, it has limi-\ntations when the goal is for generating sequences of discrete\ntokens. A major reason lies in that the discrete outputs from\nthe generative model make it difﬁcult to pass the gradient up-\ndate from the discriminative model to the generative model.\nAlso, the discriminative model can only assess a complete\nsequence, while for a partially generated sequence, it is non-\ntrivial to balance its current score and the future one once\nthe entire sequence has been generated. In this paper, we pro-\npose a sequence generation framework, called SeqGAN, to\nsolve the problems. Modeling the data generator as a stochas-\ntic policy in reinforcement learning (RL), SeqGAN bypasses\nthe generator differentiation problem by directly performing\ngradient policy update. The RL reward signal comes from\nthe GAN discriminator judged on a complete sequence, and\nis passed back to the intermediate state-action steps using\nMonte Carlo search. Extensive experiments on synthetic data\nand real-world tasks demonstrate signiﬁcant improvements\nover strong baselines.\nIntroduction\nGenerating sequential synthetic data that mimics the real\none is an important problem in unsupervised learning. Re-\ncently, recurrent neural networks (RNNs) with long short-\nterm memory (LSTM) cells (Hochreiter and Schmidhuber\n1997) have shown excellent performance ranging from nat-\nural language generation to handwriting generation (Wen\net al. 2015; Graves 2013). The most common approach to\ntraining an RNN is to maximize the log predictive likelihood\nof each true token in the training sequence given the pre-\nvious observed tokens (Salakhutdinov 2009). However, as\nargued in (Bengio et al. 2015), the maximum likelihood ap-\nproaches suffer from so-calledexposure biasin the inference\nstage: the model generates a sequence iteratively and pre-\ndicts next token conditioned on its previously predicted ones\nthat may be never observed in the training data. Such a dis-\ncrepancy between training and inference can incur accumu-\nlatively along with the sequence and will become prominent\n∗Weinan Zhang is the corresponding author.\nCopyright c⃝2017, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nas the length of sequence increases. To address this prob-\nlem, (Bengio et al. 2015) proposed a training strategy called\nscheduled sampling (SS), where the generative model is par-\ntially fed with its own synthetic data as preﬁx (observed to-\nkens) rather than the true data when deciding the next token\nin the training stage. Nevertheless, (Husz ´ar 2015) showed\nthat SS is an inconsistent training strategy and fails to ad-\ndress the problem fundamentally. Another possible solution\nof the training/inference discrepancy problem is to build\nthe loss function on the entire generated sequence instead\nof each transition. For instance, in the application of ma-\nchine translation, a task speciﬁc sequence score/loss, bilin-\ngual evaluation understudy (BLEU) (Papineni et al. 2002),\ncan be adopted to guide the sequence generation. However,\nin many other practical applications, such as poem genera-\ntion (Zhang and Lapata 2014) and chatbot (Hingston 2009),\na task speciﬁc loss may not be directly available to score a\ngenerated sequence accurately.\nGeneral adversarial net (GAN) proposed by (Goodfellow\nand others 2014) is a promising framework for alleviating\nthe above problem. Speciﬁcally, in GAN a discriminative\nnet Dlearns to distinguish whether a given data instance is\nreal or not, and a generative net G learns to confuse D by\ngenerating high quality data. This approach has been suc-\ncessful and been mostly applied in computer vision tasks of\ngenerating samples of natural images (Denton et al. 2015).\nUnfortunately, applying GAN to generating sequences\nhas two problems. Firstly, GAN is designed for generat-\ning real-valued, continuous data but has difﬁculties in di-\nrectly generating sequences of discrete tokens, such as texts\n(Husz´ar 2015). The reason is that in GANs, the genera-\ntor starts with random sampling ﬁrst and then a determistic\ntransform, govermented by the model parameters. As such,\nthe gradient of the loss from D w.r.t. the outputs by G is\nused to guide the generative modelG(paramters) to slightly\nchange the generated value to make it more realistic. If\nthe generated data is based on discrete tokens, the “slight\nchange” guidance from the discriminative net makes little\nsense because there is probably no corresponding token for\nsuch slight change in the limited dictionary space (Goodfel-\nlow 2016). Secondly, GAN can only give the score/loss for\nan entire sequence when it has been generated; for a partially\ngenerated sequence, it is non-trivial to balance how good as\nit is now and the future score as the entire sequence.\narXiv:1609.05473v6  [cs.LG]  25 Aug 2017In this paper, to address the above two issues, we follow\n(Bachman and Precup 2015; Bahdanau et al. 2016) and con-\nsider the sequence generation procedure as a sequential de-\ncision making process. The generative model is treated as an\nagent of reinforcement learning (RL); the state is the gener-\nated tokens so far and the action is the next token to be gener-\nated. Unlike the work in (Bahdanau et al. 2016) that requires\na task-speciﬁc sequence score, such as BLEU in machine\ntranslation, to give the reward, we employ a discriminator to\nevaluate the sequence and feedback the evaluation to guide\nthe learning of the generative model. To solve the problem\nthat the gradient cannot pass back to the generative model\nwhen the output is discrete, we regard the generative model\nas a stochastic parametrized policy. In our policy gradient,\nwe employ Monte Carlo (MC) search to approximate the\nstate-action value. We directly train the policy (generative\nmodel) via policy gradient (Sutton et al. 1999), which natu-\nrally avoids the differentiation difﬁculty for discrete data in\na conventional GAN.\nExtensive experiments based on synthetic and real data\nare conducted to investigate the efﬁcacy and properties of\nthe proposed SeqGAN. In our synthetic data environment,\nSeqGAN signiﬁcantly outperforms the maximum likelihood\nmethods, scheduled sampling and PG-BLEU. In three real-\nworld tasks, i.e. poem generation, speech language gener-\nation and music generation, SeqGAN signiﬁcantly outper-\nforms the compared baselines in various metrics including\nhuman expert judgement.\nRelated Work\nDeep generative models have recently drawn signiﬁcant\nattention, and the ability of learning over large (unla-\nbeled) data endows them with more potential and vitality\n(Salakhutdinov 2009; Bengio et al. 2013). (Hinton, Osin-\ndero, and Teh 2006) ﬁrst proposed to use the contrastive di-\nvergence algorithm to efﬁciently training deep belief nets\n(DBN). (Bengio et al. 2013) proposed denoising autoen-\ncoder (DAE) that learns the data distribution in a supervised\nlearning fashion. Both DBN and DAE learn a low dimen-\nsional representation (encoding) for each data instance and\ngenerate it from a decoding network. Recently, variational\nautoencoder (V AE) that combines deep learning with sta-\ntistical inference intended to represent a data instance in\na latent hidden space (Kingma and Welling 2014), while\nstill utilizing (deep) neural networks for non-linear mapping.\nThe inference is done via variational methods. All these gen-\nerative models are trained by maximizing (the lower bound\nof) training data likelihood, which, as mentioned by (Good-\nfellow and others 2014), suffers from the difﬁculty of ap-\nproximating intractable probabilistic computations.\n(Goodfellow and others 2014) proposed an alternative\ntraining methodology to generative models, i.e. GANs,\nwhere the training procedure is a minimax game between\na generative model and a discriminative model. This frame-\nwork bypasses the difﬁculty of maximum likelihood learn-\ning and has gained striking successes in natural image gen-\neration (Denton et al. 2015). However, little progress has\nbeen made in applying GANs to sequence discrete data gen-\neration problems, e.g. natural language generation (Husz ´ar\n2015). This is due to the generator network in GAN is de-\nsigned to be able to adjust the output continuously, which\ndoes not work on discrete data generation (Goodfellow\n2016).\nOn the other hand, a lot of efforts have been made to gen-\nerate structured sequences. Recurrent neural networks can\nbe trained to produce sequences of tokens in many applica-\ntions such as machine translation (Sutskever, Vinyals, and\nLe 2014; Bahdanau, Cho, and Bengio 2014). The most pop-\nular way of training RNNs is to maximize the likelihood of\neach token in the training data whereas (Bengio et al. 2015)\npointed out that the discrepancy between training and gen-\nerating makes the maximum likelihood estimation subopti-\nmal and proposed scheduled sampling strategy (SS). Later\n(Husz´ar 2015) theorized that the objective function under-\nneath SS is improper and explained the reason why GANs\ntend to generate natural-looking samples in theory. Conse-\nquently, the GANs have great potential but are not practi-\ncally feasible to discrete probabilistic models currently.\nAs pointed out by (Bachman and Precup 2015), the se-\nquence data generation can be formulated as a sequen-\ntial decision making process, which can be potentially be\nsolved by reinforcement learning techniques. Modeling the\nsequence generator as a policy of picking the next token,\npolicy gradient methods (Sutton et al. 1999) can be adopted\nto optimize the generator once there is an (implicit) re-\nward function to guide the policy. For most practical se-\nquence generation tasks, e.g. machine translation (Sutskever,\nVinyals, and Le 2014), the reward signal is meaningful only\nfor the entire sequence, for instance in the game of Go (Sil-\nver et al. 2016), the reward signal is only set at the end of the\ngame. In those cases, state-action evaluation methods such\nas Monte Carlo (tree) search have been adopted (Browne\net al. 2012). By contract, our proposed SeqGAN extends\nGANs with the RL-based generator to solve the sequence\ngeneration problem, where a reward signal is provided by\nthe discriminator at the end of each episode via Monte Carlo\napproach, and the generator picks the action and learns the\npolicy using estimated overall rewards.\nSequence Generative Adversarial Nets\nThe sequence generation problem is denoted as follows.\nGiven a dataset of real-world structured sequences, train\na θ-parameterized generative model Gθ to produce a se-\nquence Y1:T = (y1,...,y t,...,y T),yt ∈ Y, where Yis\nthe vocabulary of candidate tokens. We interpret this prob-\nlem based on reinforcement learning. In timestep t, the state\nsis the current produced tokens (y1,...,y t−1) and the ac-\ntion ais the next token yt to select. Thus the policy model\nGθ(yt|Y1:t−1) is stochastic, whereas the state transition is\ndeterministic after an action has been chosen, i.e. δa\ns,s′ = 1\nfor the next state s′ = Y1:t if the current state s = Y1:t−1\nand the action a= yt; for other next states s′′, δa\ns,s′′ = 0.\nAdditionally, we also train a φ-parameterized discrimina-\ntive model Dφ (Goodfellow and others 2014) to provide a\nguidance for improving generator Gθ. Dφ(Y1:T) is a prob-\nability indicating how likely a sequence Y1:T is from real\nsequence data or not. As illustrated in Figure 1, the dis-Reward\nNext\naction\nState\nMC\nsearchG D\nGenerate\nTrue data\nTrain\nG\nReal World\nD\nReward\nReward\nReward\nPolicy Gradient\nFigure 1: The illustration of SeqGAN. Left:Dis trained over\nthe real data and the generated data byG. Right: Gis trained\nby policy gradient where the ﬁnal reward signal is provided\nby Dand is passed back to the intermediate action value via\nMonte Carlo search.\ncriminative model Dφ is trained by providing positive ex-\namples from the real sequence data and negative examples\nfrom the synthetic sequences generated from the generative\nmodel Gθ. At the same time, the generative modelGθ is up-\ndated by employing a policy gradient and MC search on the\nbasis of the expected end reward received from the discrim-\ninative model Dφ. The reward is estimated by the likelihood\nthat it would fool the discriminative model Dφ. The speciﬁc\nformulation is given in the next subsection.\nSeqGAN via Policy Gradient\nFollowing (Sutton et al. 1999), when there is no interme-\ndiate reward, the objective of the generator model (policy)\nGθ(yt|Y1:t−1) is to generate a sequence from the start state\ns0 to maximize its expected end reward:\nJ(θ) =E[RT|s0,θ] =\n∑\ny1∈Y\nGθ(y1|s0) ·QGθ\nDφ(s0,y1), (1)\nwhere RT is the reward for a complete sequence. Note that\nthe reward is from the discriminator Dφ, which we will dis-\ncuss later. QGθ\nDφ(s,a) is the action-value function of a se-\nquence, i.e. the expected accumulative reward starting from\nstate s, taking action a, and then following policy Gθ. The\nrational of the objective function for a sequence is that start-\ning from a given initial state, the goal of the generator is\nto generate a sequence which would make the discriminator\nconsider it is real.\nThe next question is how to estimate the action-value\nfunction. In this paper, we use the REINFORCE algorithm\n(Williams 1992) and consider the estimated probability of\nbeing real by the discriminatorDφ(Yn\n1:T) as the reward. For-\nmally, we have:\nQGθ\nDφ(a= yT,s = Y1:T−1) =Dφ(Y1:T). (2)\nHowever, the discriminator only provides a reward value for\na ﬁnished sequence. Since we actually care about the long-\nterm reward, at every timestep, we should not only consider\nthe ﬁtness of previous tokens (preﬁx) but also the resulted\nfuture outcome. This is similar to playing the games such\nas Go or Chess where players sometimes would give up the\nimmediate interests for the long-term victory (Silver et al.\n2016). Thus, to evaluate the action-value for an intermediate\nstate, we apply Monte Carlo search with a roll-out policyGβ\nto sample the unknown last T −ttokens. We represent an\nN-time Monte Carlo search as\n{\nY1\n1:T,...,Y N\n1:T\n}\n= MCGβ(Y1:t; N), (3)\nwhere Yn\n1:t = (y1,...,y t) and Yn\nt+1:T is sampled based on\nthe roll-out policy Gβ and the current state. In our experi-\nment, Gβ is set the same as the generator, but one can use\na simpliﬁed version if the speed is the priority (Silver et al.\n2016). To reduce the variance and get more accurate assess-\nment of the action value, we run the roll-out policy starting\nfrom current state till the end of the sequence forN times to\nget a batch of output samples. Thus, we have:\nQGθ\nDφ(s= Y1:t−1,a = yt) = (4)\n{ 1\nN\n∑N\nn=1 Dφ(Yn\n1:T), Yn\n1:T ∈MCGβ(Y1:t; N) for t<T\nDφ(Y1:t) for t= T,\nwhere, we see that when no intermediate reward, the func-\ntion is iteratively deﬁned as the next-state value starting from\nstate s′= Y1:t and rolling out to the end.\nA beneﬁt of using the discriminator Dφ as a reward func-\ntion is that it can be dynamically updated to further improve\nthe generative model iteratively. Once we have a set of more\nrealistic generated sequences, we shall re-train the discrimi-\nnator model as follows:\nmin\nφ\n−EY∼pdata [log Dφ(Y)] −EY∼Gθ[log(1 −Dφ(Y))]. (5)\nEach time when a new discriminator model has been ob-\ntained, we are ready to update the generator. The proposed\npolicy based method relies upon optimizing a parametrized\npolicy to directly maximize the long-term reward. Following\n(Sutton et al. 1999), the gradient of the objective function\nJ(θ) w.r.t. the generator’s parametersθcan be derived as\n∇θJ(θ) =∑T\nt=1EY1:t−1∼Gθ\n[∑\nyt∈Y\n∇θGθ(yt|Y1:t−1)·QGθ\nDφ(Y1:t−1,yt)].\n(6)\nThe above form is due to the deterministic state transi-\ntion and zero intermediate rewards. The detailed derivation\nis provided in the appendix. Using likelihood ratios (Glynn\n1990; Sutton et al. 1999), we build an unbiased estimation\nfor Eq. (6) (on one episode):\n∇θJ(θ) ≃\nT∑\nt=1\n∑\nyt∈Y\n∇θGθ(yt|Y1:t−1) ·QGθ\nDφ(Y1:t−1,yt) (7)\n=\nT∑\nt=1\n∑\nyt∈Y\nGθ(yt|Y1:t−1)∇θlogGθ(yt|Y1:t−1) ·QGθ\nDφ(Y1:t−1,yt)\n=\nT∑\nt=1\nEyt∼Gθ(yt|Y1:t−1)[∇θlogGθ(yt|Y1:t−1) ·QGθ\nDφ(Y1:t−1,yt)],\nwhere Y1:t−1 is the observed intermediate state sampled\nfrom Gθ. Since the expectation E[·] can be approximated by\nsampling methods, we then update the generator’s parame-\nters as:\nθ←θ+ αh∇θJ(θ), (8)\nwhere αh ∈R+ denotes the corresponding learning rate\nat h-th step. Also the advanced gradient algorithms such as\nAdam and RMSprop can be adopted here.\nIn summary, Algorithm 1 shows full details of the pro-\nposed SeqGAN. At the beginning of the training, we use the\nmaximum likelihood estimation (MLE) to pre-train Gθ onAlgorithm 1Sequence Generative Adversarial Nets\nRequire: generator policy Gθ; roll-out policy Gβ; discriminator\nDφ; a sequence dataset S= {X1:T}\n1: Initialize Gθ, Dφ with random weights θ,φ.\n2: Pre-train Gθ using MLE on S\n3: β ←θ\n4: Generate negative samples using Gθ for training Dφ\n5: Pre-train Dφ via minimizing the cross entropy\n6: repeat\n7: for g-steps do\n8: Generate a sequence Y1:T = (y1,...,y T) ∼Gθ\n9: for tin 1 :T do\n10: Compute Q(a= yt; s= Y1:t−1) by Eq. (4)\n11: end for\n12: Update generator parameters via policy gradient Eq. (8)\n13: end for\n14: for d-steps do\n15: Use current Gθ to generate negative examples and com-\nbine with given positive examples S\n16: Train discriminator Dφ for kepochs by Eq. (5)\n17: end for\n18: β ←θ\n19: until SeqGAN converges\ntraining set S. We found the supervised signal from the pre-\ntrained discriminator is informative to help adjust the gener-\nator efﬁciently.\nAfter the pre-training, the generator and discriminator are\ntrained alternatively. As the generator gets progressed via\ntraining on g-steps updates, the discriminator needs to be re-\ntrained periodically to keeps a good pace with the generator.\nWhen training the discriminator, positive examples are from\nthe given dataset S, whereas negative examples are gener-\nated from our generator. In order to keep the balance, the\nnumber of negative examples we generate for each d-step is\nthe same as the positive examples. And to reduce the vari-\nability of the estimation, we use different sets of negative\nsamples combined with positive ones, which is similar to\nbootstrapping (Quinlan 1996).\nThe Generative Model for Sequences\nWe use recurrent neural networks (RNNs) (Hochreiter and\nSchmidhuber 1997) as the generative model. An RNN\nmaps the input embedding representations x1,..., xT of\nthe sequence x1,...,x T into a sequence of hidden states\nh1,..., hT by using the update function grecursively.\nht = g(ht−1,xt) (9)\nMoreover, a softmax output layer zmaps the hidden states\ninto the output token distribution\np(yt|x1,...,x t) =z(ht) =softmax(c + V ht), (10)\nwhere the parameters are a bias vector cand a weight ma-\ntrix V. To deal with the common vanishing and exploding\ngradient problem (Goodfellow, Bengio, and Courville 2016)\nof the backpropagation through time, we leverage the Long\nShort-Term Memory (LSTM) cells (Hochreiter and Schmid-\nhuber 1997) to implement the update function g in Eq. (9).\nIt is worth noticing that most of the RNN variants, such as\nthe gated recurrent unit (GRU) (Cho et al. 2014) and soft at-\ntention mechanism (Bahdanau, Cho, and Bengio 2014), can\nbe used as a generator in SeqGAN.\nThe Discriminative Model for Sequences\nDeep discriminative models such as deep neural network\n(DNN) (Vesel`y et al. 2013), convolutional neural network\n(CNN) (Kim 2014) and recurrent convolutional neural net-\nwork (RCNN) (Lai et al. 2015) have shown a high perfor-\nmance in complicated sequence classiﬁcation tasks. In this\npaper, we choose the CNN as our discriminator as CNN has\nrecently been shown of great effectiveness in text (token se-\nquence) classiﬁcation (Zhang and LeCun 2015). Most dis-\ncriminative models can only perform classiﬁcation well for\nan entire sequence rather than the unﬁnished one. In this pa-\nper, we also focus on the situation where the discriminator\npredicts the probability that a ﬁnished sequence is real.1\nWe ﬁrst represent an input sequence x1,...,x T as:\nE1:T = x1 ⊕x2 ⊕... ⊕xT, (11)\nwhere xt ∈ Rk is the k-dimensional token embedding\nand ⊕is the concatenation operator to build the matrix\nE1:T ∈RT×k. Then a kernel w ∈Rl×k applies a convo-\nlutional operation to a window size of lwords to produce a\nnew feature map:\nci = ρ(w ⊗Ei:i+l−1 + b), (12)\nwhere ⊗operator is the summation of elementwise pro-\nduction, b is a bias term and ρ is a non-linear function.\nWe can use various numbers of kernels with different win-\ndow sizes to extract different features. Finally we apply\na max-over-time pooling operation over the feature maps\n˜c= max{c1,...,c T−l+1}.\nTo enhance the performance, we also add the highway ar-\nchitecture (Srivastava, Greff, and Schmidhuber 2015) based\non the pooled feature maps. Finally, a fully connected layer\nwith sigmoid activation is used to output the probability that\nthe input sequence is real. The optimization target is to min-\nimize the cross entropy between the ground truth label and\nthe predicted probability as formulated in Eq. (5).\nDetailed implementations of the generative and discrimi-\nnative models are provided in the appendix.\nSynthetic Data Experiments\nTo test the efﬁcacy and add our understanding of SeqGAN,\nwe conduct a simulated test with synthetic data2. To simulate\nthe real-world structured sequences, we consider a language\nmodel to capture the dependency of the tokens. We use a\nrandomly initialized LSTM as the true model, aka, the ora-\ncle, to generate the real data distribution p(xt|x1,...,x t−1)\nfor the following experiments.\n1In our work, the generated sequence has a ﬁxed length T, but\nnote that CNN is also capable of the variable-length sequence dis-\ncrimination with the max-over-time pooling technique (Kim 2014).\n2 Experiment code: https://github.com/LantaoYu/SeqGANEvaluation Metric\nThe beneﬁt of having such oracle is that ﬁrstly, it provides\nthe training dataset and secondly evaluates the exact perfor-\nmance of the generative models, which will not be possi-\nble with real data. We know that MLE is trying to mini-\nmize the cross-entropy between the true data distribution p\nand our approximation q, i.e. −Ex∼plog q(x). However, the\nmost accurate way of evaluating generative models is that\nwe draw some samples from it and let human observers re-\nview them based on their prior knowledge. We assume that\nthe human observer has learned an accurate model of the\nnatural distribution phuman(x). Then in order to increase the\nchance of passing Turing Test, we actually need to min-\nimize the exact opposite average negative log-likelihood\n−Ex∼qlog phuman(x) (Husz´ar 2015), with the role of pand\nqexchanged. In our synthetic data experiments, we can con-\nsider the oracle to be the human observer for real-world\nproblems, thus a perfect evaluation metric should be\nNLLoracle = −EY1:T∼Gθ\n[ T∑\nt=1\nlog Goracle(yt|Y1:t−1)\n]\n, (13)\nwhere Gθ and Goracle denote our generative model and the\noracle respectively.\nAt the test stage, we use Gθ to generate 100,000 se-\nquence samples and calculate NLL oracle for each sample by\nGoracle and their average score. Also signiﬁcance tests are\nperformed to compare the statistical properties of the gener-\nation performance between the baselines and SeqGAN.\nTraining Setting\nTo set up the synthetic data experiments, we ﬁrst initialize\nthe parameters of an LSTM network following the normal\ndistribution N(0,1) as the oracle describing the real data\ndistribution Goracle(xt|x1,...,x t−1). Then we use it to gen-\nerate 10,000 sequences of length 20 as the training set Sfor\nthe generative models.\nIn SeqGAN algorithm, the training set for the discrimina-\ntor is comprised by the generated examples with the label\n0 and the instances from Swith the label 1. For different\ntasks, one should design speciﬁc structure for the convolu-\ntional layer and in our synthetic data experiments, the kernel\nsize is from 1 to T and the number of each kernel size is be-\ntween 100 to 2003. Dropout (Srivastava et al. 2014) and L2\nregularization are used to avoid over-ﬁtting.\nFour generative models are compared with SeqGAN. The\nﬁrst model is a random token generation. The second one is\nthe MLE trained LSTMGθ. The third one is scheduled sam-\npling (Bengio et al. 2015). The fourth one is the Policy Gra-\ndient with BLEU (PG-BLEU). In the scheduled sampling,\nthe training process gradually changes from a fully guided\nscheme feeding the true previous tokens into LSTM, towards\na less guided scheme which mostly feeds the LSTM with its\ngenerated tokens. A curriculum rate ωis used to control the\nprobability of replacing the true tokens with the generated\nones. To get a good and stable performance, we decrease ω\nby 0.002 for every training epoch. In the PG-BLEU algo-\nrithm, we use BLEU, a metric measuring the similarity be-\ntween a generated sequence and references (training data),\nto score the ﬁnished samples from Monte Carlo search.\n3Implementation details are in the appendix.\nTable 1: Sequence generation performance comparison. The\np-value is between SeqGAN and the baseline from T-test.\nAlgorithm Random MLE SS PG-BLEU SeqGAN\nNLL 10.310 9.038 8.985 8.946 8.736\np-value <10−6 <10−6 <10−6 <10−6\n0 50 100 150 200 250\nEpochs\n8.6\n8.8\n9.0\n9.2\n9.4\n9.6\n9.8\n10.0NLL/uni00A0by/uni00A0oracle\nLearning/uni00A0curve\nSeqGAN\nMLE\nSchedule/uni00A0Sampling\nPG/uni00ADBLEU\nFigure 2: Negative log-likelihood convergence w.r.t. the\ntraining epochs. The vertical dashed line represents the end\nof pre-training for SeqGAN, SS and PG-BLEU.\nResults\nThe NLLoracle performance of generating sequences from the\ncompared policies is provided in Table 1. Since the evalua-\ntion metric is fundamentally instructive, we can see the im-\npact of SeqGAN, which outperforms other baselines signif-\nicantly. A signiﬁcance T-test on the NLLoracle score distribu-\ntion of the generated sequences from the compared models\nis also performed, which demonstrates the signiﬁcant im-\nprovement of SeqGAN over all compared models.\nThe learning curves shown in Figure 4 illustrate the su-\nperiority of SeqGAN explicitly. After about 150 training\nepochs, both the maximum likelihood estimation and the\nschedule sampling methods converge to a relatively high\nNLLoracle score, whereas SeqGAN can improve the limit of\nthe generator with the same structure as the baselines sig-\nniﬁcantly. This indicates the prospect of applying adversar-\nial training strategies to discrete sequence generative mod-\nels to breakthrough the limitations of MLE. Additionally,\nSeqGAN outperforms PG-BLEU, which means the discrim-\ninative signal in GAN is more general and effective than a\npredeﬁned score (e.g. BLEU) to guide the generative policy\nto capture the underlying distribution of the sequence data.\nDiscussion\nIn our synthetic data experiments, we ﬁnd that the stability\nof SeqGAN depends on the training strategy. More speciﬁ-\ncally, the g-steps, d-steps and k parameters in Algorithm 1\nhave a large effect on the convergence and performance of\nSeqGAN. Figure 3 shows the effect of these parameters. In\nFigure 3(a), the g-steps is much larger than the d-steps and\nepoch number k, which means we train the generator for\nmany times until we update the discriminator. This strategy\nleads to a fast convergence but as the generator improves\nquickly, the discriminator cannot get fully trained and thus\nwill provide a misleading signal gradually. In Figure 3(b),\nwith more discriminator training epochs, the unstable train-\ning process is alleviated. In Figure 3(c), we train the genera-\ntor for only one epoch and then before the discriminator gets0 50 100 150 200\nEpochs\n8.70\n8.949.00\n9.50\n10.00NLL/uni00A0by/uni00A0oracle\nSeqGAN\n(a) g-steps=100, d-steps=1, k=10\n0 50 100 150 200\nEpochs\n8.70\n8.89\n9.00\n9.50\n10.00NLL/uni00A0by/uni00A0oracle\nSeqGAN (b) g-steps=30, d-steps=1, k=30\n0 50 100 150 200\nEpochs\n8.70\n8.81\n9.00\n9.50\n10.00NLL/uni00A0by/uni00A0oracle\nSeqGAN\n(c) g-steps=1, d-steps=1, k=10\n0 50 100 150 200 250\nEpochs\n8.73\n9.00\n9.50\n10.00NLL/uni00A0by/uni00A0oracle\nSeqGAN (d) g-steps=1, d-steps=5, k=3\nFigure 3: Negative log-likelihood convergence performance\nof SeqGAN with different training strategies. The vertical\ndashed line represents the beginning of adversarial training.\nfooled, we update it immediately based on the more realistic\nnegative examples. In such a case, SeqGAN learns stably.\nThe d-steps in all three training strategies described above\nis set to 1, which means we only generate one set of nega-\ntive examples with the same number as the given dataset,\nand then train the discriminator on it for various k epochs.\nBut actually we can utilize the potentially unlimited num-\nber of negative examples to improve the discriminator. This\ntrick can be considered as a type of bootstrapping, where\nwe combine the ﬁxed positive examples with different neg-\native examples to obtain multiple training sets. Figure 3(d)\nshows this technique can improve the overall performance\nwith good stability, since the discriminator is shown more\nnegative examples and each time the positive examples are\nemphasized, which will lead to a more comprehensive guid-\nance for training generator. This is in line with the theo-\nrem in (Goodfellow and others 2014). When analyzing the\nconvergence of generative adversarial nets, an important as-\nsumption is that the discriminator is allowed to reach its op-\ntimum given G. Only if the discriminator is capable of dif-\nferentiating real data from unnatural data consistently, the\nsupervised signal from it can be meaningful and the whole\nadversarial training process can be stable and effective.\nReal-world Scenarios\nTo complement the previous experiments, we also test Se-\nqGAN on several real-world tasks, i.e. poem composition,\nspeech language generation and music generation.\nText Generation\nFor text generation scenarios, we apply the proposed Seq-\nGAN to generate Chinese poems and Barack Obama polit-\nical speeches. In the poem composition task, we use a cor-\npus4 of 16,394 Chinese quatrains, each containing four lines\n4http://homepages.inf.ed.ac.uk/mlap/Data/EMNLP14/\nTable 2: Chinese poem generation performance comparison.\nAlgorithm Human score p-value BLEU-2 p-value\nMLE 0.4165 0.0034 0.6670 <10−6\nSeqGAN 0.5356 0.7389\nReal data 0.6011 0.746\nTable 3: Obama political speech generation performance.\nAlgorithm BLEU-3 p-value BLEU-4 p-value\nMLE 0.519 <10−6 0.416 0.00014SeqGAN 0.556 0.427\nTable 4: Music generation performance comparison.\nAlgorithm BLEU-4 p-value MSE p-value\nMLE 0.9210 <10−6 22.38 0.00034SeqGAN 0.9406 20.62\nof twenty characters in total. To focus on a fully automatic\nsolution and stay general, we did not use any prior knowl-\nedge of special structure rules in Chinese poems such as\nspeciﬁc phonological rules. In the Obama political speech\ngeneration task, we use a corpus 5, which is a collection of\n11,092 paragraphs from Obama’s political speeches.\nWe use BLEU score as an evaluation metric to measure\nthe similarity degree between the generated texts and the\nhuman-created texts. BLEU is originally designed to auto-\nmatically judge the machine translation quality (Papineni et\nal. 2002). The key point is to compare the similarity between\nthe results created by machine and the references provided\nby human. Speciﬁcally, for poem evaluation, we set n-gram\nto be 2 (BLEU-2) since most words (dependency) in classi-\ncal Chinese poems consist of one or two characters (Yi, Li,\nand Sun 2016) and for the similar reason, we use BLEU-3\nand BLEU-4 to evaluate Obama speech generation perfor-\nmance. In our work, we use the whole test set as the refer-\nences instead of trying to ﬁnd some references for the fol-\nlowing line given the previous line (He, Zhou, and Jiang\n2012). The reason is in generation tasks we only provide\nsome positive examples and then let the model catch the pat-\nterns of them and generate new ones. In addition to BLEU,\nwe also choose poem generation as a case for human judge-\nment since a poem is a creative text construction and human\nevaluation is ideal. Speciﬁcally, we mix the 20 real poems\nand 20 each generated from SeqGAN and MLE. Then 70\nexperts on Chinese poems are invited to judge whether each\nof the 60 poem is created by human or machines. Once re-\ngarded to be real, it gets +1 score, otherwise 0. Finally, the\naverage score for each algorithm is calculated.\nThe experiment results are shown in Tables 2 and 3, from\nwhich we can see the signiﬁcant advantage of SeqGAN over\nthe MLE in text generation. Particularly, for poem composi-\ntion, SeqGAN performs comparably to real human data.\nMusic Generation\nFor music composition, we use Nottingham 6 dataset as our\ntraining data, which is a collection of 695 music of folk tunes\nin midi ﬁle format. We study the solo track of each music. In\nour work, we use 88 numbers to represent 88 pitches, which\n5https://github.com/samim23/obama-rnn\n6http://www.iro.umontreal.ca/˜lisa/deep/datacorrespond to the 88 keys on the piano. With the pitch sam-\npling for every 0.4s 7, we transform the midi ﬁles into se-\nquences of numbers from 1 to 88 with the length 32.\nTo model the ﬁtness of the discrete piano key patterns,\nBLEU is used as the evaluation metric. To model the ﬁtness\nof the continuous pitch data patterns, the mean squared error\n(MSE) (Manaris et al. 2007) is used for evaluation.\nFrom Table 4, we see that SeqGAN outperforms the MLE\nsigniﬁcantly in both metrics in the music generation task.\nConclusion\nIn this paper, we proposed a sequence generation method,\nSeqGAN, to effectively train generative adversarial nets for\nstructured sequences generation via policy gradient. To our\nbest knowledge, this is the ﬁrst work extending GANs to\ngenerate sequences of discrete tokens. In our synthetic data\nexperiments, we used an oracle evaluation mechanism to\nexplicitly illustrate the superiority of SeqGAN over strong\nbaselines. For three real-world scenarios, i.e., poems, speech\nlanguage and music generation, SeqGAN showed excellent\nperformance on generating the creative sequences. We also\nperformed a set of experiments to investigate the robustness\nand stability of training SeqGAN. For future work, we plan\nto build Monte Carlo tree search and value network (Silver et\nal. 2016) to improve action decision making for large scale\ndata and in the case of longer-term planning.\nAcknowledgments\nWe sincerely thank Tianxing He for many helpful discus-\nsions and comments on the manuscript.\nReferences\n[Bachman and Precup 2015] Bachman, P., and Precup, D. 2015.\nData generation as sequential decision making. In NIPS, 3249–\n3257.\n[Bahdanau et al. 2016] Bahdanau, D.; Brakel, P.; Xu, K.; et al.\n2016. An actor-critic algorithm for sequence prediction.\narXiv:1607.07086.\n[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.; and\nBengio, Y . 2014. Neural machine translation by jointly learning to\nalign and translate. arXiv:1409.0473.\n[Bengio et al. 2013] Bengio, Y .; Yao, L.; Alain, G.; and Vincent, P.\n2013. Generalized denoising auto-encoders as generative models.\nIn NIPS, 899–907.\n[Bengio et al. 2015] Bengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer,\nN. 2015. Scheduled sampling for sequence prediction with recur-\nrent neural networks. In NIPS, 1171–1179.\n[Browne et al. 2012] Browne, C. B.; Powley, E.; Whitehouse, D.;\nLucas, S. M.; et al. 2012. A survey of monte carlo tree search\nmethods. IEEE TCIAIG4(1):1–43.\n[Cho et al. 2014] Cho, K.; Van Merri ¨enboer, B.; Gulcehre, C.; et al.\n2014. Learning phrase representations using RNN encoder-\ndecoder for statistical machine translation. EMNLP.\n[Denton et al. 2015] Denton, E. L.; Chintala, S.; Fergus, R.; et al.\n2015. Deep generative image models using a laplacian pyramid of\nadversarial networks. In NIPS, 1486–1494.\n7http://deeplearning.net/tutorial/rnnrbm.html\n[Glynn 1990] Glynn, P. W. 1990. Likelihood ratio gradient es-\ntimation for stochastic systems. Communications of the ACM\n33(10):75–84.\n[Goodfellow and others 2014] Goodfellow, I., et al. 2014. Genera-\ntive adversarial nets. In NIPS, 2672–2680.\n[Goodfellow, Bengio, and Courville 2016] Goodfellow, I.; Bengio,\nY .; and Courville, A. 2016. Deep learning. 2015.\n[Goodfellow 2016] Goodfellow, I. 2016. Generative adversarial\nnetworks for text. http://goo.gl/Wg9DR7.\n[Graves 2013] Graves, A. 2013. Generating sequences with recur-\nrent neural networks. arXiv:1308.0850.\n[He, Zhou, and Jiang 2012] He, J.; Zhou, M.; and Jiang, L. 2012.\nGenerating chinese classical poems with statistical machine trans-\nlation models. In AAAI.\n[Hingston 2009] Hingston, P. 2009. A turing test for computer\ngame bots. IEEE TCIAIG1(3):169–186.\n[Hinton, Osindero, and Teh 2006] Hinton, G. E.; Osindero, S.; and\nTeh, Y .-W. 2006. A fast learning algorithm for deep belief nets.\nNeural computation18(7):1527–1554.\n[Hochreiter and Schmidhuber 1997] Hochreiter, S., and Schmidhu-\nber, J. 1997. Long short-term memory. Neural computation\n9(8):1735–1780.\n[Husz´ar 2015] Husz ´ar, F. 2015. How (not) to train your\ngenerative model: Scheduled sampling, likelihood, adversary?\narXiv:1511.05101.\n[Kim 2014] Kim, Y . 2014. Convolutional neural networks for sen-\ntence classiﬁcation. arXiv:1408.5882.\n[Kingma and Welling 2014] Kingma, D. P., and Welling, M. 2014.\nAuto-encoding variational bayes. ICLR.\n[Lai et al. 2015] Lai, S.; Xu, L.; Liu, K.; and Zhao, J. 2015. Recur-\nrent convolutional neural networks for text classiﬁcation. In AAAI,\n2267–2273.\n[Manaris et al. 2007] Manaris, B.; Roos, P.; Machado, P.; et al.\n2007. A corpus-based hybrid approach to music analysis and com-\nposition. In NCAI, volume 22, 839.\n[Papineni et al. 2002] Papineni, K.; Roukos, S.; Ward, T.; and Zhu,\nW.-J. 2002. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, 311–318.\n[Quinlan 1996] Quinlan, J. R. 1996. Bagging, boosting, and c4. 5.\nIn AAAI/IAAI, Vol. 1, 725–730.\n[Salakhutdinov 2009] Salakhutdinov, R. 2009. Learning deep gen-\nerative models. Ph.D. Dissertation, University of Toronto.\n[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.; Guez,\nA.; Sifre, L.; et al. 2016. Mastering the game of go with deep\nneural networks and tree search. Nature 529(7587):484–489.\n[Srivastava et al. 2014] Srivastava, N.; Hinton, G. E.; Krizhevsky,\nA.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: a\nsimple way to prevent neural networks from overﬁtting. JMLR\n15(1):1929–1958.\n[Srivastava, Greff, and Schmidhuber 2015] Srivastava, R. K.; Gr-\neff, K.; and Schmidhuber, J. 2015. Highway networks.\narXiv:1505.00387.\n[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.; and\nLe, Q. V . 2014. Sequence to sequence learning with neural net-\nworks. In NIPS, 3104–3112.\n[Sutton et al. 1999] Sutton, R. S.; McAllester, D. A.; Singh, S. P.;\nMansour, Y .; et al. 1999. Policy gradient methods for reinforce-\nment learning with function approximation. In NIPS, 1057–1063.[Vesel`y et al. 2013] Vesel `y, K.; Ghoshal, A.; Burget, L.; and Povey,\nD. 2013. Sequence-discriminative training of deep neural net-\nworks. In INTERSPEECH, 2345–2349.\n[Wen et al. 2015] Wen, T.-H.; Gasic, M.; Mrksic, N.; Su, P.-H.;\nVandyke, D.; and Young, S. 2015. Semantically conditioned\nLSTM-based natural language generation for spoken dialogue sys-\ntems. arXiv:1508.01745.\n[Williams 1992] Williams, R. J. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforcement learning.\nMachine learning8(3-4):229–256.\n[Yi, Li, and Sun 2016] Yi, X.; Li, R.; and Sun, M. 2016. Gen-\nerating chinese classical poems with RNN encoder-decoder.\narXiv:1604.01537.\n[Zhang and Lapata 2014] Zhang, X., and Lapata, M. 2014. Chinese\npoetry generation with recurrent neural networks. InEMNLP, 670–\n680.\n[Zhang and LeCun 2015] Zhang, X., and LeCun, Y . 2015. Text\nunderstanding from scratch. arXiv:1502.01710.Appendix\nIn Section 1, we present the step-by-step derivation of Eq. (6) in the paper. In Section 2, the detailed realization of the\ngenerative model and the discriminative model is discussed, including the model parameter settings. In Section 3, an interesting\nablation study is provided, which is a supplementary to the discussions of the synthetic data experiments.\nProof for Eq. (6)\nFor readability, we provide the detailed derivation of Eq. (6) here by following (Sutton et al. 1999).\nAs mentioned in SEQUENCE GENERATIVE ADVERSARIAL NETS section, the state transition is deterministic after an action\nhas been chosen, i.e. δa\ns,s′ = 1for the next state s′= Y1:t if the current state s= Y1:t−1 and the action a= yt; for other next\nstates s′′, δa\ns,s′′ = 0. In addition, the intermediate reward Ra\ns is 0. We re-write the action value and state value as follows:\nQGθ(s= Y1:t−1,a = yt) =Ra\ns +\n∑\ns′∈S\nδa\nss′VGθ(s′) =VGθ(Y1:t) (14)\nVGθ(s= Y1:t−1) =\n∑\nyt∈Y\nGθ(yt|Y1:t−1) ·QGθ(Y1:t−1,yt) (15)\nFor the start state s0, the value is calculated as\nVGθ(s0) =E[RT|s0,θ] (16)\n=\n∑\ny1∈Y\nGθ(y1|s0) ·QGθ(s0,y1),\nwhich is the objective function J(θ) to maximize in Eq. (1) of the paper.\nThen we can obtain the gradient of the objective function, deﬁned in Eq. (1), w.r.t. the generator’s parametersθ:\n∇θJ(θ)\n= ∇θVGθ(s0) =∇θ[\n∑\ny1∈Y\nGθ(y1|s0) ·QGθ(s0,y1)]\n=\n∑\ny1∈Y\n[∇θGθ(y1|s0) ·QGθ(s0,y1) +Gθ(y1|s0) ·∇θQGθ(s0,y1)]\n=\n∑\ny1∈Y\n[∇θGθ(y1|s0) ·QGθ(s0,y1) +Gθ(y1|s0) ·∇θVGθ(Y1:1)]\n=\n∑\ny1∈Y\n∇θGθ(y1|s0) ·QGθ(s0,y1) +\n∑\ny1∈Y\nGθ(y1|s0)∇θ[\n∑\ny2∈Y\nGθ(y2|Y1:1)QGθ(Y1:1,y2)]\n=\n∑\ny1∈Y\n∇θGθ(y1|s0) ·QGθ(s0,y1) +\n∑\ny1∈Y\nGθ(y1|s0)\n∑\ny2∈Y\n[∇θGθ(y2|Y1:1) ·QGθ(Y1:1,y2)\n+ Gθ(y2|Y1:1)∇θQGθ(Y1:1,y2)]\n=\n∑\ny1∈Y\n∇θGθ(y1|s0) ·QGθ(s0,y1) +\n∑\nY1:1\nP(Y1:1|s0; Gθ)\n∑\ny2∈Y\n∇θGθ(y2|Y1:1) ·QGθ(Y1:1,y2)\n+\n∑\nY1:2\nP(Y1:2|s0; Gθ)∇θVGθ(Y1:2)\n=\nT∑\nt=1\n∑\nY1:t−1\nP(Y1:t−1|s0; Gθ)\n∑\nyt∈Y\n∇θGθ(yt|Y1:t−1) ·QGθ(Y1:t−1,yt)\n=\nT∑\nt=1\nEY1:t−1∼Gθ[\n∑\nyt∈Y\n∇θGθ(yt|Y1:t−1) ·QGθ(Y1:t−1,yt)], (17)\nwhich is the result in Eq. (6) of the paper.\nModel Implementations\nIn this section, we present a full version of the discussed generative model and discriminative model in our paper submission.The Generative Model for SequencesWe use recurrent neural networks (RNNs) (Hochreiter and Schmidhuber 1997) as the\ngenerative model. An RNN maps the input embedding representations x1,..., xT of the sequence x1,...,x T into a sequence\nof hidden states h1,..., hT by using the update function grecursively.\nht = g(ht−1,xt) (18)\nMoreover, a softmax output layer zmaps the hidden states into the output token distribution\np(yt|x1,...,x t) =z(ht) =softmax(c+ Vht), (19)\nwhere the parameters are a bias vector cand a weight matrix V.\nThe vanishing and exploding gradient problem in backpropagation through time (BPTT) issues a challenge of learning long-\nterm dependencies to recurrent neural network (Goodfellow, Bengio, and Courville 2016). To address such problems, gated\nRNNs have been designed based on the basic idea of creating paths through time that have derivatives that neither vanish nor\nexplode. Among various gated RNNs, we choose the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber 1997)\nto be our generative networks with the update equations:\nft = σ(Wf ·[ht−1,xt] +bf),\nit = σ(Wi ·[ht−1,xt] +bi),\not = σ(Wo ·[ht−1,xt] +bo),\nst = ft ⊙st−1 + it ⊙tanh(Ws ·[ht−1,xt] +bs),\nht = ot ⊙tanh(st),\n(20)\nwhere [h,x] is the vector concatenation and ⊙is the elementwise product.\nFor simplicity, we use the standard LSTM as the generator, while it is worth noticing that most of the RNN variants, such as\nthe gated recurrent unit (GRU) (Cho et al. 2014) and soft attention mechanism (Bahdanau, Cho, and Bengio 2014), can be used\nas a generator in SeqGAN.\nThe standard way of training an RNN Gθ is the maximum likelihood estimation (MLE), which involves minimizing the neg-\native log-likelihood −∑T\nt=1 log Gθ(yt = xt|{x1,...,x t−1}) for a generated sequence (y1,...,y T) given input (x1,...,x T).\nHowever, when applying MLE to generative models, there is a discrepancy between training and generating (Bengio et al. 2015;\nHusz´ar 2015), which motivates our work.\nThe Discriminative Model for SequencesDeep discriminative models such as deep neural network (DNN) (Vesel `y et al.\n2013), convolutional neural network (CNN) (Kim 2014) and recurrent convolutional neural network (RCNN) (Lai et al. 2015)\nhave shown a high performance in complicated sequence classiﬁcation tasks. In this paper, we choose the CNN as our dis-\ncriminator as CNN has recently been shown of great effectiveness in text (token sequence) classiﬁcation (Zhang and LeCun\n2015).\nAs far as we know, except for some speciﬁc tasks, most discriminative models can only perform classiﬁcation well for a whole\nsequence rather than the unﬁnished one. In case of some speciﬁc tasks, one may design a classiﬁer to provide intermediate\nreward signal to enhance the performance of our framework. But to make it more general, we focus on the situation where\ndiscriminator can only provide ﬁnal reward, i.e., the probability that a ﬁnished sequence was real.\nWe ﬁrst represent an input sequence x1,...,x T as:\nE1:T = x1 ⊕x2 ⊕... ⊕xT, (21)\nwhere xt ∈Rk is the k-dimensional token embedding and ⊕is the vertical concatenation operator to build the matrix E1:T ∈\nRT×k. Then a kernel w∈Rl×k applies a convolutional operation to a window size of lwords to produce a new feature map:\nci = ρ(w⊗Ei:i+l−1 + b), (22)\nwhere ⊗operator is the summation of elementwise production, b is a bias term and ρ is a non-linear function. We can use\nvarious numbers of kernels with different window sizes to extract different features. Speciﬁcally, a kernel wwith window size\nlapplied to the concatenated embeddings of input sequence will produce a feature map\nc= [c1,...,c T−l+1]. (23)\nFinally we apply a max-over-time pooling operation over the feature map ˜c = max{c}and pass all pooled features from\ndifferent kernels to a fully connected softmax layer to get the probability that a given sequence is real.\nWe perform an empirical experiment to choose the kernel window sizes and numbers as shown in Table 5. For different tasks,\none should design speciﬁc structures for the discriminator.\nTo enhance the performance, we also add the highway architecture (Srivastava, Greff, and Schmidhuber 2015) before the\nﬁnal fully connected layer:\nτ = σ(WT ·˜c+ bT),\n˜C= τ·H(˜c,WH) + (1 −τ) ·˜c, (24)Table 5: Convolutional layer structures.\nSequence length (window size, kernel numbers)\n20\n(1, 100),(2, 200),(3, 200),(4, 200),(5, 200)\n(6, 100),(7, 100),(8, 100),(9, 100),(10, 100)\n(15, 160),(20, 160)\n32\n(1, 100),(2, 200),(3, 200),(4, 200),(5, 200)\n(6, 100),(7, 100),(8, 100),(9, 100),(10, 100)\n(16, 160),(24, 160),(32, 160)\nwhere WT, bT and WH are highway layer weights, Hdenotes an afﬁne transform followed by a non-linear activation function\nsuch as a rectiﬁed linear unit (ReLU) and τ is the “transform gate” with the same dimensionality as H(˜c,WH) and ˜c. Finally,\nwe apply a sigmoid transformation to get the probability that a given sequence is real:\nˆy= σ(Wo · ˜C+ bo) (25)\nwhere Wo and bo is the output layer weight and bias.\nWhen optimizing discriminative models, supervised training is applied to minimize the cross entropy, which is widely used\nas the objective function for classiﬁcation and prediction tasks:\nL(y,ˆy) =−ylog ˆy−(1 −y) log(1−ˆy), (26)\nwhere yis the ground truth label of the input sequence and ˆyis the predicted probability from the discriminative models.\nMore Ablation Study\n0 50 100 150 200 250 300 350 400\nEpochs\n8.6\n8.8\n9.0\n9.2\n9.4\n9.6\n9.8\n10.0\nNLL/uni00A0by/uni00A0oracle\nSeqGAN/uni00A0with/uni00A0insufficient/uni00A0pre/uni00ADtraining\nSeqGAN/uni00A0with/uni00A0sufficient/uni00A0pre/uni00ADtraining\nFigure 4: Negative log-likelihood performance with different pre-training epochs before the adversarial training. The vertical\ndashed lines represent the start of adversarial training.\nIn the D ISCUSSION subsection of S YNTHETIC DATA EXPERIMENTS section of our paper, we discussed the ablation study\nof three hyperparameters of SeqGAN, i.e., g-steps, d-steps and kepoch number. Here we provide another ablation study which\nis instructive for the better training of SeqGAN.\nAs described in our paper, we start the adversarial training process after the convergence of MLE supervised pre-training.\nHere we further conduct experiments to investigate the performance of SeqGAN when the supervised pre-training is insufﬁcient.\nAs shown in Figure 4, if we pre-train the generative model with conventional MLE methods for only 20 epochs, which is far\nfrom convergence, then the adversarial training process improves the generator quite slowly and unstably. The reason is that\nin SeqGAN, the discriminative model provides reward guidance when training the generator and if the generator acts almost\nrandomly, the discriminator will identify the generated sequence to be unreal with high conﬁdence and almost every action\nthe generator takes receives a low (uniﬁed) reward, which does not guide the generator towards a good improvement direction,\nresulting in an ineffective training procedure. This indicates that in order to apply adversarial training strategies to sequence\ngenerative models, a sufﬁcient pre-training is necessary."
    }
}