{
    "title": "Multi-Task Learning Using Uncertainty to Weigh Losses",
    "content": {
        "page_content": "Multi-Task Learning Using Uncertainty to Weigh Losses\nfor Scene Geometry and Semantics\nAlex Kendall\nUniversity of Cambridge\nagk34@cam.ac.uk\nYarin Gal\nUniversity of Oxford\nyarin@cs.ox.ac.uk\nRoberto Cipolla\nUniversity of Cambridge\nrc10001@cam.ac.uk\nAbstract\nNumerous deep learning applications beneﬁt from multi-\ntask learning with multiple regression and classiﬁcation ob-\njectives. In this paper we make the observation that the\nperformance of such systems is strongly dependent on the\nrelative weighting between each task’s loss. Tuning these\nweights by hand is a difﬁcult and expensive process, mak-\ning multi-task learning prohibitive in practice. We pro-\npose a principled approach to multi-task deep learning\nwhich weighs multiple loss functions by considering the ho-\nmoscedastic uncertainty of each task. This allows us to si-\nmultaneously learn various quantities with different units\nor scales in both classiﬁcation and regression settings. We\ndemonstrate our model learning per-pixel depth regression,\nsemantic and instance segmentation from a monocular in-\nput image. Perhaps surprisingly, we show our model can\nlearn multi-task weightings and outperform separate mod-\nels trained individually on each task.\n1. Introduction\nMulti-task learning aims to improve learning efﬁciency\nand prediction accuracy by learning multiple objectives\nfrom a shared representation [7]. Multi-task learning is\nprevalent in many applications of machine learning – from\ncomputer vision [27] to natural language processing [11] to\nspeech recognition [23].\nWe explore multi-task learning within the setting of vi-\nsual scene understanding in computer vision. Scene under-\nstanding algorithms must understand both the geometry and\nsemantics of the scene at the same time. This forms an in-\nteresting multi-task learning problem because scene under-\nstanding involves joint learning of various regression and\nclassiﬁcation tasks with different units and scales. Multi-\ntask learning of visual scene understanding is of crucial\nimportance in systems where long computation run-time is\nprohibitive, such as the ones used in robotics. Combining\nall tasks into a single model reduces computation and allows\nthese systems to run in real-time.\nPrior approaches to simultaneously learning multiple\ntasks use a na ¨ıve weighted sum of losses, where the loss\nweights are uniform, or manually tuned [38, 27, 15]. How-\never, we show that performance is highly dependent on an\nappropriate choice of weighting between each task’s loss.\nSearching for an optimal weighting is prohibitively expen-\nsive and difﬁcult to resolve with manual tuning. We observe\nthat the optimal weighting of each task is dependent on the\nmeasurement scale (e.g. meters, centimetres or millimetres)\nand ultimately the magnitude of the task’s noise.\nIn this work we propose a principled way of combining\nmultiple loss functions to simultaneously learn multiple ob-\njectives using homoscedastic uncertainty. We interpret ho-\nmoscedastic uncertainty as task-dependent weighting and\nshow how to derive a principled multi-task loss function\nwhich can learn to balance various regression and classiﬁca-\ntion losses. Our method can learn to balance these weight-\nings optimally, resulting in superior performance, compared\nwith learning each task individually.\nSpeciﬁcally, we demonstrate our method in learning\nscene geometry and semantics with three tasks. Firstly, we\nlearn to classify objects at a pixel level, also known as se-\nmantic segmentation [32, 3, 42, 8, 45]. Secondly, our model\nperforms instance segmentation, which is the harder task of\nsegmenting separate masks for each individual object in an\nimage (for example, a separate, precise mask for each in-\ndividual car on the road) [37, 18, 14, 4]. This is a more\ndifﬁcult task than semantic segmentation, as it requires not\nonly an estimate of each pixel’s class, but also which object\nthat pixel belongs to. It is also more complicated than ob-\nject detection, which often predicts object bounding boxes\nalone [17]. Finally, our model predicts pixel-wise metric\ndepth. Depth by recognition has been demonstrated using\ndense prediction networks with supervised [15] and unsu-\npervised [16] deep learning. However it is very hard to esti-\nmate depth in a way which generalises well. We show that\nwe can improve our estimation of geometry and depth by\nusing semantic labels and multi-task deep learning.\nIn existing literature, separate deep learning models\n1\narXiv:1705.07115v3  [cs.CV]  24 Apr 2018Encoder\nSemantic\nDecoder\nInput Image\nMulti-Task\nLoss\nInstance\nDecoder\nDepth\nDecoder\nSemantic\nTask\nUncertainty\nInstance\nTask\nUncertainty\nDepth\nTask\nUncertainty\nΣ\nFigure 1: Multi-task deep learning. We derive a principled way of combining multiple regression and classiﬁcation loss functions for\nmulti-task learning. Our architecture takes a single monocular RGB image as input and produces a pixel-wise classiﬁcation, an instance\nsemantic segmentation and an estimate of per pixel depth. Multi-task learning can improve accuracy over separately trained models because\ncues from one task, such as depth, are used to regularize and improve the generalization of another domain, such as segmentation.\nwould be used to learn depth regression, semantic segmen-\ntation and instance segmentation to create a complete scene\nunderstanding system. Given a single monocular input im-\nage, our system is the ﬁrst to produce a semantic segmenta-\ntion, a dense estimate of metric depth and an instance level\nsegmentation jointly (Figure 1). While other vision mod-\nels have demonstrated multi-task learning, we show how to\nlearn to combine semantics and geometry. Combining these\ntasks into a single model ensures that the model agrees be-\ntween the separate task outputs while reducing computa-\ntion. Finally, we show that using a shared representation\nwith multi-task learning improves performance on various\nmetrics, making the models more effective.\nIn summary, the key contributions of this paper are:\n1. a novel and principled multi-task loss to simultane-\nously learn various classiﬁcation and regression losses\nof varying quantities and units using homoscedastic\ntask uncertainty,\n2. a uniﬁed architecture for semantic segmentation, in-\nstance segmentation and depth regression,\n3. demonstrating the importance of loss weighting in\nmulti-task deep learning and how to obtain superior\nperformance compared to equivalent separately trained\nmodels.\n2. Related Work\nMulti-task learning aims to improve learning efﬁciency\nand prediction accuracy for each task, when compared to\ntraining a separate model for each task [40, 5]. It can be con-\nsidered an approach to inductive knowledge transfer which\nimproves generalisation by sharing the domain information\nbetween complimentary tasks. It does this by using a shared\nrepresentation to learn multiple tasks – what is learned from\none task can help learn other tasks [7].\nFine-tuning [1, 36] is a basic example of multi-task\nlearning, where we can leverage different learning tasks by\nconsidering them as a pre-training step. Other models al-\nternate learning between each training task, for example in\nnatural language processing [11]. Multi-task learning can\nalso be used in a data streaming setting [40], or to prevent\nforgetting previously learned tasks in reinforcement learn-\ning [26]. It can also be used to learn unsupervised features\nfrom various data sources with an auto-encoder [35].\nIn computer vision there are many examples of methods\nfor multi-task learning. Many focus on semantic tasks, such\nas classiﬁcation and semantic segmentation [30] or classiﬁ-\ncation and detection [38]. MultiNet [39] proposes an archi-\ntecture for detection, classiﬁcation and semantic segmenta-\ntion. CrossStitch networks [34] explore methods to com-\nbine multi-task neural activations. Uhrig et al. [41] learn\nsemantic and instance segmentations under a classiﬁcation\nsetting. Multi-task deep learning has also been used for ge-\nometry and regression tasks. [15] show how to learn se-\nmantic segmentation, depth and surface normals. PoseNet\n[25] is a model which learns camera position and orienta-\ntion. UberNet [27] learns a number of different regression\nand classiﬁcation tasks under a single architecture. In this\nwork we are the ﬁrst to propose a method for jointly learn-\ning depth regression, semantic and instance segmentation.\nLike the model of [15], our model learns both semantic and\ngeometry representations, which is important for scene un-\nderstanding. However, our model learns the much harder\ntask of instance segmentation which requires knowledge of\nboth semantics and geometry. This is because our model\nmust determine the class and spatial relationship for each\npixel in each object for instance segmentation.\n200.10.20.30.40.50.60.70.80.91\n45\n50\n55\n60\nClassiﬁcation Weight\nIoU Classiﬁcation (%)\nClassiﬁcation\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0.58\n0.6\n0.62\n0.64\nDepth Weight\nRMS Inverse Depth Error (m−1)\nClassiﬁcation\nDepth Regression\nTask Weights Class Depth\nClass Depth IoU [%] Err. [px]\n1.0 0.0 59.4 -\n0.975 0.025 59.5 0.664\n0.95 0.05 59.9 0.603\n0.9 0.1 60.1 0.586\n0.85 0.15 60.4 0.582\n0.8 0.2 59.6 0.577\n0.7 0.3 59.0 0.573\n0.5 0.5 56.3 0.602\n0.2 0.8 47.2 0.625\n0.1 0.9 42.7 0.628\n0.0 1.0 - 0.640\nLearned weights\n62.7 0.533 with task uncertainty\n(this work, Section 3.2)\n(a) Comparing loss weightings when learning semantic classiﬁcation and depth regression\n00.10.20.30.40.50.60.70.80.91\n3.8\n4\n4.2\n4.4\n4.6\n4.8\n5\nInstance Weight\nRMS Instance (px)\nInstance Regression\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7\nDepth Weight\nRMS Inverse Depth Error (m−1)Instance Regression\nDepth Regression\nTask Weights Instance Depth\nInstance Depth Err. [px] Err. [px]\n1.0 0.0 4.61\n0.75 0.25 4.52 0.692\n0.5 0.5 4.30 0.655\n0.4 0.6 4.14 0.641\n0.3 0.7 4.04 0.615\n0.2 0.8 3.83 0.607\n0.1 0.9 3.91 0.600\n0.05 0.95 4.27 0.607\n0.025 0.975 4.31 0.624\n0.0 1.0 0.640\nLearned weights\n3.54 0.539 with task uncertainty\n(this work, Section 3.2)\n(b) Comparing loss weightings when learning instance regression and depth regression\nFigure 2: Learning multiple tasks improves the model’s representation and individual task performance . These ﬁgures and tables\nillustrate the advantages of multi-task learning for (a) semantic classiﬁcation and depth regression and (b) instance and depth regression.\nPerformance of the model in individual tasks is seen at both edges of the plot where w = 0 and w = 1. For some balance of weightings\nbetween each task, we observe improved performance for both tasks. All models were trained with a learning rate of 0.01 with the\nrespective weightings applied to the losses using the loss function in (1). Results are shown using the Tiny CityScapes validation dataset\nusing a down-sampled resolution of 128 × 256.\nMore importantly, all previous methods which learn mul-\ntiple tasks simultaneously use a na ¨ıve weighted sum of\nlosses, where the loss weights are uniform, or crudely and\nmanually tuned. In this work we propose a principled\nway of combining multiple loss functions to simultaneously\nlearn multiple objectives using homoscedastic task uncer-\ntainty. We illustrate the importance of appropriately weight-\ning each task in deep learning to achieve good performance\nand show that our method can learn to balance these weight-\nings optimally.\n3. Multi Task Learning with Homoscedastic\nUncertainty\nMulti-task learning concerns the problem of optimising a\nmodel with respect to multiple objectives. It is prevalent in\nmany deep learning problems. The naive approach to com-\nbining multi objective losses would be to simply perform a\nweighted linear sum of the losses for each individual task:\nLtotal =\n∑\ni\nwiLi. (1)\nThis is the dominant approach used by prior work [39,\n38, 30, 41], for example for dense prediction tasks [27],\nfor scene understanding tasks [15] and for rotation (in\nquaternions) and translation (in meters) for camera pose\n[25]. However, there are a number of issues with this\nmethod. Namely, model performance is extremely sensitive\nto weight selection, wi, as illustrated in Figure 2. These\nweight hyper-parameters are expensive to tune, often taking\nmany days for each trial. Therefore, it is desirable to ﬁnd a\nmore convenient approach which is able to learn the optimal\nweights.\nMore concretely, let us consider a network which learns\nto predict pixel-wise depth and semantic class from an in-\nput image. In Figure 2 the two boundaries of each plot show\nmodels trained on individual tasks, with the curves showing\n3performance for varying weights wi for each task. We ob-\nserve that at some optimal weighting, the joint network per-\nforms better than separate networks trained on each task in-\ndividually (performance of the model in individual tasks is\nseen at both edges of the plot: w= 0and w= 1). At near-\nby values to the optimal weight the network performs worse\non one of the tasks. However, searching for these optimal\nweightings is expensive and increasingly difﬁcult with large\nmodels with numerous tasks. Figure 2 also shows a similar\nresult for two regression tasks; instance segmentation and\ndepth regression. We next show how to learn optimal task\nweightings using ideas from probabilistic modelling.\n3.1. Homoscedastic uncertainty as task-dependent\nuncertainty\nIn Bayesian modelling, there are two main types of un-\ncertainty one can model [24].\n•Epistemic uncertainty is uncertainty in the model,\nwhich captures what our model does not know due to\nlack of training data. It can be explained away with\nincreased training data.\n•Aleatoric uncertainty captures our uncertainty with re-\nspect to information which our data cannot explain.\nAleatoric uncertainty can be explained away with the\nability to observe all explanatory variables with in-\ncreasing precision.\nAleatoric uncertainty can again be divided into two sub-\ncategories.\n•Data-dependent or Heteroscedastic uncertainty is\naleatoric uncertainty which depends on the input data\nand is predicted as a model output.\n•Task-dependent or Homoscedastic uncertainty is\naleatoric uncertainty which is not dependent on the in-\nput data. It is not a model output, rather it is a quantity\nwhich stays constant for all input data and varies be-\ntween different tasks. It can therefore be described as\ntask-dependent uncertainty.\nIn a multi-task setting, we show that the task uncertainty\ncaptures the relative conﬁdence between tasks, reﬂecting\nthe uncertainty inherent to the regression or classiﬁcation\ntask. It will also depend on the task’s representation or unit\nof measure. We propose that we can use homoscedastic\nuncertainty as a basis for weighting losses in a multi-task\nlearning problem.\n3.2. Multi-task likelihoods\nIn this section we derive a multi-task loss function based\non maximising the Gaussian likelihood with homoscedastic\nuncertainty. Let fW(x) be the output of a neural network\nwith weights W on input x. We deﬁne the following proba-\nbilistic model. For regression tasks we deﬁne our likelihood\nas a Gaussian with mean given by the model output:\np(y|fW(x)) =N(fW(x),σ2) (2)\nwith an observation noise scalar σ. For classiﬁcation we\noften squash the model output through a softmax function,\nand sample from the resulting probability vector:\np(y|fW(x)) =Softmax(fW(x)). (3)\nIn the case of multiple model outputs, we often deﬁne the\nlikelihood to factorise over the outputs, given some sufﬁ-\ncient statistics. We deﬁne fW(x) as our sufﬁcient statistics,\nand obtain the following multi-task likelihood:\np(y1,..., yK|fW(x)) =p(y1|fW(x))...p(yK|fW(x))\n(4)\nwith model outputs y1,..., yK (such as semantic segmenta-\ntion, depth regression, etc).\nIn maximum likelihood inference, we maximise the log\nlikelihood of the model. In regression, for example, the log\nlikelihood can be written as\nlog p(y|fW(x)) ∝− 1\n2σ2 ||y −fW(x)||2 −log σ (5)\nfor a Gaussian likelihood (or similarly for a Laplace like-\nlihood) with σ the model’s observation noise parameter –\ncapturing how much noise we have in the outputs. We then\nmaximise the log likelihood with respect to the model pa-\nrameters W and observation noise parameter σ.\nLet us now assume that our model output is composed of\ntwo vectors y1 and y2, each following a Gaussian distribu-\ntion:\np(y1,y2|fW(x)) =p(y1|fW(x)) ·p(y2|fW(x))\n= N(y1; fW(x),σ2\n1) ·N(y2; fW(x),σ2\n2).\n(6)\nThis leads to the minimisation objective, L(W,σ1,σ2),\n(our loss) for our multi-output model:\n= −log p(y1,y2|fW(x))\n∝ 1\n2σ2\n1\n||y1 −fW(x)||2 + 1\n2σ2\n2\n||y2 −fW(x)||2 + logσ1σ2\n= 1\n2σ2\n1\nL1(W) + 1\n2σ2\n2\nL2(W) + logσ1σ2\n(7)\nWhere we wrote L1(W) =||y1 −fW(x)||2 for the loss of\nthe ﬁrst output variable, and similarly for L2(W).\nWe interpret minimising this last objective with respect\nto σ1 and σ2 as learning the relative weight of the losses\n4L1(W) and L2(W) adaptively, based on the data. As σ1\n– the noise parameter for the variable y1 – increases, we\nhave that the weight of L1(W) decreases. On the other\nhand, as the noise decreases, we have that the weight of\nthe respective objective increases. The noise is discouraged\nfrom increasing too much (effectively ignoring the data) by\nthe last term in the objective, which acts as a regulariser for\nthe noise terms.\nThis construction can be trivially extended to multiple\nregression outputs. However, the extension to classiﬁcation\nlikelihoods is more interesting. We adapt the classiﬁcation\nlikelihood to squash a scaled version of the model output\nthrough a softmax function:\np(y|fW(x),σ) =Softmax( 1\nσ2 fW(x)) (8)\nwith a positive scalar σ. This can be interpreted as a Boltz-\nmann distribution (also called Gibbs distribution) where the\ninput is scaled byσ2 (often referred to astemperature). This\nscalar is either ﬁxed or can be learnt, where the parameter’s\nmagnitude determines how ‘uniform’ (ﬂat) the discrete dis-\ntribution is. This relates to its uncertainty, as measured in\nentropy. The log likelihood for this output can then be writ-\nten as\nlog p(y = c|fW(x),σ) = 1\nσ2 fW\nc (x)\n−log\n∑\nc′\nexp\n( 1\nσ2 fW\nc′ (x)\n) (9)\nwith fW\nc (x) the c’th element of the vectorfW(x).\nNext, assume that a model’s multiple outputs are com-\nposed of a continuous output y1 and a discrete out-\nput y2, modelled with a Gaussian likelihood and a soft-\nmax likelihood, respectively. Like before, the joint loss,\nL(W,σ1,σ2), is given as:\n= −log p(y1,y2 = c|fW(x))\n= −log N(y1; fW(x),σ2\n1) ·Softmax(y2 = c; fW(x),σ2)\n= 1\n2σ2\n1\n||y1 −fW(x)||2 + logσ1 −log p(y2 = c|fW(x),σ2)\n= 1\n2σ2\n1\nL1(W) + 1\nσ2\n2\nL2(W) + logσ1\n+ log\n∑\nc′ exp\n(\n1\nσ2\n2\nfW\nc′ (x)\n)\n(∑\nc′ exp\n(\nfW\nc′ (x)\n))1\nσ2\n2\n≈ 1\n2σ2\n1\nL1(W) + 1\nσ2\n2\nL2(W) + logσ1 + logσ2,\n(10)\nwhere again we write L1(W) = ||y1 − fW(x)||2\nfor the Euclidean loss of y1, write L2(W) =\n−log Softmax(y2,fW(x)) for the cross entropy loss of y2\n(with fW(x) not scaled), and optimise with respect to W\nas well as σ1, σ2. In the last transition we introduced the ex-\nplicit simplifying assumption 1\nσ2\n∑\nc′ exp\n(\n1\nσ2\n2\nfW\nc′ (x)\n)\n≈\n(∑\nc′ exp\n(\nfW\nc′ (x)\n))1\nσ2\n2\nwhich becomes an equality\nwhen σ2 →1. This has the advantage of simplifying the\noptimisation objective, as well as empirically improving re-\nsults.\nThis last objective can be seen as learning the relative\nweights of the losses for each output. Large scale values\nσ2 will decrease the contribution of L2(W), whereas small\nscale σ2 will increase its contribution. The scale is regulated\nby the last term in the equation. The objective is penalised\nwhen setting σ2 too large.\nThis construction can be trivially extended to arbitrary\ncombinations of discrete and continuous loss functions, al-\nlowing us to learn the relative weights of each loss in a\nprincipled and well-founded way. This loss is smoothly dif-\nferentiable, and is well formed such that the task weights\nwill not converge to zero. In contrast, directly learning the\nweights using a simple linear sum of losses (1) would result\nin weights which quickly converge to zero. In the following\nsections we introduce our experimental model and present\nempirical results.\nIn practice, we train the network to predict the log vari-\nance, s := logσ2. This is because it is more numerically\nstable than regressing the variance, σ2, as the loss avoids\nany division by zero. The exponential mapping also allows\nus to regress unconstrained scalar values, where exp(−s)\nis resolved to the positive domain giving valid values for\nvariance.\n4. Scene Understanding Model\nTo understand semantics and geometry we ﬁrst propose\nan architecture which can learn regression and classiﬁcation\noutputs, at a pixel level. Our architecture is a deep con-\nvolutional encoder decoder network [3]. Our model con-\nsists of a number of convolutional encoders which produce\na shared representation, followed by a corresponding num-\nber of task-speciﬁc convolutional decoders. A high level\nsummary is shown in Figure 1.\nThe purpose of the encoder is to learn a deep mapping to\nproduce rich, contextual features, using domain knowledge\nfrom a number of related tasks. Our encoder is based on\nDeepLabV3 [10], which is a state of the art semantic seg-\nmentation framework. We use ResNet101 [20] as the base\nfeature encoder, followed by an Atrous Spatial Pyramid\nPooling (ASPP) module [10] to increase contextual aware-\nness. We apply dilated convolutions in this encoder, such\nthat the resulting feature map is sub-sampled by a factor of\n5(a) Input Image\n (b) Semantic Segmentation\n(c) Instance vector regression\n (d) Instance Segmentation\nFigure 3: Instance centroid regression method. For each pixel,\nwe regress a vector pointing to the instance’s centroid. The loss is\nonly computed over pixels which are from instances. We visualise\n(c) by representing colour as the orientation of the instance vector,\nand intensity as the magnitude of the vector.\n8 compared to the input image dimensions.\nWe then split the network into separate decoders (with\nseparate weights) for each task. The purpose of the decoder\nis to learn a mapping from the shared features to an output.\nEach decoder consists of a 3 ×3 convolutional layer with\noutput feature size 256, followed by a1×1 layer regressing\nthe task’s output. Further architectural details are described\nin Appendix A.\nSemantic Segmentation. We use the cross-entropy loss\nto learn pixel-wise class probabilities, averaging the loss\nover the pixels with semantic labels in each mini-batch.\nInstance Segmentation. An intuitive method for deﬁn-\ning which instance a pixel belongs to is an association to the\ninstance’s centroid. We use a regression approach for in-\nstance segmentation [29]. This approach is inspired by [28]\nwhich identiﬁes instances using Hough votes from object\nparts. In this work we extend this idea by using votes from\nindividual pixels using deep learning. We learn an instance\nvector, ˆxn, for each pixel coordinate,cn, which points to the\ncentroid of the pixel’s instance,in, such that in = ˆxn+ cn.\nWe train this regression with an L1 loss using ground truth\nlabels xn, averaged over all labelled pixels, NI, in a mini-\nbatch: LInstance = 1\n|NI|\n∑\nNI\n∥xn −ˆxn∥1.\nFigure 3 details the representation we use for instance\nsegmentation. Figure 3(a) shows the input image and a\nmask of the pixels which are of an instance class (at test\ntime inferred from the predicted semantic segmentation).\nFigure 3(b) and Figure 3(c) show the ground truth and pre-\ndicted instance vectors for both x and y coordinates. We\nthen cluster these votes using OPTICS [2], resulting in the\npredicted instance segmentation output in Figure 3(d).\nOne of the most difﬁcult cases for instance segmentation\nalgorithms to handle is when the instance mask is split due\n(a) Input Image\n (b) Instance Segmentation\nFigure 4: This example shows two cars which are occluded by\ntrees and lampposts, making the instance segmentation challeng-\ning. Our instance segmentation method can handle occlusions ef-\nfectively. We can correctly handle segmentation masks which are\nsplit by occlusion, yet part of the same instance, by incorporating\nsemantics and geometry.\nto occlusion. Figure 4 shows that our method can handle\nthese situations, by allowing pixels to vote for their instance\ncentroid with geometry. Methods which rely on watershed\napproaches [4], or instance edge identiﬁcation approaches\nfail in these scenarios.\nTo obtain segmentations for each instance, we now need\nto estimate the instance centres, ˆin. We propose to con-\nsider the estimated instance vectors,ˆxn, as votes in a Hough\nparameter space and use a clustering algorithm to identify\nthese instance centres. OPTICS [2], is an efﬁcient density\nbased clustering algorithm. It is able to identify an unknown\nnumber of multi-scale clusters with varying density from a\ngiven set of samples. We chose OPICS for two reasons.\nCrucially, it does not assume knowledge of the number of\nclusters like algorithms such as k-means [33]. Secondly, it\ndoes not assume a canonical instance size or density like\ndiscretised binning approaches [12]. Using OPTICS, we\ncluster the points cn + ˆxn into a number of estimated in-\nstances, ˆi. We can then assign each pixel, pn to the instance\nclosest to its estimated instance vector, cn + ˆxn.\nDepth Regression. We train with supervised labels us-\ning pixel-wise metric inverse depth using aL1 loss function:\nLDepth = 1\n|ND|\n∑\nND\ndn − ˆdn\n\n1\n. Our architecture esti-\nmates inverse depth, ˆdn, because it can represent points at\ninﬁnite distance (such as sky). We can obtain inverse depth\nlabels, dn, from a RGBD sensor or stereo imagery. Pixels\nwhich do not have an inverse depth label are ignored in the\nloss.\n5. Experiments\nWe demonstrate the efﬁcacy of our method on\nCityScapes [13], a large dataset for road scene understand-\ning. It comprises of stereo imagery, from automotive grade\nstereo cameras with a 22cmbaseline, labelled with instance\nand semantic segmentations from 20 classes. Depth images\nare also provided, labelled using SGM [22], which we treat\nas pseudo ground truth. Additionally, we assign zero in-\nverse depth to pixels labelled as sky. The dataset was col-\n6Task Weights Segmentation Instance Inverse Depth\nLoss Seg. Inst. Depth IoU [%] Mean Error [px] Mean Error [px]\nSegmentation only 1 0 0 59.4% - -\nInstance only 0 1 0 - 4.61 -\nDepth only 0 0 1 - - 0.640\nUnweighted sum of losses 0.333 0.333 0.333 50.1% 3.79 0.592\nApprox. optimal weights 0.89 0.01 0.1 62.8% 3.61 0.549\n2 task uncertainty weighting ✓ ✓ 61.0% 3.42 -\n2 task uncertainty weighting ✓ ✓ 62.7% - 0.533\n2 task uncertainty weighting ✓ ✓ - 3.54 0.539\n3 task uncertainty weighting ✓ ✓ ✓ 63.4% 3.50 0.522\nTable 1: Quantitative improvement when learning semantic segmentation, instance segmentation and depth with our multi-task loss.\nExperiments were conducted on the Tiny CityScapes dataset (sub-sampled to a resolution of 128 × 256). Results are shown from the\nvalidation set. We observe an improvement in performance when training with our multi-task loss, over both single-task models and\nweighted losses. Additionally, we observe an improvement when training on all three tasks ( 3 × ✓) using our multi-task loss, compared\nwith all pairs of tasks alone (denoted by 2 × ✓). This shows that our loss function can automatically learn a better performing weighting\nbetween the tasks than the baselines.\nlected from a number of cities in ﬁne weather and consists\nof 2,975 training and 500 validation images at 2048 ×1024\nresolution. 1,525 images are withheld for testing on an on-\nline evaluation server.\nFurther training details, and optimisation hyperparame-\nters, are provided in Appendix A.\n5.1. Model Analysis\nIn Table 1 we compare individual models to multi-task\nlearning models using a na¨ıve weighted loss or the task un-\ncertainty weighting we propose in this paper. To reduce the\ncomputational burden, we train each model at a reduced res-\nolution of 128 ×256 pixels, over 50,000 iterations. When\nwe downsample the data by a factor of four, we also need\nto scale the disparity labels accordingly. Table 1 clearly il-\nlustrates the beneﬁt of multi-task learning, which obtains\nsigniﬁcantly better performing results than individual task\nmodels. For example, using our method we improve classi-\nﬁcation results from 59.4% to 63.4%.\nWe also compare to a number of na¨ıve multi-task losses.\nWe compare weighting each task equally and using approx-\nimately optimal weights. Using a uniform weighting results\nin poor performance, in some cases not even improving on\nthe results from the single task model. Obtaining approxi-\nmately optimal weights is difﬁcult with increasing number\nof tasks as it requires an expensive grid search over param-\neters. However, even these weights perform worse com-\npared with our proposed method. Figure 2 shows that using\ntask uncertainty weights can even perform better compared\nto optimal weights found through ﬁne-grained grid search.\nWe believe that this is due to two reasons. First, grid search\nis restricted in accuracy by the resolution of the search.\nSecond, optimising the task weights using a homoscedas-\ntic noise term allows for the weights to be dynamic during\ntraining. In general, we observe that the uncertainty term\ndecreases during training which improves the optimisation\nprocess.\nIn Appendix B we ﬁnd that our task-uncertainty loss is\nrobust to the initialisation chosen for the parameters. These\nquickly converge to a similar optima in a few hundred train-\ning iterations. We also ﬁnd the resulting task weightings\nvaries throughout the course of training. For our ﬁnal model\n(in Table 2), at the end of training, the losses are weighted\nwith the ratio 43 : 1 : 0.16 for semantic segmentation, depth\nregression and instance segmentation, respectively.\nFinally, we benchmark our model using the full-size\nCityScapes dataset. In Table 2 we compare to a number of\nother state of the art methods in all three tasks. Our method\nis the ﬁrst model which completes all three tasks with a sin-\ngle model. We compare favourably with other approaches,\noutperforming many which use comparable training data\nand inference tools. Figure 5 shows some qualitative ex-\namples of our model.\n6. Conclusions\nWe have shown that correctly weighting loss terms is\nof paramount importance for multi-task learning problems.\nWe demonstrated that homoscedastic (task) uncertainty is\nan effective way to weight losses. We derived a principled\nloss function which can learn a relative weighting automati-\ncally from the data and is robust to the weight initialization.\nWe showed that this can improve performance for scene\nunderstanding tasks with a uniﬁed architecture for seman-\n7Semantic Segmentation Instance Segmentation Monocular Disparity Estimation\nMethod IoU class iIoU class IoU cat iIoU cat AP AP 50% AP 100m AP 50m Mean Error [px] RMS Error [px]\nSemantic segmentation, instance segmentation and depth regression methods (this work)\nMulti-Task Learning 78.5 57.4 89.9 77.7 21.6 39.0 35.0 37.0 2.92 5.88\nSemantic segmentation and instance segmentation methods\nUhrig et al. [41] 64.3 41.6 85.9 73.9 8.9 21.1 15.3 16.7 - -\nInstance segmentation only methods\nMask R-CNN [19] - - - - 26.2 49.9 37.6 40.1 - -\nDeep Watershed [4] - - - - 19.4 35.3 31.4 36.8 - -\nR-CNN + MCG [13] - - - - 4.6 12.9 7.7 10.3 - -\nSemantic segmentation only methods\nDeepLab V3 [10] 81.3 60.9 91.6 81.7 - - - - - -\nPSPNet [44] 81.2 59.6 91.2 79.2 - - - - - -\nAdelaide [31] 71.6 51.7 87.3 74.1 - - - - - -\nTable 2: CityScapes Benchmark [13]. We show results from the test dataset using the full resolution of 1024 × 2048 pixels. For the\nfull leaderboard, please see www.cityscapes-dataset.com/benchmarks. The disparity (inverse depth) metrics were computed\nagainst the CityScapes depth maps, which are sparse and computed using SGM stereo [21]. Note, these comparisons are not entirely fair,\nas many methods use ensembles of different training datasets. Our method is the ﬁrst to address all three tasks with a single model.\n(a) Input image\n (b) Segmentation output\n (c) Instance output\n (d) Depth output\nFigure 5: Qualitative results for multi-task learning of geometry and semantics for road scene understanding . Results are shown\non test images from the CityScapes dataset using our multi-task approach with a single network trained on all tasks. We observe that\nmulti-task learning improves the smoothness and accuracy for depth perception because it learns a representation that uses cues from other\ntasks, such as segmentation (and vice versa).\ntic segmentation, instance segmentation and per-pixel depth\nregression. We demonstrated modelling task-dependent ho-\nmoscedastic uncertainty improves the model’s representa-\ntion and each task’s performance when compared to sepa-\nrate models trained on each task individually.\nThere are many interesting questions left unanswered.\nFirstly, our results show that there is usually not a single op-\ntimal weighting for all tasks. Therefore, what is the optimal\nweighting? Is multitask learning is an ill-posed optimisa-\ntion problem without a single higher-level goal?\nA second interesting question is where the optimal loca-\ntion is for splitting the shared encoder network into separate\ndecoders for each task? And, what network depth is best for\nthe shared multi-task representation?\nFinally, why do the semantics and depth tasks out-\nperform the semantics and instance tasks results in Table 1?\nClearly the three tasks explored in this paper are compli-\nmentary and useful for learning a rich representation about\nthe scene. It would be beneﬁcial to be able to quantify the\nrelationship between tasks and how useful they would be\nfor multitask representation learning.\n8References\n[1] P. Agrawal, J. Carreira, and J. Malik. Learning to see by\nmoving. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 37–45, 2015. 2\n[2] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and J. Sander.\nOptics: ordering points to identify the clustering structure. In\nACM Sigmod Record, volume 28, pages 49–60. ACM, 1999.\n6\n[3] V . Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A\ndeep convolutional encoder-decoder architecture for scene\nsegmentation. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2017. 1, 5\n[4] M. Bai and R. Urtasun. Deep watershed transform for\ninstance segmentation. arXiv preprint arXiv:1611.08303 ,\n2016. 1, 6, 8\n[5] J. Baxter et al. A model of inductive bias learning. J. Artif.\nIntell. Res.(JAIR), 12(149-198):3, 2000. 2\n[6] S. R. Bul `o, L. Porzi, and P. Kontschieder. In-place activated\nbatchnorm for memory-optimized training of dnns. arXiv\npreprint arXiv:1712.02616, 2017.\n[7] R. Caruana. Multitask learning. In Learning to learn, pages\n95–133. Springer, 1998. 1, 2\n[8] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and\nA. L. Yuille. Semantic image segmentation with deep con-\nvolutional nets and fully connected crfs. In ICLR, 2015. 1\n[9] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and\nA. L. Yuille. Deeplab: Semantic image segmentation with\ndeep convolutional nets, atrous convolution, and fully con-\nnected crfs. arXiv preprint arXiv:1606.00915, 2016.\n[10] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Re-\nthinking atrous convolution for semantic image segmenta-\ntion. arXiv preprint arXiv:1706.05587, 2017. 5, 8, 11\n[11] R. Collobert and J. Weston. A uniﬁed architecture for natural\nlanguage processing: Deep neural networks with multitask\nlearning. In Proceedings of the 25th international conference\non Machine learning, pages 160–167. ACM, 2008. 1, 2\n[12] D. Comaniciu and P. Meer. Mean shift: A robust approach\ntoward feature space analysis. IEEE Transactions on pattern\nanalysis and machine intelligence, 24(5):603–619, 2002. 6\n[13] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\nR. Benenson, U. Franke, S. Roth, and B. Schiele. The\ncityscapes dataset for semantic urban scene understanding.\nIn In Proc. IEEE Conf. on Computer Vision and Pattern\nRecognition, 2016. 6, 8\n[14] J. Dai, K. He, and J. Sun. Instance-aware semantic segmenta-\ntion via multi-task network cascades. In In Proc. IEEE Conf.\non Computer Vision and Pattern Recognition, 2016. 1\n[15] D. Eigen and R. Fergus. Predicting depth, surface normals\nand semantic labels with a common multi-scale convolu-\ntional architecture. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2650–2658, 2015. 1,\n2, 3\n[16] R. Garg and I. Reid. Unsupervised cnn for single view depth\nestimation: Geometry to the rescue. Computer Vision–ECCV\n2016, pages 740–756, 2016. 1\n[17] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In In Proc. IEEE Conf. on Computer Vision\nand Pattern Recognition, pages 580–587, 2014. 1\n[18] B. Hariharan, P. Arbel ´aez, R. Girshick, and J. Malik. Hyper-\ncolumns for object segmentation and ﬁne-grained localiza-\ntion. In In Proc. IEEE Conf. on Computer Vision and Pattern\nRecognition, pages 447–456. IEEE, 2014. 1\n[19] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick. Mask r-cnn.\narXiv preprint arXiv:1703.06870, 2017. 8\n[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In In Proc. IEEE Conf. on Computer\nVision and Pattern Recognition, 2016. 5, 11\n[21] H. Hirschmuller. Accurate and efﬁcient stereo processing by\nsemi-global matching and mutual information. In In Proc.\nIEEE Conf. on Computer Vision and Pattern Recognition ,\nvolume 2, pages 807–814. IEEE, 2005. 8\n[22] H. Hirschmuller. Stereo processing by semiglobal matching\nand mutual information. IEEE Transactions on pattern anal-\nysis and machine intelligence, 30(2):328–341, 2008. 6\n[23] J.-T. Huang, J. Li, D. Yu, L. Deng, and Y . Gong. Cross-\nlanguage knowledge transfer using multilingual deep neural\nnetwork with shared hidden layers. In Acoustics, Speech and\nSignal Processing (ICASSP), 2013 IEEE International Con-\nference on, pages 7304–7308. IEEE, 2013. 1\n[24] A. Kendall and Y . Gal. What uncertainties do we need in\nbayesian deep learning for computer vision? arXiv preprint\narXiv:1703.04977, 2017. 4\n[25] A. Kendall, M. Grimes, and R. Cipolla. Convolutional net-\nworks for real-time 6-dof camera relocalization. In Pro-\nceedings of the International Conference on Computer Vi-\nsion (ICCV), 2015. 2, 3\n[26] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Des-\njardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho,\nA. Grabska-Barwinska, et al. Overcoming catastrophic for-\ngetting in neural networks. Proceedings of the National\nAcademy of Sciences, page 201611835, 2017. 2\n[27] I. Kokkinos. Ubernet: Training auniversal’convolutional\nneural network for low-, mid-, and high-level vision us-\ning diverse datasets and limited memory. arXiv preprint\narXiv:1609.02132, 2016. 1, 2, 3\n[28] B. Leibe, A. Leonardis, and B. Schiele. Robust object detec-\ntion with interleaved categorization and segmentation. Inter-\nnational Journal of Computer Vision (IJCV) , 77(1-3):259–\n289, 2008. 6\n[29] X. Liang, Y . Wei, X. Shen, J. Yang, L. Lin, and S. Yan.\nProposal-free network for instance-level object segmenta-\ntion. arXiv preprint arXiv:1509.02636, 2015. 6\n[30] Y . Liao, S. Kodagoda, Y . Wang, L. Shi, and Y . Liu. Un-\nderstand scene categories by objects: A semantic regularized\nscene classiﬁer using convolutional neural networks. In2016\nIEEE International Conference on Robotics and Automation\n(ICRA), pages 2318–2325. IEEE, 2016. 2, 3\n[31] G. Lin, C. Shen, I. Reid, et al. Efﬁcient piecewise training\nof deep structured models for semantic segmentation. arXiv\npreprint arXiv:1504.01013, 2015. 8\n[32] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In Proc. IEEE Conf. on\nComputer Vision and Pattern Recognition, 2015. 1\n9[33] J. MacQueen et al. Some methods for classiﬁcation and anal-\nysis of multivariate observations. In Proceedings of the ﬁfth\nBerkeley symposium on mathematical statistics and proba-\nbility, volume 1, pages 281–297. Oakland, CA, USA., 1967.\n6\n[34] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-\nstitch networks for multi-task learning. InProceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 3994–4003, 2016. 2\n[35] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y . Ng.\nMultimodal deep learning. In Proceedings of the 28th inter-\nnational conference on machine learning (ICML-11) , pages\n689–696, 2011. 2\n[36] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and\ntransferring mid-level image representations using convolu-\ntional neural networks. In In Proc. IEEE Conf. on Computer\nVision and Pattern Recognition , pages 1717–1724. IEEE,\n2014. 2\n[37] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to seg-\nment object candidates. In Advances in Neural Information\nProcessing Systems, pages 1990–1998, 2015. 1\n[38] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y . LeCun. Overfeat: Integrated recognition, localization\nand detection using convolutional networks. International\nConference on Learning Representations (ICLR), 2014. 1, 2,\n3\n[39] M. Teichmann, M. Weber, M. Zoellner, R. Cipolla, and\nR. Urtasun. Multinet: Real-time joint semantic reasoning\nfor autonomous driving. arXiv preprint arXiv:1612.07695,\n2016. 2, 3\n[40] S. Thrun. Is learning the n-th thing any easier than learning\nthe ﬁrst? In Advances in neural information processing sys-\ntems, pages 640–646. MORGAN KAUFMANN PUBLISH-\nERS, 1996. 2\n[41] J. Uhrig, M. Cordts, U. Franke, and T. Brox. Pixel-level\nencoding and depth layering for instance-level semantic la-\nbeling. arXiv preprint arXiv:1604.05096, 2016. 2, 3, 8\n[42] F. Yu and V . Koltun. Multi-scale context aggregation by di-\nlated convolutions. In ICLR, 2016. 1\n[43] S. Zagoruyko and N. Komodakis. Wide residual networks. In\nE. R. H. Richard C. Wilson and W. A. P. Smith, editors,Pro-\nceedings of the British Machine Vision Conference (BMVC),\npages 87.1–87.12. BMV A Press, September 2016.\n[44] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene\nparsing network. arXiv preprint arXiv:1612.01105, 2016. 8\n[45] S. Zheng, S. Jayasumana, B. Romera-Paredes, V . Vineet,\nZ. Su, D. Du, C. Huang, and P. Torr. Conditional random\nﬁelds as recurrent neural networks. In International Confer-\nence on Computer Vision (ICCV), 2015. 1\n10A. Model Architecture Details\nWe base our model on the recently introduced\nDeepLabV3 [10] segmentation architecture. We use\nResNet101 [20] as our base feature encoder, with dilated\nconvolutions, resulting in a feature map which is downsam-\npled by a factor of 8 compared with the original input im-\nage. We then append dilated (atrous) convolutional ASPP\nmodule [10]. This module is designed to improve the con-\ntextual reasoning of the network. We use an ASPP module\ncomprised of four parallel convolutional layers, with 256\noutput channels and dilation rates (1, 12, 24, 36), with ker-\nnel sizes (12, 32, 32, 32). Additionally, we also apply global\naverage pooling to the encoded features, and convolve them\nto 256 dimensions with a 1 ×1 kernel. We apply batch\nnormalisation to each of these layers and concatenate the\nresulting 1280 features together. This produces the shared\nrepresentation between each task.\nWe then split the network, to decode this representation\nto a given task output. For each task, we construct a decoder\nconsisting of two layers. First, we apply a1×1 convolution,\noutputting 256 features, followed by batch normalisation\nand a non-linear activation. Finally, we convolve this output\nto the required dimensions for a given task. For classiﬁca-\ntion, this will be equal to the number of semantic classes,\notherwise the output will be 1 or 2 channels for depth or in-\nstance segmentation respectively. Finally, we apply bilinear\nupsampling to scale the output to the same resolution as the\ninput.\nThe majority of the model’s parameters and depth is in\nthe feature encoding, with very little ﬂexibility in each task\ndecoder. This illustrates the attraction of multitask learning;\nmost of the compute can be shared between each task to\nlearn a better shared representation.\nA.1. Optimisation\nFor all experiments, we use an initial learning rate\nof 2.5 ×10−3 and polynomial learning rate decay (1 −\niter\nmaxiter )0.9. We train using stochastic gradient descent,\nwith Nesterov updates and momentum 0.9 and weight de-\ncay 104. We conduct all experiments in this paper using\nPyTorch.\nFor the experiments on the Tiny CityScapes validation\ndataset (using a down-sampled resolution of 128 ×256) we\ntrain over 50,000 iterations, using 256 ×256 crops with\nbatch size of 8 on a single NVIDIA 1080Ti GPU. We apply\nrandom horizontal ﬂipping to the data.\nFor the full-scale CityScapes benchmark experiment, we\ntrain over 100,000 iterations with a batch size of 16. We\napply random horizontal ﬂipping (with probability 0.5) and\nrandom scaling (selected from 0.7 - 2.0) to the data dur-\ning training, before making a 512 ×512 crop. The training\ndata is sampled uniformly, and is randomly shufﬂed for each\nepoch. Training takes ﬁve days on a single computer with\nfour NVIDIA 1080Ti GPUs.\nB. Further Analysis\nThis task uncertainty loss is also robust to the value we\nuse to initialise the task uncertainty values. One of the at-\ntractive properties of our approach to weighting multi-task\nlosses is that it is robust to the initialisation choice for the\nhomoscedastic noise parameters. Figure 6 shows that for an\narray of initial choices of log σ2 from −2.0 to 5.0 the ho-\nmoscedastic noise and task loss is able to converge to the\nsame minima. Additionally, the homoscedastic noise terms\nconverges after only 100 iterations, while the network re-\nquires 30,000+ iterations to train. Therefore our model is\nrobust to the choice of initial value for the weighting terms.\nFigure 7 shows losses and uncertainty estimates for each\ntask during training of the ﬁnal model on the full-size\nCityScapes dataset. At a point 500 iterations into training,\nthe model estimates task variance of 0.60, 62.5 and 13.5 for\nsemantic segmentation, instance segmentation and depth re-\ngression, respectively. Becuase the losses are weighted by\nthe inverse of the uncertainty estimates, this results in a task\nweighting ratio of approximately 23 : 0.22 : 1 between se-\nmantics, instance and depth, respectively. At the conclu-\nsion of training, the three tasks have uncertainty estimates\nof 0.075, 3.25 and 20.4, which results in effective weighting\nbetween the tasks of 43: 0.16 : 1. This shows how the task\nuncertainty estimates evolve over time, and the approximate\nﬁnal weightings the network learns. We observe they are far\nfrom uniform, as is often assumed in previous literature.\nInterestingly, we observe that this loss allows the net-\nwork to dynamically tune the weighting. Typically, the ho-\nmoscedastic noise terms decrease in magnitude as training\nprogresses. This makes sense, as during training the model\nbecomes more effective at a task. Therefore the error, and\nuncertainty, will decrease. This has a side-effect of increas-\ning the effective learning rate – because the overall uncer-\ntainty decreases, the weight for each task’s loss increases.\nIn our experiments we compensate for this by annealing the\nlearning rate with a power law.\nFinally, a comment on the model’s failure modes. The\nmodel exhibits similar failure modes to state-of-the-art\nsingle-task models. For example, failure with objects out of\nthe training distribution, occlusion or visually challenging\nsituations. However, we also observe our multi-task model\ntends to fail with similar effect in all three modalities. Ie. an\nerroneous pixel’s prediction in one task will often be highly\ncorrelated with error in another modality. Some examples\ncan be seen in Figure 8.\n110 50 100 150 200 250\nTraining Iterations\n−1\n0\n1\n2\n3\n4\n5\nTask Uncertainty\n(a) Semantic segmentation task\n0 50 100 150 200 250\nTraining Iterations\n1\n2\n3\n4\n5\n6\n7\n8Task Uncertainty (b) Instance segmentation task\n0 50 100 150 200 250\nTraining Iterations\n−1\n0\n1\n2\n3\n4\n5\nTask Uncertainty (c) Depth regression task\nFigure 6: Training plots showing convergence of homoscedastic noise and task loss for an array of initialisation choices for the ho-\nmoscedastic uncertainty terms for all three tasks. Each plot shows the the homoscedastic noise value optimises to the same solution from\na variety of initialisations. Despite the network taking 10, 000+ iterations for the training loss to converge, the task uncertainty converges\nvery rapidly after only 100 iterations.\n0 20000 40000 60000 80000 100000\nTraining Iterations\n0.0\n0.2\n0.4\n0.6\n0.8Loss\n0 20000 40000 60000 80000 100000\nTraining Iterations\n0\n20\n40\n60\n80\n100\n120\n140Loss\n0 20000 40000 60000 80000 100000\nTraining Iterations\n5\n10\n15\n20Loss\n0 20000 40000 60000 80000 100000\nTraining Iterations\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75Task Uncertainty (σ2)\n(a) Semantic segmentation task\n0 20000 40000 60000 80000 100000\nTraining Iterations\n20\n30\n40\n50\n60\n70\n80\n90Task Uncertainty (σ2) (b) Instance segmentation task\n0 20000 40000 60000 80000 100000\nTraining Iterations\n5\n10\n15\n20\n25\n30\n35\n40Task Uncertainty (σ2) (c) Depth regression task\nFigure 7: Learning task uncertainty. These training plots show the losses and task uncertainty estimates for each task during training.\nResults are shown for the ﬁnal model, trained on the fullsize CityScapes dataset.\n12C. Further Qualitative Results\n(a) Input image\n (b) Semantic segmentation\n (c) Instance segmentation\n (d) Depth regression\nFigure 8: More qualitative results on test images from the CityScapes dataset.\n13D. Failure Examples\n(a) Input image\n (b) Semantic segmentation\n (c) Instance segmentation\n (d) Depth regression\nFigure 9: Example where our model fails on the CityScapes test data. The ﬁrst two rows show examples of challenging visual effects\nsuch as reﬂection, which confuse the model. Rows three and four show the model incorrectly distinguishing between road and footpath.\nThis is a common mistake, which we believe is due to a lack of contextual reasoning. Rows ﬁve, six and seven demonstrate incorrect\nclassiﬁcation of a rare class (bus, fence and motorbike, respectively). Finally, the last two rows show failure due to occlusion and where\nthe object is too big for the model’s receptive ﬁeld. Additionally, we observe that failures are highly correlated between the modes, which\nmakes sense as each output is conditioned on the same feature vector. For example, in the second row, the incorrect labelling of the\nreﬂection as a person causes the depth estimation to predict human geometry.\n14"
    }
}