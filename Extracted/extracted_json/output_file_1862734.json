{
    "title": "arXiv:2412.11526                                                                                                     December 16, 2024       1 ",
    "content": {
        "page_content": "arXiv:2412.11526                                                                                                     December 16, 2024       1 \nProbabilities-Informed Machine Learning  Mohsen Rashki Department of Architectural Engineering, University of Sistan and Baluchestan, Zahedan, Iran Email address: Mrashki@eng.usb.ac.ir  Abstract Machine learning (ML) has emerged as a powerful tool for tackling complex regression and classification tasks, yet its success often hinges on the quality of training data. This study introduces an ML paradigm inspired by domain knowledge of the structure of output function, akin to physics-informed ML, but rooted in probabilistic principles rather than physical laws. The proposed approach integrates the probabilistic structure of the target variable—such as its cumulative distribution function—into the training process. This probabilistic information is obtained from historical data or estimated using structural reliability methods during experimental design. By embedding domain-specific probabilistic insights into the learning process, the technique enhances model accuracy and mitigates risks of overfitting and underfitting. Applications in regression, image denoising, and classification demonstrate the approach's effectiveness in addressing real-world problems.  Keywords: Probabilistic Machine Learning, Reliability Theory, Informed Machine Learning, Support Vector Machine (SVM), Artificial Neural Networks (ANN), Optimization-Based Learning, Uncertainty.  1. Introduction Over the last decade, Machine Learning (ML) has emerged as a transformative tool for solving complex prediction and classification problems [1], [2]. As a natural evolution of traditional regression methods [3], ML models such as Support Vector Regression (SVR) [4] and Artificial Neural Networks (ANN) [5] have been developed to handle non-linear relationships and high-dimensional datasets [6] with increasing accuracy and robustness.  For instance, SVR has proven to be a robust regression tool because it can generalize well with limited data and capture nonlinear relationships using kernel functions [7].  Similarly, ANN, inspired by the neural architecture of the human brain, has become foundational to ML [5]. Typically, these methods use inputs (X) and outputs (Y) to construct surrogate models that aim to minimize the difference between the predicted and actual output values. These models have found applications across diverse fields, including engineering, medicine, and economics, demonstrating their versatility and potential [8], [9], [10]. In many real-world applications, additional prior information regarding the output model can be leveraged to enhance its accuracy and robustness [11] [12]. For instance, in physical systems, knowledge of the governing laws of physics has been successfully incorporated into ML by developing physics-informed neural networks (PINNs) [13], leading to improved efficiency and accuracy in prediction tasks [14]. In addition to physical laws, probabilistic information about the structure of the problem may also exist in practical scenarios [15]. Moreover, in many systems, the output variable is inherently probabilistic, necessitating models to approximate the probabilistic structure of the output [16]. Methods such as the Gaussian Processes (GPs) [17] [18], Bayesian neural networks [19], quantiles [20] and generative modeling [21] have shown great potential for integrating such probabilistic arXiv:2412.11526                                                                                                     December 16, 2024       2 \nissues in ML. Furthermore, as demonstrated in [15], [22] and [23], probabilities-informed modeling enhances robust decision-making and ensures reliable predictions, particularly in complex systems (see also [24] and [25] where the concept of failure probability is utilized to improve the training process in PINNs). These findings underscore the critical role of leveraging the complete probabilistic structure of the output function during model selection and training in ML. This paper develops probabilities-informed modeling as an explicit framework for incorporating probabilistic information into ML to enhance its effectiveness. In the next section, we discuss how probabilistic information can be effectively utilized to select an appropriate model for prediction tasks and propose strategies for deriving the probabilistic structure of the output function Y. Section 3 introduces a framework for training ML models by integrating probabilistic information. In Section 4, we validate the proposed method through various applications, and finally, Section 5 presents the conclusions and potential future directions.  2. Problem Statement and Study Motivation In ML, overfitting and underfitting are fundamental challenges that impair model generalization and prediction reliability. Overfitting occurs when a model captures noise or overly specific patterns in the training data, reducing its ability to generalize to unseen data. Conversely, underfitting arises when the model fails to capture the inherent complexity of the data, leading to suboptimal predictive performance. These issues are particularly acute in cases where the target variable Y exhibits complex or probabilistic behavior, such as multi-modal distributions or inherent uncertainties. Relying solely on error-based metrics like Root Mean Squared Error (RMSE) for model selection often leads to the choice of an inadequate model, either due to overfitting or underfitting, failing to represent the underlying data distribution accurately. For instance, consider a dataset with 𝑛=10 input features 𝐗=[𝑋!,𝑋\",…,𝑋!#] and corresponding outputs 𝒀. The scatter plot of data is presented in Fig. 1.  The objective is to train a predictive model 𝑌+=𝑓(𝑿) to approximate the relationship between 𝑋 and 𝑌, explore polynomial models of order 𝑖 (e.g., 𝑖=1,2,…,5).  When the dataset (𝑿,𝒀) is the sole source of information and the problem is treated as a black-box problem (e.g., in the absence of domain information about the output Y), RMSE is typically employed to evaluate model performance and select the optimal polynomial order.  In this scenario, selecting a polynomial of order 𝑖=2 (Model #2) might appear reasonable due to its balance between simplicity, and error minimization, leaving room for ambiguity regarding how well the model captures the underlying structures of 𝑌.  \n   Fig. 1. Illustration of available data alongside fitted polynomial models of varying complexity  -0.20 0.20.40.60.8 1X\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1Y\nData\n-0.20 0.20.40.60.8 1 1.2X\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2Y\nDataOrder 1Order 2\n-0.20 0.20.40.60.8 1 1.2X\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2Y\nDataOrder 4Order 5arXiv:2412.11526                                                                                                     December 16, 2024       3 \n  2.1.  The Value of Probabilities-Informed Modeling Recent research has demonstrated that incorporating probability-informed modeling improves mobile network traffic analysis [15] and time-dependent reliability results [23] [22] in intricate systems.  In the context of the previously discussed problem, we explored the limitations of selecting a model based solely on the dataset (𝑿,𝒀). In this subsection, suppose a scenario in which some additional probabilistic information about X and 𝑌 is available. Let 𝐗∈ℝ$ follow a known distribution 𝑃%(𝒙) and 𝑌 be governed by a conditional probability distribution 𝑃&(𝑦|𝐗=𝒙). Assume the marginal distributions 𝑃% and 𝑃&, as well as the empirical cumulative distribution function (CDF) of 𝒀, are available as shown in Fig. 2.  \n Fig. 2. The dataset (𝑿,𝒀) and full probabilistic structure of 𝑌  This additional information enables us to evaluate model suitability beyond RMSE. Consider two polynomial models: 𝑓\"(𝒙) (order 2) and 𝑓'(𝒙) (order 5), as shown in Fig. 3. Using the known distribution 𝑃%(𝒙), we can generate a large number of synthetic samples of X (uniform samples in the mentioned case), compute the corresponding 𝑌+ values for each model, and construct histograms or empirical CDFs of the predicted 𝑌. Let 𝐹+(!(𝑦) and 𝐹+(\"(𝑦) denote the empirical CDFs of predictions from the two models, and let 𝐹&(𝑦) be the empirical CDF of the 𝑌 (i.e., available probabilistic information). By comparing 𝐹+(!(𝑦) and 𝐹+(\"(𝑦) against 𝐹&(𝑦), we can assess how well each model captures the probabilistic structure of 𝑌. These explanations are illustrated in Fig 3 in detail. Fig. 3-D illustrates that the predictions from 𝑓\"(𝒙) fail to match 𝐹&(𝑦), despite achieving a reasonable RMSE in the training process, indicating that Model #2 underestimates the true complexity of\t𝑌. In contrast, predictions from 𝑓'(𝒙) align closely with 𝐹&(𝑦), suggesting that polynomial order 5 better captures the probabilistic structure of 𝑌 while maintaining acceptable RMSE. \n0 0.2 0.4 0.6 0.8 1\nX\n-1\n-0.5\n0\n0.5\n1\nY\nScatter Plot\n0 0.2 0.4 0.6 0.8 1\nX\n0\n0.5\n1\nCDF\nCumulative Distribution Function of X\n0 0.2 0.4 0.6 0.8 1\nCDF\n-1\n-0.5\n0\n0.5\n1\nY\nCumulative Distribution Function of YarXiv:2412.11526                                                                                                     December 16, 2024       4 \n Fig. 3. (A) Two candidate prediction models, 𝐹+(!(𝑦) and 𝐹+(\"(𝑦); (B) Sample generation based on the distribution of X and evaluation of 𝑌+\t, (C) Derivation of the PDF of 𝑌+\t, and, D) Calculation of the CDF of 𝑌+  The proposed explanation underscores that incorporating probability information about the structure of 𝑌 provides a robust framework for model selection. By incorporating the known marginal distribution 𝑃%(𝒙) and the conditional distribution 𝑃&(𝑦|𝐗=𝒙), we transform the problem from a purely data-driven black box into a semi-informed problem. By combining this probabilistic information with conventional metrics like RMSE, as shown in Fig. 4, we can identify models that not only minimize error but also faithfully represent the underlying probabilistic data structures (e.g., Polynomial order 5, in this example).  \n0 0.5 1\nX\n-1\n-0.5\n0\n0.5\n1\nY\nScatter Plot\n(Fitted) Order 5\n(Fitted) Order 2\nInput data\n0 0.5 1\nX\n0\n0.5\n1\nProbability Density\nHistogram of X\n0 1 2 3\nProbability Density\n-1\n-0.5\n0\n0.5\n1\nY\nHistogram of Y\nFull data\nData from order 5\nData from order 2\n0 0.5 1\nX\n0\n0.5\n1\nCDF\nCumulative Distribution Function of X\n0 0.5 1\nCDF\n-1\n-0.5\n0\n0.5\n1\nY\nCumulative Distribution Function of Y\nFull data\nOrder 5\nOrder 2\n(B) \n(C) \n(D) \n0 0.2 0.4 0.6 0.8 1\nX\n-3\n-2\n-1\n0\n1\n2Y\nPolynomial order 2\nPolynomial order 5\nInput data\n(A) \n Sampling from fitted functions\n arXiv:2412.11526                                                                                                     December 16, 2024       5 \n Fig. 4. Model selection based on the dataset (𝑿,𝒀) and the probabilistic behavior of 𝒀   2.2. Deriving the Probabilistic Structure of Output 𝒀 In this section, we focus on deriving the CDF 𝐹&(𝑦) as the probabilistic structure of 𝑌. The derived CDF allows us to identify regions where 𝑌 is more likely or less likely to occur, and therefore serves as a benchmark for evaluating candidate models 𝑌+=𝑓(𝐗).  By comparing the empirical CDF of model predictions 𝐹+&(𝑦) with 𝐹&(𝑦), we can determine how well the model captures the true probabilistic behavior of 𝑌.   2.2.1. Empirical Derivation of the CDF of 𝒀 (In the Presence of Data Y) In many real-world scenarios, empirical data enable the estimation of the\t𝑃%(𝒙) and CDF 𝐹&(𝑦), even when the relationship between 𝑌 and its influencing factors 𝐗 is unknown or sparsely documented. For instance: • Engineering: The number of cycles to failure for materials under repeated loading is empirically characterized, allowing CDF estimation, though its dependence on material and loading parameters remains unclear. • Economics: Selling time or price of properties is recorded, enabling CDF estimation despite limited knowledge of influencing market and property features. • Manufacturing: Time-to-failure for components is measured empirically, facilitating CDF estimation, while its relationship to production and operational factors remains uncertain. • Energy Systems: Energy consumption patterns are observed and modeled empirically, though the connection to temperature, seasonality, and human behavior often lacks data and explicit definition. • Medical Research: Survival times of patients are empirically modeled, though their relationship with treatment and patient-specific factors is complex and data-limited. \n             \n Probability-Informed Model Selection \nAvailable/Attainable Information \n      Selected Prediction model \n0 0.5 1X\n-1\n0\n1Y\n0 0.5 1X\n-1\n0\n1Y\n 0 0.5 1\nX\n-1\n0\n1Y\n0 0.5 1\nProbability Density\n-1\n0\n1Y\n0 0.5 1\nCDF\n-1\n0\n1Y\n0 0.5 1-1\n0\n1Y\nDataOrder 5\n0 0.5 1-1\n0\n1Y\nDataOrder 4\n0 0.5 1\n-1\n0\n1Y DataOrder 2\n0 0.5 1\n-1\n0\n1Y DataOrder 1\nX \n X \n X \n X arXiv:2412.11526                                                                                                     December 16, 2024       6 \nThese examples highlight scenarios where 𝐹&(𝑦) can be estimated directly, while the functional dependency on 𝐗 requires innovative modeling approaches. Empirical data in these cases are often derived from observed measurements or experiments, where the output variable 𝑌  is recorded. For a set of thresholds {𝑦*}*+!, (where 𝑦* represents a specific value within the range of 𝑌), 𝐹&(𝑦*)=ℙ(𝑌≤𝑦*) can be estimated directly from the data using non-parametric methods, such as the empirical CDF: 𝐹&(𝑦*)=!$∑𝐼(.#/.$)$1+! ,\t     (1) where 𝑦!, 𝑦\",…,\t𝑦$ are the observed values of 𝑌, and 𝐼(.#/.) is the indicator function, equal to 1 if 𝑦1≤𝑦* and 0 otherwise. Then, the final CDF 𝐹&(𝑌) for any 𝑦 can be expressed as: 𝐹&(𝑦)=@0\t\t\t\t\t\t\t\t\t\t\t\tif\t𝑦<𝑦21$,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t𝐹&(𝑦*)\t\tfor\t𝑦*≤𝑦<𝑦*3!(interpolated),1\t\t\t\t\t\t\t\t\t\t\t\tif\t𝑦≥𝑦245,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t      (2) where 𝑦21$ and 𝑦245 are the minimum and maximum thresholds.   2.2.2. CDF Estimation of 𝒀 (In the Absence of Data Y) Through Structural Reliability Theory  In cases where empirical data for 𝑌 is unavailable, the CDF can still be estimated by leveraging a properly designed set of experiments (DOE) and employing structural reliability methods. These approaches provide probabilistic insights into 𝑌 based on the known or assumed distributions of input variables 𝐗. In this context, the key challenge lies in deriving reliable probabilistic results while ensuring the sampled data are designed to adequately represent the variability and dependency structure of 𝐗, as required by the DOE. Let 𝐗=[𝑋!,𝑋\",…,𝑋$] represent the vector of input variables. Suppose 𝐗 is distributed according to a joint probability density function 𝑃%(𝒙). Then, the CDF of 𝑌, which gives the probability that Y takes a value less than or equal to 𝑦, is given by: 𝐹&(𝑦)=ℙ(𝑌≤𝑦)=∫𝑃&(𝑦)𝑑𝑦.67 =∫𝑃%(𝒙)𝑑𝒙\t&/. .       (3) Equation (3) represents a well-known probability integral in structural reliability theory and can be used to derive probabilities for required thresholds {𝑦*}*+!,.  Recognizing that estimating extremely small probabilities can be disregarded in this context (as accurately predicting extreme values may not be necessary), various numerical and approximate methods from structural reliability theory can be utilized to estimate 𝐹&(𝑦) (e.g., Monte Carlo simulation [26], Importance sampling [27], Subset simulation [28], Soft Monte Carlo simulation [29], etc.). Notably, the same input data 𝐗 can serve a dual purpose: it supports reliability analysis and simultaneously provides a foundation for training models based on the dataset (𝑿,𝒀) (See Fig. 5). As a straightforward approach, random sampling based on the Monte Carlo method (MCS) can be used to estimate the CDF and desired dataset for proper machine training.  arXiv:2412.11526                                                                                                     December 16, 2024       7 \n Fig. 5. In the absence of empirical data Y, a dual-purpose DOE is required to construct an appropriate dataset (𝑿,𝒀) and to derive the CDF of 𝑌 using a reliability method (estimating very small probabilities is unnecessary when precise prediction of extreme values is not essential)  3. Probabilities-Informed Machine Learning (PRIML) The core idea of Physics-Informed Machine Learning (PIML) is to incorporate physical laws or constraints directly into ML models to enhance their predictive accuracy and consistency with real-world phenomena. Noting the point that the value of probabilities-informed modeling has formerly been discussed for specific applications (see [15], [23] and [22]), this study presents Probabilities-Informed Machine Learning (PRIML) as a generalized framework that integrates probabilistic knowledge into ML, focusing on both accuracy and capturing the true distribution of the data. As a result, unlike traditional models that prioritize minimizing prediction errors (e.g., RMSE), PRIML introduces a probabilistic constraint to ensure the predicted outputs 𝑌+ align with the distributional properties of the true outputs 𝑌. This is achieved through two complementary components: • RMSE: To minimize individual prediction errors and achieve accuracy at the point level. • CDF-based distance measure: To align predicted and true distributions of output 𝑌, ensuring consistency with the underlying probabilistic structure. Using the proposed insight, developments in reliability theory can be employed to train better models, whether empirical (probabilistic) data is available or when dealing with complex black-box problems. By leveraging reliability-based approaches, we can obtain more reliable results that provide critical insights for improving model accuracy, stability, and overall performance, even in the absence of physical constraints or detailed models.  3.1. Mathematical Formulation  Let 𝑿∈ℝ$ represent the input features and 𝑌∈ℝ represent the output variable, which follows a known probability distribution 𝑃&(𝑦). The goal is to train a model 𝑓8(𝑋) with parameters 𝜽, such that: 1. 𝑓8(𝑋) accurately predicts 𝑌, minimizing individual prediction errors (low RMSE) 2. The distribution of 𝑓8(𝑋), denoted 𝑃+&(𝑦), matches the true distribution 𝑃&(𝑦). To achieve this, we define a loss function that balances point-wise accuracy and distributional alignment: ℒ(𝜃)=𝛼∙ℒ9:;:(𝜃)+𝛽∙ℒ<=>?(𝜃),      (4) where ℒ9:;:(𝜃) represents the data-driven loss term that focuses on minimizing prediction errors (e.g., mean squared error denoted as MSE) and ℒ<=>?(𝜃)=ℒ<=>?Y𝑃&(𝑦),𝑃+&(𝑦),Z quantifies the alignment between the predicted and true cumulative distributions. The coefficients 𝛼 and 𝛽 \n 0 0.5 1\nX\n-1\n0\n1Y\n!!(#\")=ℙ('≤#\") \n!!(##)=ℙ('≤##) \n!!(#$)=ℙ('≤#$) \nThe final CDF !!(#) \n The dataset (%,') \n                  ? arXiv:2412.11526                                                                                                     December 16, 2024       8 \nbalance these terms, 𝛼 emphasizing point-wise prediction accuracy and 𝛽 prioritizing distributional alignment to capture the system's probabilistic behavior. The data-driven loss term ensures point-wise prediction accuracy: ℒ9:;:(𝜃)=!@∑(𝑌1−𝑓8(𝑋1))\"@1+! ,      (5) where 𝑁 is the number of training samples. To measure the alignment between the true distribution 𝑃&(𝑦) and the predicted distribution 𝑃+&(𝑦), we define the cumulative distribution-based distance as loss function ℒ<=>?(𝜃). Let the empirical CDFs of 𝑌 and 𝑌+ (the predictions) be 𝐹&(𝑦)=ℙ(𝑌≤𝑦), and 𝐹+&(𝑦;𝜃)=ℙ(\t𝑓8(𝑋)≤𝑦). Then the loss ℒ<=>?(𝜃) is expressed as: ℒ<=>?(𝜃)=∫^𝐹&(𝑦)−𝐹+&(𝑦;𝜃)^𝑑𝑦767 .      (6) In practice, the integral is approximated using a finite set of thresholds {𝑦!, 𝑦\",…,\t𝑦*}: ℒ<=>?(𝜃)≈∑^𝐹&(𝑦)−𝐹+&(𝑦;𝜃)^*A+! .      (7) Therefore, the proposed loss function can be replaced with alternative CDF distance measures (𝐷(𝐹&,𝐹+&)) ℒ<=>?(𝜃)=\t𝐷(𝐹&,𝐹+&),      (8) where the Bhattacharyya Distance: 𝐷BY𝐹&,𝐹+&Z=−log∫b𝐹&(𝑦)𝐹+&(𝑦)𝑑𝑦       (9) or Kullback-Leibler (KL) divergence 𝐷,CY𝐹&,𝐹+&Z=∫𝐹&(𝑦)logcD%(.)DE%(.)d𝑑𝑦,       (10) or, based on the in-hand problem and employed machine learning approach, other distance measures (e.g., 𝐷FGY𝐹&,𝐹+&Z as Jensen-Shannon Divergence) [30] [31] [32] can be used to present the total loss function. A comprehensive overview of potential loss functions and evaluation measures, which may apply to the proposed approach, can be found in [16]. In the general case where other prior knowledge is involved, the total loss function integrates terms for proposed objectives (e.g., prediction accuracy and distributional consistency) and other constraints. For instance, the loss function with physics-based constraints can be expressed as: ℒ(𝜃)=𝛼∙ℒ9:;:(𝜃)+𝛽∙ℒ<=>?(𝜃)+𝛾∙ℒ<HIJKLJ(𝜃),       (11) where 𝛾 is a weighting factor to balance the contributions of the last term and ℒ<HIJKLJ(𝜃) is the physics-based loss term that penalizes violations of physical constraints or governing equations ℒ<HIJKLJ(𝜃)=!@∑fℛ(𝑌+,𝐗,𝜃)f\",@1+!        (12)  in which ℛ is a residual function encoding the physical laws. Fig. 6 describes the flow of information in the proposed approach. \n Fig. 6. The flow of information in the probabilities-informed machine learning \n \nProbabilistic knowledge  \nOther forms of prior knowledge (e.g., Physics, Rules, etc.) \nMachine Learning \n Solution  !\":$→&'  \nProblem  !:$→&  \nEmpirical data \n Probabilistic structure of Y  \nStructural reliability \nDataset ((,*)  arXiv:2412.11526                                                                                                     December 16, 2024       9 \n3.2. Pseudo-Algorithm and Architecture of Model  The required steps of the proposed approach are as follows: 1. Data Preparation and Distribution Estimation o Input: X (features), 𝑌 (target). o If empirical data is available:  o Derive 𝑃%(𝒙) and 𝐹&(𝑦) using empirical observations. o If no empirical data is available:  o Assume a probability distribution 𝑃%(𝒙) for input X and use a reliability approach to estimate the CDF of 𝑌 and augment the dataset by generating corresponding samples for X.  2. Model Initialization o Define a machine learning model 𝑓8(𝑋) with trainable parameters 𝜃. o Generate samples from 𝑃%(𝒙), predict the output 𝑓8(𝑋)=𝑌+, and derive the CDF 𝐹+&. o Set the loss function to include: § ℒ9:;:(𝜃) for point-wise accuracy (e.g., mean squared error). § ℒ<=>?(𝜃) for probabilistic consistency (e.g., 𝐷BY𝐹&,𝐹+&Z). 3. Loss Function Definition o Formulate the total loss: ℒ(𝜃)=𝛼∙ℒ9:;:(𝜃)+𝛽∙ℒ<=>?(𝜃). and any additional constraints derived from prior knowledge, if available. 4. Model Training o Optimize\t𝜃 by minimizing the total loss ℒ(𝜃): 𝜃∗=\targmin8ℒ(𝜃)\t\tUse an optimization method to update 𝜃.\t5. Model Evaluation and Validation o Validate the model using test data: § Check point-wise accuracy with metrics like RMSE or MSE. § Verify probabilistic alignment by comparing 𝐹&(𝑦) and 𝐹+&(𝑦;𝜃) using metrics like Bhattacharyya distance, KL divergence, Wasserstein distance, etc. In the case of successful optimization, the trained model 𝑓8(𝑋) predicts 𝑌 with both point-wise accuracy and alignment to the true distribution 𝐹&(𝑦). In this study, SVR and ANN are utilized to demonstrate how the proposed approach can enhance the prediction capabilities of learning machines.  Based on the proposed implementations, Fig. 7 illustrates the architecture of the neural network method, incorporating the probabilistic structure of Y as prior knowledge, where the weights W and bias terms B in the network are considered as hyperparameters. It is worth mentioning that the number of layers, neurons, and other architectural details can also be treated as parameters for optimization. Table 1 presents some important hyperparameters for SVR as a learning machine. These parameters control the complexity, regularization, and kernel behavior of the model, which directly influence the model's predictive accuracy. Like the neural network-based approach, the learning process requires running the SVR model in each optimization process to derive the hyperparameters 𝜽 ={𝒦, ℬ, 𝜖, ℱ} that minimize the cost in Eq. (4).    arXiv:2412.11526                                                                                                     December 16, 2024       10 \n Fig. 7. Architecture of the Probabilities-Informed Neural Network. Other learning models (e.g., SVR with the hyperparameters listed in Table 1) can be embedded into the proposed framework as alternatives to Neural Networks  Table 1. Potential hyperparameters for SVR optimization using probabilities knowledge Hyperparameter Sign Description KernelScale 𝒦 Determines the kernel's feature mapping scale. \nBoxConstraint ℬ Regularization controlling the trade-off between training accuracy and model smoothness. Epsilon 𝜖 Margin of tolerance for prediction errors. KernelFunction ℱ Kernel type for feature mapping.  4. Verification and Performance Evaluation To validate the proposed ML approach, we test it against several benchmark problems, encompassing both synthetic and real-world datasets. The Monte Carlo simulation was used to derive the CDFs while ANN and SVR were employed as the ML model 𝑓8(𝑋). The optimization is performed by tuning the key hyperparameters 𝜃 using Adam and Bayesian optimization for ANN and SVR, respectively.  \n \n \nparameters ! =[W,B] \nX1 \nXn !\"!($;&) \nData loss \n(\"#)!!,!\"!+  \nProbabilities loss \nOptimization: \"∗= argmin\"ℒ(\") ℒ(\")=-∙ℒ#$%$(\")+0∙ℒ&'()(\")\n… \n… \n … \n… \n… \nPrior knowledge:  !!  X ~\t#!(%)  \n Loss \n<ϵ ? Stop Yes \nLearning Machine (ANN, SVR, etc.) arXiv:2412.11526                                                                                                     December 16, 2024       11 \nThe capabilities of Probabilities-Informed Neural Networks (PRINN) are investigated in comparison to traditional ANNs for a Structural Health Monitoring (SHM) problem. Additionally, SVR optimized using the proposed approach is validated on benchmark image-denoising and classification problems.  4.1. Verification of PRINN  The proposed PRINN framework (see Fig. 7) is applied to a synthetic structural health monitoring (SHM) dataset. The goal is to predict damage levels under varying structural and environmental conditions. Datasets with varying sample sizes were synthetically generated. The inputs in these datasets included dynamic load amplitude (𝐹N), vibration frequency (𝑓O), material degradation factor (𝑀N), and crack density (𝐶P) represented measurable structural parameters, while the outputs corresponded to normalized damage levels.  For verification purposes, the damage level (𝐷), can be synthesized as a function of the input variables in the normalized range of [-1, 1] with added Gaussian noise 𝜐~𝒩(0,0.1\") to represent the real-world measurement variability: 𝐷=𝑆𝑖𝑛(𝜋𝐹P)∙log(|𝑓O|+1)+0.8∙𝑒6|R&6#.'|∙𝑐𝑜𝑠(2𝜋𝐶P)−0.5∙ D&∙('!3R&!3|U&|+0.2∙sin(10∙𝐹P)∙sin(5∙𝑓O)+𝜐.       (13) To model the nonlinear relationship between the input features and the damage levels, a feedforward neural network with two hidden layers, each with 120 neurons, followed by a hyperbolic tangent (tanh) activation function to introduce nonlinearity. The network was trained using a total of 1500 iterations with a learning rate of 0.01 for both the baseline model and the proposed approach. The loss function of Eq. (4) is defined as: ℒ(𝜃)=𝛼∙ℒ9:;:(𝜃)+(1−𝛼)∙𝐷FGY𝐹&,𝐹+&Z,      (14) where ℒ9:;:(𝜃) represents the data-driven loss, and 𝐷FGY𝐹&,𝐹+&Z is the Jensen-Shannon Divergence between the predicted and actual cumulative distributions (see Section 2.2). The parameter Alpha (𝛼) is considered dynamic with respect to iterations as shown in Fig 8.  \n Fig. 8. Dynamic parameter Alpha (𝛼): Adapting the weighting factor to balance probability and data loss during training  Assuming that the probability distribution of input variables and CDF information on damage level from empirical observation is available, we trained ANN with different input samples n as n=5 and n=100.  In the case of n=5, due to the very small input size, the baseline model achieves near-zero training error but struggles with poor generalization, as shown in Fig. 9. This failure to capture the behavior \n0 500 1000 1500\nIteration\n0\n0.2\n0.4\n0.6\n0.8\n1\nAlphaarXiv:2412.11526                                                                                                     December 16, 2024       12 \nof the original model highlights significant overfitting. In contrast, the probabilities-informed method achieves near-perfect performance by emphasizing alignment between the predicted and actual distributions (see Fig. 10). This alignment is crucial for accurately capturing uncertainty in real-world applications. Notably, training a model with only 5 samples while achieving a correlation coefficient of 92% underscores the importance of incorporating probabilistic information into the training process of PRINN.  \n Fig. 9. Comparison of performance metrics between the proposed PRINN approach and baseline models in the structural health monitoring example with n=5  \n Fig. 10. Comparison of output CDFs between the proposed PRINN approach and baseline models in the structural health monitoring example with n=5  Fig. 11 compares the performance metrics of the proposed PRINN approach and baseline models for n=100. The results indicate that increasing the input sample size improves the accuracy of both models. For the baseline model, the correlation coefficient increases to 58%, reflecting some improvement in capturing the probabilistic information of Y (See Fig 12). Nonetheless, given the low correlation coefficient, the model's accuracy may not be adequate for real-world applications. \n-4 -3 -2 -1 0 1 2 3 4\nTrue Values\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5Predicted Values\nPRINN Model\nBaseline Model\nIdeal Line\n-4 -3 -2 -1 0 1 2 3 4\nValue\n0\n0.2\n0.4\n0.6\n0.8\n1\nCDF\nTrue Data\nPRINN Model\nBaseline ModelarXiv:2412.11526                                                                                                     December 16, 2024       13 \nIn contrast, the PRINN approach achieves a correlation coefficient of 96.6%, demonstrating the high accuracy of the model when a balanced number of input samples and probabilistic information of the output Y are utilized. These findings emphasize the utility of the probabilities-informed method in scenarios where capturing the probabilistic characteristics of predictions is critical. By integrating probabilistic metrics during optimization, the proposed approach effectively balances pointwise error minimization and distributional fidelity, providing a significant advantage over conventional methods in applications requiring robust representation of uncertainty.  \n Fig. 11. Comparison of performance metrics between the proposed PRINN approach and baseline models in the structural health monitoring example with n=100  \n Fig. 12. Comparison of output CDFs between the proposed PRINN approach and baseline models in the structural health monitoring example with n=100  4.2. Verification of Probabilities-Informed Support Vector Machine This section explores the scenario where SVR is used as the learning machine. For verification purposes, we compare the performance of three SVR models with the same training points: \n-3 -2 -1 0 1 2 3\nTrue Values\n-4\n-3\n-2\n-1\n0\n1\n2\n3Predicted Values\nPRINN Model\nBaseline Model\nIdeal Line\nMSE RMSECorrelation\nMetrics\n0\n0.2\n0.4\n0.6\n0.8\n1\nValues\n0.053\n0.735\n0.231\n0.857\n0.966\n0.581\nPRINN ModelBaseline Model\n-4 -3 -2 -1 0 1 2 3\nValue\n0\n0.2\n0.4\n0.6\n0.8\n1\nCDF\nTrue Data\nPRINN Model\nBaseline ModelarXiv:2412.11526                                                                                                     December 16, 2024       14 \n1. Baseline model (SVR with default parameters): This model is trained using the standard hyperparameter settings provided by Matlab's SVR implementation (version 2024a). It serves as a reference to assess the improvements achieved by the proposed approach. 2. Proposed probabilities-informed approach: This model leverages a custom loss function defined as Eq. (4).  3. RMSE-optimized approach: A model trained solely to minimize the data-driven loss term ℒ(𝜃)=ℒ9:;:(𝜃)     (15) This scenario assesses the impact of incorporating probabilistic information on model performance.  4.2.1. Image Denoising Example In this example, we evaluate the effectiveness of the probabilities-informed approach in image denoising. The denoising task involves reconstructing the original (noise-free) image from its noisy counterpart. To evaluate the proposed method, we use the \"Cameraman\" image, a well-known image available in Matlab's library. Gaussian noise with a mean of zero and a standard deviation of 0.1 (ν\t∼N(0,0.12)) is added to the image to simulate real-world noise. The original and noisy images are shown in Fig. 13. The task involves using the proposed probabilities-informed trained model to remove noise from images and comparing the results with two alternatives: a baseline SVR model trained using default Matlab settings, and a model optimized solely based on a data-driven loss function.  Instead of estimating the CDF of the output Y (which may not be directly available for the original image) during the training process, we estimate the CDF of the training samples. The objective is to identify a model that not only achieves accurate denoising but also captures the probabilistic structure inherent in the training samples. This allows us to demonstrate how the inclusion of probabilistic optimization in the proposed approach improves the image quality compared to traditional methods. In this denoising process, the proposed probabilities-informed model leverages a custom loss function defined as: ℒ(𝜃)=0.3∙ℒ9:;:(𝜃)+0.7∙𝐷BY𝐹&,𝐹+&Z.  The effectiveness of the denoising is assessed using two key metrics: • PSNR (Peak Signal-to-Noise Ratio): A metric that compares the quality of the denoised image, where higher values indicate better image quality. • SSIM (Structural Similarity Index): This measures how similar the denoised image is to the original image, focusing on structural content. We have used the Bayesian optimization approach to obtain hyperparameters of SVR. The denoising results and corresponding metrics are presented and plotted in Figs. 14 and 15.  The PSNR for the probabilities-informed SVR model is higher compared to the baseline SVR and RMSE-optimized model, indicating that the denoised image has fewer distortions and artifacts. Similarly, the SSIM for the probabilities-informed SVR model is also higher than the other two models, demonstrating that the denoised image preserves structural details better. This metric is crucial because it focuses on the perceptual quality of the image, and our approach provides better structural fidelity than the other models. The results reveal the potential of the proposed approach for image-denoising problems, demonstrating that probabilistic metrics can be seamlessly integrated into the objective function.   arXiv:2412.11526                                                                                                     December 16, 2024       15 \n Fig. 13. The original and noised image of example 2  \n Fig. 14. The denoised images of three SVR models  \n Fig. 15. The denoising metrics of three SVR models   \nOriginal Image\n50 100 150 200 250\n50\n100\n150\n200\n250\nNoisy Image\n50 100 150 200 250\n50\n100\n150\n200\n250\nDenoised (Proposed approach)\n50 100 150 200 250\n50\n100\n150\n200\n250\nDenoised (Baseline)\n50 100 150 200 250\n50\n100\n150\n200\n250\nDenoised (RMSE optimized)\n50 100 150 200 250\n50\n100\n150\n200\n250\nPSNR\nProposed approach\nBaseline\nRMSE optimized\n0\n5\n10\n15\n20\n25PSNR (dB)\nSSIM\nProposed approach\nBaseline\nRMSE optimized\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45SSIM\nCDF Distance\nProposed approach\nBaseline\nRMSE optimized\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7CDF DistancearXiv:2412.11526                                                                                                     December 16, 2024       16 \n4.2.2. Moderate/High Dimensional Classification Example As the last example, we investigate the classification performance of the proposed approach on the “Ionosphere” dataset, which is a well-known dataset in ML. The corresponding dataset contains 351 instances, each described by 34 numeric features, and the target variable is binary. The objective is to train classification models to predict whether the radar return is from the ionosphere (good) or not (bad). The dataset has an approximately balanced distribution of the two classes, which makes it suitable for classification tasks. The problem is firstly solved by the three Support Vector Classification (SVC) models while 80% of the data is used for testing, and the remaining 20% is used for training (poor X and Y dataset). The obtained confusion matrix and classification error of trained models are reported in Figs 16-17, and more accurate discussions of results, and performance metrics for three considered models based on the confusion matrix are summarized in Table 2.  \n Fig. 16. Confusion matrix of the three SVC models for the Ionosphere dataset  \n Fig. 17. Classification error and CDF distance of the three SVC models for the Ionosphere dataset  Table 2. Performance metrics of three SVC models Model Accuracy Precision Recall F1-Score Proposed approach 0.91 0.90 0.91 0.91 Baseline 0.83 0.85 0.78 0.79 RMSE optimized 0.65 0.82 0.51 0.42  \n0 1\nPredicted Class\n0\n1 True Class\nBaseline model\n59\n7\n41\n173\n0 1\nPredicted Class\n0\n1 True Class\nProposed approach\n88\n12\n12\n168\n0 1\nPredicted Class\n0\n1 True Class\nRMSE optimized model\n3 97\n180\nCDF Distance Classification Error0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35Metric Value\nProposed approachBaselineRMSE optimizedarXiv:2412.11526                                                                                                     December 16, 2024       17 \nIn the case of the Accuracy metric, the baseline SVC model achieves an accuracy of 0.8286, while the RMSE optimized model achieves a much lower accuracy of 0.6536, indicating that optimizing for RMSE alone does not lead to good classification results and fails to align to maximize classification performance. In comparison, the proposed probabilities-informed model demonstrates a notable improvement with an accuracy of 0.9143, marking an increase of nearly 8% over the default model. This suggests that the optimization method significantly enhances the model's ability to classify instances correctly.  In the case of the Precision metric, the Baseline model achieves a precision of 0.8512, indicating that 85.12% of the instances predicted as positive are correctly classified as good. However, the RMSE-optimized model performs less effectively with a precision of 0.8249, suggesting that (as in the former case) focusing on RMSE optimization is not ideal for improving precision in classification tasks. The proposed probabilities-informed model improves precision to 0.9067, meaning that it better identifies positive instances as true positives.  In this example, the Recall metric for the Baseline model is 0.7756, meaning it correctly identifies 77.56% of the actual positive instances (good). The proposed probabilities-informed model shows a substantial improvement in recall, reaching 0.9067, which means it correctly identifies 90.67% of all actual positive instances. In contrast, the RMSE-optimized model significantly underperforms with a recall of 0.5150, missing a large proportion of the positive instances. The F1-score for the Baseline model is 0.7945, reflecting a reasonable balance between precision and recall. The proposed probabilities-informed model achieves a higher F1-score of 0.9067, indicating an improved balance between precision and recall. However, the RMSE-optimized model performs poorly with an F1-score of 0.4230, further confirming that optimizing for RMSE is not suitable for classification tasks where balancing precision and recall is essential. As the second try, 50% of the data is used for testing, and the remaining 50% is used for training. The results of the confusion matrix are presented in Fig 15. Results show that the proposed model outperforms the other models across all evaluation metrics, including accuracy, precision, recall, and F1-score. These results demonstrate the effectiveness of the proposed optimization approach in improving classification performance on the Ionosphere dataset.  \n Fig. 15. Confusion matrix of three SVC models for the Ionosphere dataset with 50% training input  5. Conclusion In this study, we demonstrated that integrating the probabilistic characteristics of the output function Y (e.g., the probability density function) into the training process of machine learning (ML) models significantly enhances model performance compared to traditional methods. This \n0 1\nPredicted Class\n0\n1 True Class\nBaseline model\n40\n4\n23\n108\n0 1\nPredicted Class\n0\n1 True Class\nProposed approach\n57\n6\n6\n106\n0 1\nPredicted Class\n0\n1 True Class\nRMSE optimized model\n2 61\n112arXiv:2412.11526                                                                                                     December 16, 2024       18 \napproach, inspired by physics-informed ML, utilizes available probabilistic insights from real-world data or estimates obtained through structural reliability methods during experimental design. In the case of using reliability methods, by assuming that the input variable X follows a specific cumulative distribution function (CDF), the corresponding CDF of Y can be derived (recognizing that estimating very small probabilities is unnecessary in this context, especially when precise prediction of extreme values is not essential), various numerical and approximate methods from structural reliability theory can be utilized to estimate 𝐹&(𝑦)). Consequently, predictive models should ensure alignment between the predicted and original output CDFs for X with the specified distribution.  To achieve this, the proposed framework optimizes ML models such (e.g., Artifical Neural Network, Support Vector Machine, and etc.) and  by minimizing divergence measures like Bhattacharyya distance between predicted and actual output distributions. Incorporating such probabilistic knowledge mitigates overfitting and underfitting by aligning model predictions with the statistical properties of the target variable. Real-world examples highlight the method’s effectiveness, demonstrating that embedding the probabilistic structure of outputs fosters improved generalization and robustness. These findings underscore the practical value of the proposed approach, establishing it as a robust alternative for enhancing ML model reliability in diverse applications.  6. References [1] K. Wang, P. P. Menon, J. Veenman, and S. Bennani, “Safety exploration using Gaussian process classification for uncertain systems,” Reliab. Eng. Syst. Saf., vol. 256, p. 110680, Apr. 2025, doi: 10.1016/j.ress.2024.110680. [2] C. Dang, M. A. Valdebenito, P. Wei, J. Song, and M. Beer, “Bayesian active learning line sampling with log-normal process for rare-event probability estimation,” Reliab. Eng. Syst. Saf., vol. 246, p. 110053, Jun. 2024, doi: 10.1016/j.ress.2024.110053. [3] D. R. Cox, “Regression Models and Life-Tables,” J. R. Stat. Soc. Ser. B Methodol., vol. 34, no. 2, pp. 187–220, 1972. [4] V. N. Vapnik, “The Nature of Statistical Learning Theory | SpringerLink.” [Online]. Available: /978-1-4757-2440-0 [5] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444, May 2015, doi: 10.1038/nature14539. [6] J. Kim, Z. Wang, and J. Song, “Adaptive active subspace-based metamodeling for high-dimensional reliability analysis,” Struct. Saf., vol. 106, p. 102404, Jan. 2024, doi: 10.1016/j.strusafe.2023.102404. [7] V. Cherkassky and Y. Ma, “Practical selection of SVM parameters and noise estimation for SVM regression,” Neural Netw., vol. 17, no. 1, pp. 113–126, Jan. 2004, doi: 10.1016/S0893-6080(03)00169-2. [8] W. Zheng, X. Yuan, X. Bao, and Y. Dong, “Adaptive support vector machine for time-variant failure probability function estimation,” Reliab. Eng. Syst. Saf., vol. 253, p. 110510, Jan. 2025, doi: 10.1016/j.ress.2024.110510. [9] P. Whig, B. Y. Kasula, N. Yathiraju, A. Jain, and S. Sharma, “Chapter 4 - Bone cancer classification and detection using machine learning technique,” in Diagnosing Musculoskeletal Conditions using Artifical Intelligence and Machine Learning to Aid Interpretation of Clinical Imaging, R. Kumar, M. Gupta, A. Abraham, and P. Antony, Eds., Academic Press, 2025, pp. 65–80. doi: 10.1016/B978-0-443-32892-3.00004-X. arXiv:2412.11526                                                                                                     December 16, 2024       19 \n[10] T. C. Silva, P. V. B. Wilhelm, and D. R. Amancio, “Machine learning and economic forecasting: The role of international trade networks,” Phys. Stat. Mech. Its Appl., vol. 649, p. 129977, Sep. 2024, doi: 10.1016/j.physa.2024.129977. [11] L. von Rueden et al., “Informed Machine Learning – A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems,” IEEE Trans. Knowl. Data Eng., vol. 35, no. 1, pp. 614–633, Jan. 2023, doi: 10.1109/TKDE.2021.3079836. [12] C. T. Mackay and D. Nowell, “Informed machine learning methods for application in engineering: A review,” Proc. Inst. Mech. Eng. Part C J. Mech. Eng. Sci., vol. 237, no. 24, pp. 5801–5818, Dec. 2023, doi: 10.1177/09544062231164575. [13] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang, “Physics-informed machine learning,” Nat. Rev. Phys., vol. 3, no. 6, pp. 422–440, Jun. 2021, doi: 10.1038/s42254-021-00314-5. [14] Y. Jiang et al., “Multi-fidelity physics-informed convolutional neural network for heat map prediction of battery packs,” Reliab. Eng. Syst. Saf., vol. 256, p. 110752, Apr. 2025, doi: 10.1016/j.ress.2024.110752. [15] A. Gorshenin, A. Kozlovskaya, S. Gorbunov, and I. Kochetkova, “Mobile network traffic analysis based on probability-informed machine learning approach,” Comput. Netw., vol. 247, p. 110433, Jun. 2024, doi: 10.1016/j.comnet.2024.110433. [16] H. Tyralis and G. Papacharalampous, “A review of predictive uncertainty estimation with machine learning,” Artif. Intell. Rev., vol. 57, no. 4, p. 94, Mar. 2024, doi: 10.1007/s10462-023-10698-8. [17] C. E. Rasmussen, “Gaussian Processes in Machine Learning,” Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, Tübingen, Germany, August 4 - 16, 2003, Revised Lectures. Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 63–71, 2004. [Online]. Available: https://doi.org/10.1007/978-3-540-28650-9_4 [18] J. Song, Z. Liang, P. Wei, and M. Beer, “Sampling-based adaptive Bayesian quadrature for probabilistic model updating,” Comput. Methods Appl. Mech. Eng., vol. 433, p. 117467, Jan. 2025, doi: 10.1016/j.cma.2024.117467. [19] Y. Gal and Z. Ghahramani, “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,” in Proceedings of The 33rd International Conference on Machine Learning, PMLR, Jun. 2016, pp. 1050–1059. [20] R. Koenker and G. Bassett, “Regression Quantiles,” Econometrica, vol. 46, no. 1, pp. 33–50, 1978, doi: 10.2307/1913643. [21] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein Generative Adversarial Networks,” in Proceedings of the 34th International Conference on Machine Learning, PMLR, Jul. 2017, pp. 214–223. Accessed: Dec. 12, 2024.  [22] A. K. Gorshenin and A. L. Vilyaev, “Machine Learning Models Informed by Connected Mixture Components for Short- and Medium-Term Time Series Forecasting,” AI, vol. 5, no. 4, Art. no. 4, Dec. 2024, doi: 10.3390/ai5040097. [23] H. Guo, J. Zhang, Y. Dong, and D. M. Frangopol, “Probability-informed neural network-driven point-evolution kernel density estimation for time-dependent reliability analysis,” Reliab. Eng. Syst. Saf., vol. 249, p. 110234, Sep. 2024, doi: 10.1016/j.ress.2024.110234. [24] Z. Gao, L. Yan, and T. Zhou, “Failure-Informed Adaptive Sampling for PINNs,” SIAM J. Sci. Comput., vol. 45, no. 4, pp. A1971–A1994, Aug. 2023, doi: 10.1137/22M1527763. [25] Z. Gao, T. Tang, L. Yan, and T. Zhou, “Failure-Informed Adaptive Sampling for PINNs, Part II: Combining with Re-sampling and Subset Simulation,” Commun. Appl. Math. Comput., vol. 6, no. 3, pp. 1720–1741, Sep. 2024, doi: 10.1007/s42967-023-00312-7. arXiv:2412.11526                                                                                                     December 16, 2024       20 \n[26] N. Metropolis and S. Ulam, “The Monte Carlo Method,” J. Am. Stat. Assoc., vol. 44, no. 247, pp. 335–341, Sep. 1949, doi: 10.1080/01621459.1949.10483310. [27] Y. Ibrahim, “Observations on applications of importance sampling in structural reliability analysis,” Struct. Saf., vol. 9, no. 4, pp. 269–281, Jun. 1991, doi: 10.1016/0167-4730(91)90049-F. [28] S. K. Au, J. Ching, and J. L. Beck, “Application of subset simulation methods to reliability benchmark problems,” Struct. Saf., vol. 29, no. 3, pp. 183–193, Jul. 2007, doi: 10.1016/j.strusafe.2006.07.008. [29] M. Rashki, “The soft Monte Carlo method,” Appl. Math. Model., vol. 94, pp. 558–575, Jun. 2021, doi: 10.1016/j.apm.2021.01.022. [30] J. K. Chung, P. L. Kannappan, C. T. Ng, and P. K. Sahoo, “Measures of distance between probability distributions,” J. Math. Anal. Appl., vol. 138, no. 1, pp. 280–292, Feb. 1989, doi: 10.1016/0022-247X(89)90335-1. [31] S. S. Vallender, “Calculation of the Wasserstein Distance Between Probability Distributions on the Line,” Theory Probab. Its Appl., vol. 18, no. 4, pp. 784–786, Sep. 1974, doi: 10.1137/1118101. [32] P. D’Alberto and A. Dasdan, “Non-Parametric Information-Theoretic Measures of One-Dimensional Distribution Functions from Continuous Time Series,” in Proceedings of the 2009 SIAM International Conference on Data Mining (SDM), in Proceedings. , Society for Industrial and Applied Mathematics, 2009, pp. 685–696. doi: 10.1137/1.9781611972795.59.  "
    }
}