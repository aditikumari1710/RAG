{
    "title": "Algebraic Machine Learning",
    "content": {
        "page_content": "Algebraic Machine Learning\nFernando Martin-Maroto1,* and Gonzalo G. de Polavieja 2,+\n1Algebraic AI Inc. Santa Cruz, CA, USA\n2Champalimaud Research, Lisbon, Portugal\n*martin.maroto@algebraic.ai\n+gonzalo.polavieja@neuro.fchampalimaud.org\nMarch 16, 2018\nMachine learning algorithms use error function minimization to ﬁt a large set\nof parameters in a preexisting model. However, error minimization eventually\nleads to a memorization of the training dataset, losing the ability to generalize to\nother datasets. To achieve generalization something else is needed, for example a\nregularization method or stopping the training when error in a validation dataset\nis minimal. Here we propose a diﬀerent approach to learning and generalization\nthat is parameter-free, fully discrete and that does not use function minimization.\nWe use the training data to ﬁnd an algebraic representation with minimal size and\nmaximal freedom, explicitly expressed as a product of irreducible components.\nThis algebraic representation is shown to directly generalize, giving high accuracy\nin test data, more so the smaller the representation. We prove that the number of\ngeneralizing representations can be very large and the algebra only needs to ﬁnd\none. We also derive and test a relationship between compression and error rate.\nWe give results for a simple problem solved step by step, hand-written character\nrecognition, and the Queens Completion problem as an example of unsupervised\nlearning. As an alternative to statistical learning, algebraic learning may oﬀer\nadvantages in combining bottom-up and top-down information, formal concept\nderivation from data and large-scale parallelization.\n1\narXiv:1803.05252v2  [cs.LG]  15 Mar 2018Contents\n1 Introduction 4\n2 The embedding algorithm 5\n2.1 A toy problem illustrating algebraic learning . . . . . . . . . . . . . . . . . . . . 5\n2.2 Elements of the algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Graph of the algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.4 The dual algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.5 Atomized models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.6 Trace and trace constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.7 Full and Sparse Crossing operations . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.8 Reduction operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.9 Batch training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3 Analysis of solutions 22\n3.1 Finding a class deﬁnition for the toy problem . . . . . . . . . . . . . . . . . . . 22\n3.2 Analysis of exact solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3 Analysis of approximated solutions . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.4 Memorizing and generalizing models . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.5 Small and random algebraic models imply high accuracy . . . . . . . . . . . . . 31\n4 Classiﬁcation of hand-written digits 34\n4.1 Using several master atomizations . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n5 Solving the N-Queens Completion Problem 44\n5.1 Board description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n5.2 Attack rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n5.3 Rule to add queens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n25.4 Deﬁnition of Rx and Cy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n5.5 Independence rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n5.6 Embedding an N Queens Completion game . . . . . . . . . . . . . . . . . . . . 46\n5.7 Solving the 2-blocked 8 ×8 completion problem . . . . . . . . . . . . . . . . . . 47\n5.8 Algebraic learning in larger chessboards . . . . . . . . . . . . . . . . . . . . . . . 49\n6 Discussion 53\nA Notation 56\nB Theorems 56\nC Algorithms 61\nD Exact atomizations 64\nD.1 The map index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\nD.2 Calculating atomizations for complex descriptions . . . . . . . . . . . . . . . . . 65\nE Error is smaller the higher the compression 68\nE.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\nE.2 The role of symmetries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n31 Introduction\nAlgebras have played an important role in logic and top-down approaches in Artiﬁcial Intel-\nligence (AI) [1]. They are still an active area of research in information systems, for example\nin knowledge representation, queries and inference [2]. Machine learning (ML) branched out\nfrom AI as a bottom-up approach of learning from data. Here we show how to use an algebraic\nstructure [3] to learn from data. This research programme may then be seen as a proposal to\nnaturally combine top-down and bottom-up approaches. More speciﬁcally, we are interested\nin an approach to learning from data that is parameter-free and transparent to make analysis\nand formal proofs easier. Also, we want to explore the formation of concepts from data as\ntransformations that lead to a large reduction of the size of an algebraic representation.\nWe show how to express learning problems as elements and relationships in an extended\nsemilattice algebra. We give a concrete algebraic algorithm, the Sparse Crossing, that ﬁnds\nsolutions as sets of “atomic” elements, or atoms. Learning takes place by algebraic transfor-\nmations that minimize the number of atoms. The algorithm is stochastic and discrete with no\nﬂoating point operations.\nThe algebraic approach has important diﬀerences to more standard approaches. It does\nnot use function minimization. Minimizing functions has proven very useful in ML. However,\nthe functions typically used have complex geometries with local minima. Navigating these\nsurfaces often requires large datasets and special methods to avoid getting stuck in the local\nminima. These surfaces depend on many parameters that might need tuning with heuristic\nprocedures.\nInstead of function minimization, our algebraic algorithm uses cardinal minimization. i.e.\nminimization of the number of atoms. It learns smoothly, with error rates in the test set\ndecreasing with the number of training examples and with no risk of getting trapped in local\nminima. We found no evidence of overﬁtting using algebraic learning, so we do not use a\nvalidation dataset. Also, it is parameter-free, so there is no need to prealocate parameter\nvalues like a network architecture, with the algebra growing by itself using the training data.\nWe studied Algebraic Learning in four examples to illustrate diﬀerent properties. We start\nwith the toy supervised problem of learning to classify images by whether they contain a vertical\nbar or not. The simplicity of this problem allows for analysis. We show how algebraic learning\nexplicitly ﬁnds that the positive examples are indeed those that contain a vertical bar. We\nshow also that the number of solutions with low error is astronomically large and the learning\nalgorithm just needs to ﬁnd one of them. This might also be the case in other systems, but for\nalgebras it can be demonstrated.\nAlgebraic Learning is designed to “compress” training examples into atoms and not directly\n4aimed at reducing the error. For this reason, we had to establish a relationship between\ncompression and accuracy. We found that an algebra picked at random among the ones obeying\nthe training examples has an error rate in test data inversely proportional to compression. We\ntested this theoretical result against experimental data obtained applying the Sparse Crossing\nalgorithm to the problem of distinguishing images with even number of vertical bars from\nthose with an odd number of bars. We found that Sparse Crossing is as at least as eﬃcient\nin transforming compression into accuracy and ﬁts very well the theoretical result when error\nrate is small.\nWe also tested the performance of algebraic learning in handwritten character classiﬁcation.\nWe used a single abstraction stage (a single processing layer) operating in raw data, without\npreprocessing and with a training set that contains miss-labels. Algebraic learning achieves a\ngood accuracy of about 99% when distinguishing a digit from the rest. This is done with no\noverﬁtting and even when accuracy is not an explicit target of the algorithm.\nOur last example is the N-blocked M ×M Queens Completion problem. Starting from\nN blocked queens on an M ×M chessboard, we need to place M −N queens on the board\nin non-attacking positions. We encode board and attack rules as algebraic relations, and show\nthat Algebraic Learning generates complete solutions for the standard 8 ×8 board and also in\nlarger boards. Learning in this example is unsupervised, with the algebra learning the structure\nof the search space.\n2 The embedding algorithm\n2.1 A toy problem illustrating algebraic learning\nConsider the very simple problem of learning how to classify 2×2 images in which pixels can be\nin black or white. We will learn how to classify these images into two classes using as training\ndata the following ﬁve examples\nWe label the two examples on the left as belonging to the “positive class” because they include\na black vertical bar, and name them as T+\n1 and T+\n2 . The three examples on the right are the\n“negative” class, T−\n1 , T−\n2 and T−\n3 . Our goal is to build an algebra that can learn from the\ntraining how to classify new images as belonging to the positive or negative class.\n52.2 Elements of the algebra\nTo embed a problem into an algebra we need the algebra to have at least one operator that\nis idempotent, associative and commutative. In this paper we use semilattices, the simplest\nalgebraic structures with such operator.\nWe will have three types of elements: constants, terms and atoms. Constants are the prim-\nitive description elements of our embedding problem. For images, for example, constants can\nbe each of the pixels in black or white. For our 2×2 images we would then have the 8 constants\nthat we write as c1 to c8.\nThe terms are formed by operating constants with the “merge” (or “idempotent sum-\nmation”) operation, for which we use the symbol ⊙. This is our binary operation that is\ncommutative, associative and idempotent. In the case of terms describing images, terms are\nsets of pixels. For example, the ﬁrst example in the training set is a term that can be expressed\nas the merge of four constants as\n= ⊙ ⊙ ⊙\nAtoms are elements created by the learning algorithm, and we reserve greek letters for them.\nSimilarly to terms being a merge of constants, ⊙ici, each constant is a merge of atoms, ⊙iφi.\nA term is therefore also a merge of atoms.\nAn idempotent operator deﬁnes a partial order. Speciﬁcally, the merge operator allows us\nto establish the inclusion relationship “ <” between elements a and b of the algebra, a < b, iﬀ\na⊙b = b. Take as example our ﬁrst training image, which was the merge of four constants,\nT+\n1 = c1 ⊙c2 ⊙c7 ⊙c8. Any of these constants, say c1, obeys c1 <T +\n1 , because c1 ⊙T+\n1 = T+\n1 .\nSimilarly, for a constant made of atoms, each of these atoms is “in” or “included in” the\nconstant.\nThe “training set” of the algebra consists of a set R of positive and negative relations of\nthe form (v <T+\n1 ) or ¬(v <T−\n3 ) where v is a constant, one we want to describe to our algebra\nby using examples and counterexamples.\nv < v ̸<\n6The learning algorithm transforms semilattices into other semilattices in a series of steps\nuntil ﬁnding one that satisﬁes the training set R. Using Model Theory[4] jargon, we want to\nﬁnd a model of the theory of semilattices extended with a set of literals (the training set R).\n2.3 Graph of the algebra\nWe use a graph to make the abstract notion of algebra more concrete and computationally\namenable. Nodes in the graph G are elements of the algebra. An enormous amount of terms\ncan be deﬁned from a set of constants. The graph has nodes only for the subset of terms\nmentioned in the training relations plus the “pinning terms”, terms that are calculated by the\nembedding algorithm and that we introduce later. We do not need to have a node for each\npossible term or element of the algebra.\nA directed edge a →b is used to represent some of the inclusion relationships between\nelements, but not all,\na→b ⇒ a<b, (1)\nwhere the implication only holds left to right. We add to the graph edges pointing from the\ncomponent constants of a term to the node of the term. If a term T is deﬁned as the merge of\nconstants ci then\nT ≡⊙ici ⇒ ∀i(ci →T). (2)\nIf all the component constants of a term T are also component constants of another term S we\nadd the edge T →S. We always use edges if any of the elements involved are atoms,\nφ→b ⇔ φ<b. (3)\nGraph edges can be seen as a graphical representation of an additional relation deﬁned in our\nalgebra that is transitive but not commutative. Graphs represent algebras only when they\nare transitively closed with respect to the edges. Directed edges are typically represented\nwith arrows. However, to avoid clutter we use simple straight lines instead of arrows pointing\nupwards in the ﬁgures, which is unambiguous because G is acyclic. Also to avoid clutter, in\nthe drawings we do not plot all implicit edges (for example, from atoms to terms). We also add\na “0” atom included in all constants. This is not strictly necessary but will make exposition\nsimpler. Our starting graph has already the form\nv\n0\n7From the edges we deﬁne the partial order < as\n∀φ((φ̸→a) ∨(φ→b)) ⇔ a<b, (4)\nwhere the universal quantiﬁer runs over all atoms. The formula says that a < bif and only if\nall the atoms edged to a are also edged to b.\nWhen the graph is transitively closed it describes an algebra we call M. This algebra\nevolves during the learning process producing a model of the training relations R at the end\nof the embedding. When we talk about M we mean the algebra described by the graph at a\ngiven stage of the algorithm.\n2.4 The dual algebra\nThe algebraic manipulations we need to do are easier to perform using not only the algebra M\nbut also an auxiliary structure M∗. This M∗ is a semilattice closely related (but diﬀerent) to\nthe dual of M [3], that we still call “the dual” and whose properties we detail in this section. We\nalso use an extended algebraic structure S that contains both semilattices M and M∗, which\nhave universes that are disjoint sets, i.e, an element of S is either and element of M or an\nelement of M∗. The unary function [ ] deﬁned for S maps the elements of M, say aand b, into\nthe elements [a] and [b] in M∗, that we call duals of aand b. The duals of constants and terms\nare always constants and the dual of atoms are a new kind of element we name “dual-of-atom”.\nM∗ has constants, dual-of-atoms and atoms but it does not contain terms. Atoms of M∗ are\nnot duals of any element of M. We refer to M∗ as the dual algebra and to M as “the master”\nalgebra.\nOur algebra Sis characterized by the transitive, noncommutative relation “→”, the partial\norder “<” and the unary operator [ ]. Besides the transitivity of “ →” and the deﬁnition of “<”\ngiven by Equation (4) we introduce the additional axiom,\na→b ⇒ [b] →[a], (5)\nthat, again, only works from left to right. It means that the edges of the graph of M are also\nedges of the graph of M∗ albeit reversed.\nThe auxiliary semilattice M∗ contains the images of the elements of M under the unary\noperator [ ], and has the reversed edges of M plus some additional edges of its own and its own\natoms. We introduced edges in M to encode deﬁnitional relations like how a given training\nimage (a term) is made up of particular pixels constants. In M∗ we add additional edges for\nthe positive order relations of R such as v <T+\n1 ,\n[T+\n1 ] →[v]. (6)\n8Positive order relations of our choosing are encoded with edges in M∗ and emerge in M as\nreversed order relations, i.e. we get ( v <T+\n1 ) from [T+\n1 ] →[v] at some point of the embedding\nprocess.\nThe graph of the dual M∗has all the reversed edges of M plus the edges corresponding to\nthe positive order relations of R and it should be also transitively closed. In this classiﬁcation\nexample, our training relations establish that v is included in the positive training terms T+\n1\nand T+\n2 , so there are edges from the duals of both terms to the dual of v. Note again that these\ntype of edges for relations of R are not in the graph of M.\n[v]\n0∗\n[0]\nAt the top of the graph of M∗ we draw the duals of the atoms of M, here only [0], and at the\nbottom of the graph we draw the atoms of M∗, here 0∗, again included to make our exposition\nsimpler.\n2.5 Atomized models\nEquation (4) deﬁnes how to derive the partial order from the transitive, noncommutative edge\nrelation “→” and an special kind of elements we call “atoms”. We say that a model for which\nthere is a description of the partial order in terms of a set of atoms is an “atomized” model.\nIn an atomized model all elements are sets of atoms. Using the language of Universal Algebra,\nwhen an algebra is atomized it explicitly becomes a direct product of directly idecomposable\nalgebras [3]. This does not mean, however, that we are restricting ourselves to some subset of\npossible models. The Stone theorem grants that any semilattice model can be described as an\natomized model [3].\nWe know how to derive the partial order from the atoms and edges but we have not given\nyet a deﬁnition for the idempotent operator. The merge (or idempotent summation) of a and\nbis the element of the algebra atomized by a set of atoms that is the union of the atoms edged\nto aand the atoms edged to b. The idempotent operator becomes a trivial set union of atoms.\nObviously this operation is idempotent, commutative and associative. It is also consistent with\nour partial order given in equation 4 that satisﬁes a<b iﬀ a⊙b= b. Consistently, the partial\norder becomes the set inclusion.\n9Before we continue with the embedding algorithm we are going to introduce some notation\nand redeﬁne the problem we are trying to solve in terms of sets of atoms. In Appendix A\nwe deﬁne some useful sets. For the moment it is enough to consider the set GLa(x) which is\nsimply the set of atoms edged to element x that is deﬁned, as always, only when the graph\nis transitively closed. The “ G” refers to the graph, the “ L” to the lower segment and the\nsuperscript “a” to the atoms. The merge of a and b corresponds with the set of atoms\nGLa(a⊙b) = GLa(a) ∪GLa(b). (7)\nFor our toy problem, we want a description of the constant vand for the pixels (also constants)\nas a sets of atoms. Speciﬁcally, we want a model for which v is a set included in the positive\ntraining images, T+\n1 and T+\n2 as\nv <T+\ni ⇔ GLa(v) ⊂GLa(T+\ni ), (8)\nwhere the atoms of a term are the union of the atoms of its component constants. We are also\nlooking for a particular atomic model for which the atoms of constant vare not all in the terms\ncorresponding with negative training examples\nv̸<T −\ni ⇔ GLa(v) ̸⊂GLa(T−\ni ). (9)\nThe diﬃculty in ﬁnding the model lies in enforcing positive and negative training relations\nsimultaneously, which translates in resolving a large system of equations and inequations over\nsets. The sets are made of elements we create in the process, the atoms, and there is the added\ndiﬃculty of ﬁnding sets as small and as random (or as free) as possible. In Sections 3.4 we\nintroduce the concept of algebraic freedom and discuss its connection with randomness.\nWe will use an operation, the crossing, to enforce positive relations one by one. By doing\nso the model evolves through a series of semilattice models, all atomized, until becoming the\nmodel we want. We can build the model step by step thanks to an invariance property related to\na construct we name trace. In the next sections we explain the trace and the crossing operation.\nAfter this we will show how to further reduce the size of model with a reduction operation and\nhow to do batch training. We will explain these operations for our toy example explicitly, and\nalso give an analysis of the exact and approximate solutions.\n2.6 Trace and trace constraints\nThe trace is central for the embedding procedure as a guiding tool for algebraic transformations.\nBy operating the algebra while keeping the trace of some elements invariant, we can control\nthe global eﬀects caused by our local changes.\nThe trace Tr(x) maps an element x∈M to a set of atoms in M∗. To calculate the trace\nof x, we ﬁnd ﬁrst its atoms in the graph of M, which we write as GLa(x). Say these are N\n10atoms φi, with φi →x. Since atoms are minima of M, dual of atoms are maxima of M∗, so\nfor each atom φi of xthere is a dual-of-atom at the top of the graph of M∗, [φi]. Each of these\n[φi] also have an associated set of atoms in M∗, GLa([φi]). The trace of x is deﬁned as the\nintersection of these N sets, Tr(x) = ⋂\ni=1,2,...,N GLa([φi]). Consistently, the trace of an atom\nφ equals Tr(φ) ≡GLa([φ]). In general we can write the trace as\nTr(x) ≡∩φ∈GLa(x)GLa([φ]). (10)\nFrom this deﬁnition it follows that the trace has the linearity property\nTr(a⊙b) = Tr(a) ∩Tr(b), (11)\nas the atoms in M for a⊙b are the union of the atoms of a and the atoms of b and therefore\nthe trace is the intersection of the traces of a and b. From this linearity and the deﬁnition of\nthe order relation, a<b iﬀ a⊙b= b, it follows that an order relation is related to the traces as\na<b ⇒ Tr(b) ⊂Tr(a). (12)\nThis makes a correspondence between order relations in M and trace interrelations between\nM and M∗ that we call trace constraints. For our toy problem, we are interested in obeying\ntrace constraints for the positive training examples, v <T+\ni , for which we then need to enforce\nTr(T+\ni ) ⊂Tr(v),\nfor v <T+\ni enforce Tr(T+\ni ) ⊂Tr(v). (13)\nThis does not cause v < T+\ni but it provides a necessary starting point. For negative training\nexamples, T−\ni , we want to obey that ¬(v < T−\ni ). This inclusion does not follow from (10),\nhowever, it can always be enforced if the embedding strategy is consistent as\nfor ¬(v <T−\ni ) enforce Tr(T−\ni ) ̸⊂Tr(v). (14)\nOnce the trace constraint is met, no transformation of M can produce v <T−\ni unless it alters\nthe traces. This constraint prevents positive relations to appear in M in places where we do\nnot want them.\nWhile the operator [ ] does not really map M into its dual semilattice, the traces of the\nelements of M form an algebra that very much resembles the dual of M. This new algebra has\ntrace constraints in the place of order relations and set intersections in the place of set unions.\nThere are still some subtle diﬀerences between a proper dual ofM and the dual algebra provided\nby the trace. For example, the trace is deﬁned with the atoms of M instead of the constants\nof M, so it depends on the particular atomization of M. While ﬁnding a proper dual of M\namounts in diﬃculty to calculate M itself, enforcing the trace constraints is easier because we\nhave the extra freedom of introducing new atoms in M. In addition, we do not have restrictions\nfor the size of the traces. We do not care if the traces are large or small.\n11We want an atomization for M but ﬁrst we have to calculate an atomization for M∗. The\natomization we are going to build for M does not correspond with the dual of M∗, neither it\ncorresponds with the dual of the algebra deﬁned by the trace. It corresponds with an algebra\nfreer than the algebra described by the traces. In Section 3.4 we explain the role that algebraic\nfreedom plays as a counterbalance to cardinal minimization.\nEnforcing the trace constraints might look challenging but it is relatively simple. We are\naided by the encoding of training relations R as directed edges in the graph of M∗ so when\nthe graph is transitively closed the “reverted” positive relations [ T+\ni ] <[v] are always satisﬁed.\nWe can start, although this step is optional, by ﬁrst requiring M∗ to satisfy the “reverted”\nnegative relations, positive and negative. That is, if we want to enforce ¬(v < T−\ni ) in M, we\nenforce ¬([T−\ni ] <[v]) by adding an atom to [T−\ni ] in M∗. In our toy example, for every negative\nexample T−\ni we then add an atom ξi →[T−\ni ], so in our example we introduce three atoms ζ1,\nζ2 and ζ3 in the graph of M∗,\nv\n0∗\n[0]\nζ1 ζ2 ζ3\nThe new atoms are not in the set GLa([v]) so the reverted negative relations are satisﬁed. In\nfact all reverted relations, positive and negative, are satisﬁed at this point. We have now the\nchance to detect if the input order relations are inconsistent. First, make sure that for each\ncouple of terms T1 and T2 mentioned in the input order relations such that the component\nconstants of T1 are a subset of those of T2 we have added the edge [ T2] →[T1]. At this point,\nafter transitive closure, the reverted order relations are satisﬁed if and only if the embedding\nis consistent.\nIf there are edges pointing in both directions between two elements of M∗ we can identify\nthem as the same element. Two ore more elements of M may share the same dual.\nWe have completed the preprocessing step that speeds up the enforcing of trace constraints\nand validates the consistency of the embedding. We start now enforcing the trace constraints\nfor the negative examples, Tr(T−\ni ) ̸⊂Tr(v). To compute the trace, we place the graph for M\nand for M∗ side to side, to left and right, respectively\n12v\n0\n[v]\n0∗\n[0]\nζ1 ζ2 ζ3\nWe are now going to apply Algorithms 1 and 2 in Appendix C to enforce the trace con-\nstraints. We start with the negative trace constraints, Algorithm 1. The trace for the neg-\native training examples is Tr(T−\ni ) = GLa([0]) = {0∗,ζ1,ζ2,ζ3}, and for constant v is also\nTr(v) = GLa([0]) = {0∗,ζ1,ζ2,ζ3}. Now it is not obeyed that Tr(T−\ni ) ̸⊂Tr(v) so we need to\nenforce it. For this we need to choose a constant c ∈M equal to v or such that [ c] receives\nedges from [ v] and not from [ T−\ni ]. We then need to add an atom φ →c. The condition is\nfulﬁlled directly by v so we add φ→v, and the corresponding dual-of-atom [ v] →[φ] in M∗.\nv\n0φ\n[φ]\n[v]\n0∗\n[0]\nζ1 ζ2 ζ3\nWe now re-check the traces, Tr(T−\ni ) = Tr(0) = {0∗,ζ1,ζ2,ζ3}and Tr(v) = Tr(0) ∩Tr(φ) =\n{0∗}, thus obeying Tr(T−\ni ) ̸⊂Tr(v), as required.\nNow, for positive trace constraints, we apply Algorithm 2 . For the positive relations\nv <T+\ni , we need to enforce Tr(T+\ni ) ⊂Tr(v). First we check the values of the traces, Tr(T+\ni ) =\nTr(0) = {0∗,ζ1,ζ2,ζ3}, and Tr(v) = Tr(0)∩Tr(φ) = Tr(φ) = {0∗}. This means that Tr(T+\ni ) ̸⊂\nTr(v), so we need to enforce the trace constraint. We add atoms ϵi to the constants ci until\nTr(T+\ni ) = Tr(0) ∩iTr(ϵi) equals Tr(v). For the ﬁrst term T+\n1 we just need to edge one atom to\nthe ﬁrst constant, ϵ1 →c1, so Tr(T+\n1 ) = Tr(0) ∩Tr(ϵ1) = {0∗,ζ1,ζ2,ζ3}∩{0∗}= {0∗}. For the\nsecond training example, T+\n2 , we need to add one atom for each of its constants, ϵ2 →c3 and\nϵ3 →c4. Doing this, we have Tr(T+\n2 ) = Tr(0) ∩Tr(ϵ2) ∩Tr(ϵ3) = {0∗,ζ1,ζ2,ζ3}∩{ 0∗,ζ2}∩\n{0∗,ζ1,ζ3}= {0∗}, as required.\nAfter imposing the trace constraints the graphs look like this\n13v\n0φ ϵ1 ϵ2 ϵ3\n[φ] [ϵ1] [ϵ2][ϵ3]\n[v]\n0∗\n[0]\nζ1 ζ2 ζ3\nWe have used our toy example to show how to enforce the trace constraints of Algorithms 1\nand 2, detailed in Appendix C. The general trace enforcing algorithm consists of repeatedly\nenforcing negative trace constraints and positive trace constraints until all constraints are sat-\nisﬁed, which usually occurs within a few iterations. The trace enforcing process always ends\nunless the embedding is inconsistent (see Theorem 7). The number of times these algorithms\nloop within the do-while statements can be easily bounded by the cardinality of the sets involved\nand is no worse than linear with the size of the model.\n2.7 Full and Sparse Crossing operations\nAfter enforcing the trace constraints, all negative relations v ̸< T−\ni are already satisﬁed in\nM. This will always be the case. To build an atomized model that also satisﬁes the positive\nrelations v <T+\ni , we use the Sparse Crossing operation. This trace-invariant operation replaces\nthe atoms ofvfor others that are also inT+\ni without interfering with previously enforced positive\nor negative relations and without changing the traces of any element of M.\nThe Sparse Crossing can be seen as an sparse version of the Full Crossing that is also\na trace-invariant operation. Both operations are similar and can be represented with a two\ndimensional matrix as follows. Consider two elements a and b with atoms\nGLa(a) = {α,β,χ } and GLa(b) = {χ,δ,ε}, (15)\nand suppose we want to enforce a<b . Extend the graph appending new atoms and edges as\nφ,ϕ,γ →α\nπ,ω,θ →β\nχ′,φ,π →χ\nδ′,ϕ,ω →δ\nε′,γ,θ →ε\nClose the graph by transitive closure and delete α,β, χ, δand εfrom the graph. Then it holds\nthat GLa(a) ⊂GLa(b) and therefore, a<b . This is the Full Crossing of a into b and can be\n14represented with the table\nχ δ ε\nχ′ δ′ ε′\nα φ ϕ γ\nβ π ω θ\nWe say that we have ”crossed” atoms α and β into b. Note that we do not need to ”cross”\natom χ of a as it is already in b.\nSince the crossing is an expensive operation that multiplies the number of atoms, we instead\ndo a Sparse Crossing. The idea is that, as long as we check that all involved atoms remain\ntrace-invariant, the Sparse Crossing operation still enforces a < band preserves all positive\nrelations. In addition, it also preserves negative relations as long as they are ”protected” by\nits corresponding negative trace constraint. Later we give details of how to compute it, but is\nis intuitive to think of it as a Full Crossing but leaving empty spaces in the table, as in this\nexample\nε1 ε2 ε3\nε′\n1 ε′\n3\nφ1 ϕ12\nφ2 ϕ22 ϕ23\nThe Full Crossing and Sparse Crossing operations transform one graph into another graph\nand, after transitive closure, the crossing operations also map one algebra into another. This\nmapping function commutes with both operations ⊙and [ ] so it is an homomorphism. This\nmeans that if q = r⊙s is true before crossing it is also true after. Since the partial order <\nis deﬁned using the idempotent operator ⊙, crossing operations preserve all inclusions between\nelements.\nFull and Sparse Crossing operations keep unaltered all positive order relations, i.e. if\np < qis true before the crossing of a into b, it is also true after. This applies to all positive\nrelations, not only the positive training relations of R+ (we use R+ for the positive relations of\nR). Crossing, however, does not preserve negative order relations: a negative relation before\ncrossing can turn positive after crossing. In fact, crossing is never an injective homomorphism\nbecause a<b , that is false before crossing, becomes true after the crossing.\nIt turns out that the enforcement of negative trace constraints and the positive trace\nconstraint Tr(b) ⊂Tr(a) is what is need to ensure that the crossing of a into b preserves the\nnegative order relations R−. In addition, the atoms introduced during the trace constraint\nenforcement stage are precisely the ones we need to carry out the Full Crossing (or Sparse\nCrossing) operation and keep all atoms trace-invariant. This follows from Theorem 1 that\nstates that the Full Crossing of a into b leaves the traces of all atoms unchanged if and only if\nthe positive trace constraint for a<b , i.e. Tr(b) ⊂Tr(a) is satisﬁed.\n15For a recipe on how to compute the Sparse Crossing, see the Algorithm 3 in Appendix\nC. In the following, we apply it to our toy example to the two positive input relations.\nThe Sparse Crossing operation of vinto T+\ni works by creating new atoms and linking them\nto atoms in v and atoms in T+\ni in a way that the traces of all atoms are kept unaltered. The\nlinearity of the trace ensures that if the trace of the atoms remain unchanged so do the traces\nof all elements.\nAfter we edge new atoms, say φ1, φ2 and φ3, to an existing atom φ of v, the trace of\nφ should be recalculated using Tr(φ) = Tr(φ1) ∩Tr(φ2) ∩Tr(φ3). Since the new atoms we\nintroduce during the Crossing operations are edged not only to φbut also to atoms of T+\ni , the\ntrace of φ could change and we do not want that to happen. Before we get rid of atom φ and\nreplace it with the new ones, we have to make sure that the trace Tr(φ) has not changed.\nWe want the Sparse Crossing to be as sparse as possible to produce a model as small as\npossible. We replace φ with as few new atoms as possible limited only by the need to keep\nTr(φ) unchanged. This need may force us to introduce more than one new atom in the row of\natom φin the Sparse Crossing matrix. It is always possible to preserve Tr(φ) provided that we\nhave enforced the positive trace constraint for a<b , i.e. Tr(b) ⊂Tr(a).\nKeeping the trace of the atoms of T+\ni unaltered is also necessary but it is easier than\npreserving the trace of φ. Suppose an atom α<T +\n1 and assume we add an edge φ1 →α. The\ntrace of α may change but, if it does, now we have the extra freedom of appending another\nnew atom edged just to α, which always has the eﬀect to leave the trace of α invariant. This\nfreedom does not exist for atoms of v, like φ. Keep in mind that the goal of the crossing is\nreplacing the atoms of v with new atoms that are in both v and T+\ni . If we add an extra atom\nφ′ edged only to φ, then φ′ is not in T+\ni .\nConsider the set of atoms Φ 1 in v and not in the ﬁrst positive example, T+\n1 , and choose\none. At the point where we are in our toy example, there is only a single atom, φ∈Φ1. Now\nwe select one of the atoms in T+\n1 , in this case only ϵ1. We create a new atom φ1, and we add\nan edge to φ and another to ϵ1, φ1 →φ and φ1 →ϵ1, obtaining the new graph\n16v\n0φ ϵ1 ϵ2 ϵ3\nφ1\n[φ]\n[φ1]\n[ϵ1] [ϵ2][ϵ3]\n[v]\n0∗\n[0]\nζ1 ζ2 ζ3\nWe have to check that the traces of all the atoms remain unchanged. We can start with atom\nφ. Initially, we have that Tr(φ)i = {0∗}, and after the crossing Tr(φ)f = Tr(φ1) = {0∗},\nso it has not changed. For atom ϵ1, we have initially Tr(ϵ1)i = {0∗}and after the crossing\nTr(ϵ1)f = Tr(φ1) = {0∗}, so it has not changed either. Now that we have checked for trace\ninvariance of the crossing, we can eliminate the original atoms φ and ϵ1, giving\nv\n0ϵ2 ϵ3φ1\n[φ1] [ϵ2][ϵ3]\n[v]\n0∗\n[0]\nζ1 ζ2 ζ3\nWe now perform the crossing between v and the second positive example, T+\n2 . We cross the\nonly atom in Φ 2 (the atoms of v that are not atoms of T+\n2 ), φ1, with one of the two atoms in\nT+\n2 , say ϵ2. We create a new atom φ2 and edges φ2 →φ1 and φ2 →ϵ2. Before the crossing, the\ntrace of atom φ1 is Tr(φ1)i = {0∗}, but after the crossing is Tr(φ1)f = {0∗,ζ2}. Since the trace\nhas changed, we proceed to select another atom in T+\n2 , that is, ϵ3. We then create a new atom\nφ3 and edges φ3 →φ1 and φ3 →ϵ3. After appending φ3 with this new edges to the graph the\ntrace of φ1 becomes Tr(φ1)f = Tr(φ2) ∩Tr(φ3) = {0∗}, so we now have the trace invariance\nwe were looking for. The trace of atom ϵ2 also remains unchanged as Tr(ϵ2)i = {0∗,ζ1}equals\nTr(ϵ2)f = Tr(φ2) = {0∗,ζ1}. For the trace of ϵ3 we have Tr(ϵ3)i = Tr(ϵ3)f = {0∗,ζ1,ζ3}so it\nalso remains unaltered. Eliminating the initial atoms φ1,ϵ2 and ϵ3 we have the graph\n17v\n0φ2 φ3\n[φ3][φ2]\n[v]\n0∗\n[0]\nζ1 ζ2 ζ3\nIf we had more atoms in v, we would repeat the same procedure for these atoms. In our present\ncase, we have ﬁnished.\nThe Sparse Crossing operation has worked; the positive training examples all obeyv <T+\ni ,\nand the negative ones v̸<T −\nj . The atoms of v are GLa(v) = {0,φ2,φ3}and the atoms for the\npositive training examples are also GLa(T+\n1,2) = {0,φ2,φ3}, while for the negative examples we\nhave GLa(T−\n1,3) = {0,φ3}and GLa(T−\n2 ) = {0,φ2}.\n2.8 Reduction operation\nModels found by Sparse Crossing are much smaller than models found by Full Crossing. We\nare still interested in further reducing their size. A suitable size reduction algorithm should be\ntrace-invariant. Trace invariance preserves trace constraints and preserving trace constraints\nensures that we will be able to carry out pending Sparse Crossing operations. This means\nthat a trace-invariant reduction scheme can be called at any time during learning, as often as\nrequired.\nWhile carrying out Sparse Crossing operations we were careful to keep the trace of all the\natoms unaltered, for the reduction operation we will focus on constants instead. An operation\nthat keeps the trace of all constants unaltered also keeps the trace of the terms unaltered and\nthe trace constraints preserved. Since atoms are not mentioned in trace constraints we do not\nreally need them to be trace-invariant; it is enough with keeping the constants trace-invariant.\nFurthermore, we can remove atoms from a model as long as we keep the traces of all the\nconstants unchanged.\nOur reduction scheme consists of ﬁnding a subset Q of the atoms that produces the same\ntraces for all constants. We can then discard the atoms that are not in Q. We start with Q\nempty. Then we review the constants one by one in random order. For each constant c we\nselect a subset of its atoms such that the trace of c calculated using only the atoms in this\nsubset corresponds with the actual trace of c. The selected atoms are added to Q before we\n18move onto the next constant. When selecting atoms for the next constant c we start with the\nintersection between its atoms and Q, i.e GLa(c)∩Q, and continue adding atoms to Quntil the\ntrace calculated taking into account the atoms of GLa(c)∩Qequates Tr(c). Once all constants\nare reviewed any atom that is not in Qcan be safely removed from the algebra. Algorithm 4\nin Appendix C is an improved version of this method.\nAt the point where we are with our toy problem we have already obtained an atomized\nmodel that distinguishes well positive from negative examples. We are going to apply the\nreduction algorithm to see if we can get rid of some of its atoms. We are going to review\nthe constants starting by the third one. In our example, the trace of the third constant only\ndepends on a single atom Tr(c3) = Tr(0) ∩Tr(φ2) = Tr(φ2), so we must keep φ2 for trace\ninvariance of this constant. An analogous situation takes place for the fourth constant, for\nwhich we have that its trace only depends on φ3, Tr(c4) = Tr(φ3), so it cannot be eliminated,\neither. The model we have obtained cannot be reduced.\nTo illustrate a simple size reduction in action, let us add to the graphs an extra atom β\nedged to the ﬁrst constant, β →c1. This new atom does not change any traces and gives the\ngraphs\nv\n0φ2 φ3β\n[β] [φ3][φ2]\n[v]\n0∗\n[0]\nζ1 ζ2 ζ3\nRevising the ﬁrst constant, we then have that its trace depends on the trace of the three\natoms and atom 0 as Tr(c1) = Tr(0) ∩Tr(φ2) ∩Tr(φ3) ∩Tr(β), with Tr(0) = {0∗,ζ1,ζ2,ζ3},\nTr(φ2) = {0∗,ζ2}, Tr(φ3) = {0∗,ζ1,ζ3}and Tr(β) = {0∗}. The constant c1 would remain trace-\ninvariant if we eliminated atoms φ2 and φ3 or only β. But, as for the invariance of constants c3\nand c4 we need atoms φ2 and φ3, it is β the atom we can eliminate. The stochastic Algorithm\n4 in Appendix C typically would delete atom β in a single call or within a few calls.\nThere are other reduction schemes. For example, a size reduction scheme based on keeping\njust enough atoms to discriminate the set R−ensures an atomization with a size under that of\nset R−. The problem with this reduction scheme is that it fails to produce good generalizing\nmodels as it seems to reduce algebraic freedom (see Section 3.4) more than one would wish,\nespecially at the initial phases of learning when the error is large and the algebra should grow\n19rather than shrink. In addition, since this scheme can violate negative trace constraints, it can\nonly be used once the full embedding is completed. If used at some intermediate stage of the\nembedding, subsequent Sparse Crossing operations may produce models that do not satisfy\nR−. This is a problem because the model can become very large before we can reduce its size.\nHowever, this scheme can be successfully applied to the dual, M∗, right before trace constraints\nare enforced, ensuring that the number of atoms in M∗ is never larger than the size of the set\nR−. This reduction scheme corresponds to Algorithm 6 in Appendix C.\nThe trace-preserving reduction scheme presented in this section works well in combination\nwith Sparse Crossing and ﬁnds small, generalizing models eﬃciently. For this reduction scheme\nthere is no guarantee that the size of M is going to end up under the size of M∗ or under the\nsize of R−. In fact, it is often the case that the atomization of M is a few times larger than\nthe atomization of M∗. Even when is smaller then M, M∗does not generalize because it is not\nsuﬃciently free.\n2.9 Batch training\nWe have seen how to learn an atomized model from a set Rof positive and negative examples.\nIn practice, we would check the accuracy of the learned model in test data. If the accuracy is\nbelow some desired level, we would continue training with a new set of examples. Rather than\na single set R we have a series of batches R0,R1,...,R n where the subscript corresponds with\nthe training epoch.\nAssume we are in epoch 1. In order to keep the graph G(S) manageable we want to remove\nfrom it the nodes corresponding to elements mentioned in R0, as well as deleting the set R0\nitself to leave space for the new set of relations R1. However, if we delete R0 we run into the\nfollowing problem; Often when we resume learning by embedding R1 some relations of R0 no\nlonger hold. We need a method to minimize the likelihood for this to occur.\nIn the following we discuss how to do batch learning. Once learning epoch 0 is completed\nwe have R0 encoded into atoms. What we do is replacing R0 by a set of relations that deﬁne its\natoms in the following way; For each atom φ, we create one term Tφ equal to the idempotent\nsummation of all the constants that do not contain atom φ. For each constant c so (φ < c),\nwe create a new relation ¬(c < Tφ). We call this set of terms and relations “the pinning\nstructure” of the algebra because they help to preserve knowledge when R0 is deleted and\nadditional learning takes place. We call terms Tφ “pinning terms” and the set of relations,\n“pinning relations” and refer to them with Rp.\nFollowing this procedure in our toy example, we would create two pinning terms. For atom\nφ2, with φ2 < c1 and φ2 < c3, we create Tφ2 = c2 ⊙c4 ⊙c5 ⊙c6 ⊙c7 ⊙c8. For atom φ3, with\n20φ3 <c1 and φ2 <c4, we create Tφ3 = c2 ⊙c3 ⊙c5 ⊙c6 ⊙c7 ⊙c8.\nWe then require the negative relations ¬(c1 <Tφ2 ), ¬(c3 <Tφ3 ), ¬(v <Tφ2 ), ¬(c1 <Tφ3 ),\n¬(c4 <Tφ3 ) and ¬(v <Tφ3 ), and obtain\nTφ2\nTφ3\nv\n0φ2 φ3\nTφ2\nTφ3\n[φ2] [φ3]\n[v]\n0∗\n[0]\nζ1 ζ2\nNew pinning terms and relations are formed at the end of each learning epoch. We do not\nneed to replace pinning relations of epochs 0 ,1,...,n −1 with the pinning relations derived\nfrom the atoms of epoch n, instead we can let them accumulate, epoch after epoch. Pinning\nrelations do not grow until becoming unmanageable. One of the reasons why this occurs is\nthat pinning terms and relations do not only get created, they also get discarded. Discarding\npinning relations is needed because the set Rn∪Rp may be inconsistent. We use Rp to refer to\nall the accumulated pinning relations.\nInconsistencies are detected as explained in Section 2.6. When we try to enforce the set\nRn ∪Rp in M∗ often we ﬁnd that some negative relations cannot be enforced. We enforce the\nrelations in M∗by adding atoms in the dual to discriminate all the (reversed) negative relations,\ni.e. the relations in the set R−\nn ∪Rp. Once we calculate the transitive closure of the graph of\nM∗ we may ﬁnd that some negative relations do not hold. It doesn’t matter how many times\nwe try or how we choose to introduce the discriminating atoms in M∗ the resulting relations\nthat do not hold are always the same and are always negative. If a relation that belongs to\nR−\nn does not hold, then the set Rn is inconsistent. In this case, something is wrong with our\ntraining set. If the relation that fails belongs to Rp we just discard it deleting its pinning term\nand associated pinning relations. We can regard Rp as a set of hypotheses; some hypotheses\nare eventually found inconsistent with new data.\nOnce inconsistent pinning relations have been discarded and we have a consistent set\nRn ∪Rp we have a couple of possible strategies. One is enforcing Rn ∪Rp in epoch n. The\nother strategy, the one used in all the experiments of this paper, consists of creating a new set\nof atoms for M∗ enforcing only the relations of Rn but with the pinning terms of Rp present\n21in the graph of M∗. Then we enforce trace constraints for Rn and also for Rp but only for\nthe relations of Rp that happen to hold in M∗. In diﬀerent epochs diﬀerent relations hold and\ndiﬀerent pinning terms are used. We found this strategy more eﬃcient and computationally\nlighter than the ﬁrst.\nPinning relations are all negative and there are good reasons for this. One reason has to do\nwith maximizing algebraic freedom and it is discussed in section Sections 3.4 and Theorem\n3, the second reason is that inconsistent negative relations can be individually detected while\ninconsistent positive relations cannot be detected so easily. In Theorem 4 of Appendix B\nwe prove that any positive relation that is entailed by a set of positive and negative relations is\nalso entailed by the positive relations of the set alone. Negative relations do not have positive\nrelations as logical consequences; their consequences are all negative, so by restricting ourselves\nto negative pinning relations we can detect and isolate inconsistences introduced by the pinning\nrelations as they arise.\nIntroducing pinning relations to deal with batch learning has other important advantage.\nPinning relations found after embedding a batch of order relationsRn+1 tend to be quite similar\nto those obtained for the previous batch Rn, more so the more the algebra has already learned.\nThis means that the number of pinning terms and relations tend to converge to a ﬁxed number\nwith training or grow very slowly. This approach is then clearly superior to one combining all\ntraining sets together.\nPinning terms and relations accumulate the knowledge of the training and can be shared\nwith other algebras.\n3 Analysis of solutions\n3.1 Finding a class deﬁnition for the toy problem\nSo far we have obtained, with 2 positive and 3 negative examples, a model with two atoms.\nWith a few more training examples, two more atoms are obtained. In terms of the constants\nthey are edged to, we can plot these four atoms as\nwhere the ﬁrst two atoms are atoms φ3 and φ2, that we derived in previous sections. Using\nmore examples, trace-invariant reduction and Sparse Crossing the reader can ﬁnd the entire\nsolution by hand.\nThe 16 possible 2 ×2 images can be correctly classiﬁed into those with a black vertical bar\n22that contain the four atoms and those without a bar that have one or more atoms missing. A\n2 ×2 image I has the property v of the positive class when its term contains all four atoms.\nThose images that do not contain a vertical bar have terms that only contain three atoms or\nless. Describing the four atoms in terms of the constants, that for clarity we have renamed as\ncijb for black pixels in row i and column j:\n(v <I) ⇔(c11b <I ∨c12b <I ) ∧(c11b <I ∨c22b <I )\n∧ (c21b <I ∨c12b <I ) ∧(c21b <I ∨c22b <I ),\n(16)\nwe get a ﬁrst order expression that deﬁnes v.\nUsing the distributive law, we can rewrite the solution in Equation (16) as\n(v <I) ⇔(c11b <I ∧c21b <I ) ∨(c12b <I ∧c22b <I ), (17)\nor more compactly as\n(v <I) ⇔∨2\nj=1 ∧2\ni=1 (cijb <I ). (18)\nThis last expression in Equation (18) says that positive examples have or column 1 or column 2\n(or both) with row 1 and row 2 in black, which is the simplest deﬁnition of an image including a\nvertical bar. We have been able to learn and derive from examples a closed expression deﬁning\nthe class of images that contain a vertical bar.\nDeriving formal class deﬁnitions form examples is an exciting subject but in this paper\nwe are mainly interested in approximate solutions that are suﬃcient for Machine Learning. To\nunderstand the approximate solutions, however, we are going to walk backwards from exact,\nclosed expressions to the atoms. Speciﬁcally, we will show that for the vertical bar problem in\na grid of size M×N there is an astronomically large number of suitable approximate solutions\nwith a desired error rate, and that our stochastic learning algorithm only needs to ﬁnd one.\n3.2 Analysis of exact solutions\nSo far we have analyzed the vertical bar problem only for 2 ×2 images. In the following we\nderive the exact solution for the more general case of M ×N images. We are not going to\nlearn the solution from examples, instead we are going to derive the form of the atoms from\nthe known concept of a vertical bar. This exact solution will be used in the next section to\nshow that, for algebraic learning to ﬁnd an approximate solution, it needs to ﬁnd some valid\nsubset of atoms from the exact solution and there are an astronomically large number of valid\nsubsets.\nAn M ×N image contains a black vertical bar if there is at least one of the N columns\nin the image for which all M pixels are black. Formally, image I has a vertical line if either\n23column 1 has all rows in black, or column 2 or any of the N columns,\n(v <I) ⇔∨N\nj=1 ∧M\ni=1 (cijb <I ), (19)\nwhere the dusjunction ∨runs over the columns of the image I and the conjunction ∧over the\nrows and cijb stands for the pixel in row i and column j being in black.\nIn order to compare Equation (19) with the general form of a solution learned using algebras\nwe are going to start by expressing the partial order a < baﬀecting any two elements a and\nb of an atomized algebra in terms of its constants rather than its atoms. We know how to\ndetermine if a<b using atoms: a<b if and only if the atoms of aare also atoms of b. Just to\nclarify, we say an atom φ is “in a” or is “of a” or “contained in a” if it is in the lower segment\nL(a) or, equivalently in the graph, if it is edged to a, i.e. if φ∈GL(a) or, also equivalently, if\nφ<a .\nElement aof an atomized algebra is lower than b(or “in b”) if and only if all the atoms of\na are also in b,\n(a<b ) ⇔∧φ<a (φ<b ), (20)\nwith the conjunction running over all atoms of a. An atom φ is contained in b if any of the\nconstants that contain that atom, cφk, is in element b,\n(φ<b ) ⇔∨k(cφk <b), (21)\nwhere index k runs along all constants cφk that contain atom φ.\nSubstituting Equation (21) into Equation (20), we can write\n(a<b ) ⇔∧φ<a ∨k (cφk <b), (22)\nwhere we have expressed a<b with the constants of the algebra.\nGoing back to our problem of vertical bars, we can apply to v < Iwhat we have learned\nand write\n(v <I) ⇔∧φ<v ∨k (cφk <I ). (23)\nwhere I is a term describing an image, and then compare it with our ﬁrst expression, Equation\n(19). This formula has a conjuction followed by a disjunction so we are going to transform\nEquation (19) to conjunctive normal form (CNF) to match the form of Equation (23).\nWe ﬁrst do the transformation for images of size 3 ×2 to keep it intuitive. For this size of\nimages, the expression for a vertical bar to be in one of these images in Equation (19) is of the\nform\n(v <I) ⇔∨2\nj=1 ∧3\ni=1 (cijb <I )\n= (c11b<I ∧c21b<I ∧c31b <I ) ∨c12b<I ∧c22b<I ∧c32b <I ).\n(24)\n24Using that conjuction and disjuction are distributive:\n(v <I) ⇔∨2\nj=1 ∧3\ni=1 (cijb <I )\n= (c11b ∨c12b) ∧(c11b ∨c22b) ∧(c11b ∨c32b)\n∨(c21b ∨c12b) ∧(c21b ∨c22b) ∧(c21b ∨c32b)\n∨(c31b ∨c12b) ∧(c31b ∨c22b) ∧(c31b ∨c32b),\n(25)\nwhere for compactness we are not writing <I after each pixel cijb. We now have a conjunction\nof 9 terms, each term a disjuntion of two constants. For the column j the indexes are clear as\nthey simply run over 1 and 2. For rows, note that each of the nine terms is one of the 9 possible\nways to assign each of the rows to a column\n(v <I) ⇔∨2\nj=1 ∧3\ni=1(cijb <I )\n= ∧9\nσ ∨2\nj=1(cσ(j)jb <I ),\n(26)\nwhere we have introduced σ, an index that runs over all possible assignations of a row to each\ncolumn. To express that we are covering all possible combinations (variations, in fact) we write\nthe symbol j →i to represent a new index that runs over all mappings from j to i. We can\nthus write\n(v <I) ⇔∨2\nj=1 ∧3\ni=1(cijb <I )\n= ∧9\nj→i ∨2\nj=1(ci(j)jb <I ),\n(27)\nwhich is the CNF form we were after. We could conveniently use symbolj →ito simply switch\n∨j∧i for ∧j→i∨j to get to the same result. The constants ci(j)jb are now written with an index\nithat depends on index j. Said dependency is diﬀerent for each of the 9 possible values of the\n“map index” j →i. Each value of the index is a possible mapping function i(j) from j to i.\nComparing the formula of Equation (27) with the formula for the algebra in Equation (23)\nwe get that for these 3 ×2 images there are 9 atoms of the form\nφj→i = ∨jci(j)jb, (28)\neach edged to a black pixel in column 1 and a black pixel in column 2.\nThe same argument follows for images of size M ×N, for which there are MN atoms of\nsize M ×N, each edged to a black pixel in each of the N columns. The MN diﬀerent atoms\ncome from the possible mappings from columns to rows,\n(v <I) ⇔∧j→iφj→i. (29)\nFor images of size 3 ×2 the exact atomization has to 3 2 = 9 atoms. For our toy example of\n2 ×2 images, the exact atomization corresponds to 2 2 = 4 atoms with the same form we found\nusing the algebraic learning algorithm. For this simple problem Sparse Crossing managed to\nﬁnd the exact atomization.\n25Following an analogous procedure, in Appendix D we show how to derive the form of the\nexact atomization for any problem for which we know a ﬁrst-order formula, with or without\nquantiﬁers.\n3.3 Analysis of approximated solutions\nConsider the vertical line problem again but this time for images of size 15 ×15. According\nto the analysis in the previous section, the exact atomization has 15 15 ≈4 ×1017 atoms, each\nhaving 15 black pixels, one per column. Now we calculate the number of atoms we would need\nfor an approximated model.\nSuppose we are dealing with a training dataset that has a 10% noise deﬁned as a probability\nto have a white background pixel transformed into black. Assume we are interested in a model\nwith a false positive error rate of 1 in a 1000, that is, out of 1000 images without a vertical bar\nwe accept, on average, one false positive. For an image without a vertical bar to be classiﬁed\nas positive, it needs to contain all atoms of constant ’ v’. Let us ﬁrst compute the probability\nthat a given image contains one of the atoms, say atom φ, of v. This atom φ is in one black\npixel per column. In total φ is edged to 15 black pixels. For an image without a bar to have\nthis atom, out of its 10% noise pixels in black it needs to have at least one black pixel in the\nsame position than one of the 15 black pixels of φ. The probability of this happening is the\nprobability that not all of the 15 black pixels in φ are white in our image,\np(φ<I ) = 1 −0.915, (30)\nwith 0 .9 = 1 −0.1 the probability that any background pixel is white.\nSuppose v has A atoms. The probability for the term associated to the image to contain\nall the atoms of v, considering the probabilities for each of the atoms to be in the image as\napproximately independent, is then given by\np(v <I) ≈(1 −0.915)A. (31)\nFor this probability to be below 1 /1000, we obtain for A that\nA> log(1/1000)\nlog(1 −0.915) = 29.9. (32)\nThis means that approximately 30 atoms suﬃce to have a false positive error under 1 in 1000\nand no false negatives. We can choose these 30 atoms among the 4 ×1017 atoms of the exact\natomization. This gives of the order of 10 529 solutions. Our learning algorithm only needs to\nﬁnd one among this astronomical number of solutions.\nIf we use Sparse Crossing to resolve this problem we start seeing atoms with the right form\nafter some few hundred examples. In fact, we ﬁnd 30 atoms (and more) of the exact solution\n26which renders the error rate to less than one in a thousand within 50 .000 examples. Learning\noccurs fast: error rate is about 5% after the ﬁrst 1000 examples. Hundreds of atoms with the\nright form are found at the end.\nIdentifying vertical bars is easy because the atoms of the exact solution are small (this is\ndiscussed in general in Appendix D ). An exact solution with small atoms is not a general\nproperty of all problems. When the atoms of the exact solution are large, an atom taken from\nan approximate solution often corresponds to a “subatom” rather than an atom of the exact\nsolution. A subatom is a smaller atom cotained only in some of the containing constants of an\natom in the exact solution. By replacing large exact atoms with smaller subatoms false negatives\nare tradeoﬀ in exchange for fewer false positives. If the subatoms are chosen appropriately the\nsize of the algebra and the error rate can be kept small.\nFor example, if we want to distinguish images with an even number of bars from images\nwith an odd number of bars, the atoms of the exact solution have a variety of forms and sizes.\nEach atom is contained in each of the white pixels of one or more complete white bars and\nalso in one black pixel of each of the remaining bars. These atoms may be very large, some\ncontained in almost half of the constants. The form of the exact solution for this problem can\nbe derived by using the technique detailed in appendix D. The atoms we obtain by Sparse\nCrossing correspond to sub atoms of the exact solution and have variable sizes. The atoms\nof the exact solution for this problem can be partitioned in classes and approximate solutions\nselect atoms in all or most of these classes. We have resolved this problem using Sparse Crossing\nwith error rates well under 1%. We have solved it for small grids like 10 ×10 with a low or\nmoderate noise below 10% and for smaller grids like 5 ×5 with a background noise as large as\n50%. For example, in 10 ×10 with a noise level of 1% the number of atoms needed is about\n1,800. If noise is increased to 2 .5% the number of atoms needed grows to 4 ,200 and give an\nerror rate under 1% and lower error of about 0 .3% if we use multiple atomizations compatible\nwith the same dual, as explained in Section 4.1.\nIn the case of the MNIST handwritten character dataset [5] there is no proper way to\ndeﬁne a exact solution, however, anything we would consider as a good candidate has atoms\nof all sizes, from a few constants to (almost) 784. Again, small approximate solutions with\nabout 1% error rate can be found with much smaller atoms, most contained in about 4 to 10\nconstants only.\n3.4 Memorizing and generalizing models\nThe abilities of memorizing and generalizing are not incompatible in humans, nor should they\nbe for machine learning algorithms. However, it seems to exist some kind of fundamental\ntrade-of between memorizing and learning. Memorizing, instead of generalizing, (also known\n27as overﬁtting) is a frequent problem for statistical learning algorithms.\nMemorizing may not be bad per-se but it is always expensive. It comes at a cost. The\ncost of memorizing for algebras is growing the model. Memorizing a relation requires to add\na new atom or to make some of the existing atoms “larger”. Speciﬁcally, an atom φ is larger\nthan atom ω if for each constant ω <cwe have φ<c .\nLearning, in the other hand, may yield a negative cost; learning a relation can make the\natomization smaller or at least grow it by an amount that is less than the information needed\nto store the relation.\nA goal of algebraic learning is to ﬁnd a small model. We want small models not only\nbecause large models are expensive but also because an atomized model with substantially\nfewer atoms than the number of independent input relations is going to generalize. In the next\nsection and in Appendix E we establish a relationship between generalization and compression\nrate for random models.\nSmallness alone, however, is not enough to guarantee a generalizing model. Data com-\npressors produce small representations of data but do not generalize. Furthermore, in order to\nacquire the information needed to extract relevant features from data, generalizing models may\nneed to “grow” in an initial learning stage. In this sense generalizing algorithms behave very\ndiﬀerently than a data compressor. So, if smallness is not enough, what is missing?\nIn order to answer this question we are going to study ﬁrst the memorizing models. We\nstart by describing two algorithms that produce models that act as memories of R. It is not\nnecessary for the reader to understand how these algorithms work to follow the discussion\nbelow.\nTo build a memory we need to encode the training positive order relations R+ as directed\nedges in the graph of M∗, construct the set Λ formed by the constants [ b] such that there is\nsome a and some relation ¬(a < b) ∈R−, add a diﬀerent atom in M∗ under each constant of\nΛ, add to the dual of each term mentioned in any relation of Rthe atoms in the intersection of\nthe duals of the term’s component constants and calculate the transitive closure of the graph\nof M∗. Then there is a one-to-one mapping between each atom of M∗ and one atom we can\nintroduce in M.\nNow, for each atom ξ ∈M∗ introduce an atom φξ ∈M that satisﬁes for each constant c\nin M:\n¬(ξ <[c]) ⇔( φξ <c). (33)\nIf a and b are constants it follows easily that\nφξ ∈dis(a,b) ⇔ξ ∈dis([b],[a]), (34)\n28so ¬(a < b) holds in M if and only if ¬([b] < [a]) holds in the dual (see the notation in\nAppendix A for the deﬁnition of discriminant). Theorem 2 extends this correspondence\nbetween discriminants to terms and therefore proves that the atomization of M satisﬁes the\nsame positive and negative relations than M∗.\nThe ﬁrst thing we should realize is that this algorithm is not stochastic. There is a single\nmodel produced by the algorithm. The model memorizes the negative input relations R− so,\nfor any pair of terms or constants aand b, the relation a<b holds unless R|= ¬(a<b ). To the\nquery “(a < b)?” the model always answers “yes” unless the input relations imply otherwise.\nThe model is therefore unable to distinguish most elements as it also satisﬁes b<a .\nThe second thing we should realize is that this model does not need more atoms than\nnegative relations we have in R−. The model may be small (particularly if there are many\nmore relations is R+) and despite that entirely unable to generalize.\nThe ability of a model to distinguish terms is captured by the concept of algebraic freedom.\nA model N is freer them a model M if for each pair a and b of elements, ¬(a < b) is true in\nM implies that it is also true in N. The freest model of an algebra (any algebra, not only a\nsemilattice, a group for example) corresponds with its “term algebra”. The term algebra is\na model that has an element for each possible term and two terms correspond with the same\nelement only if their equality is entailed by the axioms of the algebra.\nThe axioms of our algebra are the axioms of a semilattice plus the input relations R. The\nmodel produced by the memorizing algorithm above is precisely the least-free model compatible\nwith these axioms. What about the freest model?\nThe freest model can also be easily built; we do not even need the auxiliary M∗. To build\nthe freest model introduce one diﬀerent atom under each constant of M and then enforce all\nthe positive relations of R+ one by one using full crossing.\nAgain, this algorithm is not stochastic and there is a single output model. Since the number\nof atoms of this model tends to grow geometrically with the number of positive relations in R+\nwe usually end up with an atomization with many atoms.\nNot surprisingly, this model also behaves as a memory. This time it remembers the relations\nin R+ and to the query ( a < b)? the model always answers “no” unless R+ |= (a < b). The\nmodel distinguishes most terms but it does not generalize and it is so large in practical problems\nthat usually cannot be computed.\nThe freest and least-free models are both memories. However, there are important dif-\nferences; free models are very large while least-free models are small. In addition, least-free\nmodels tend to produce larger atoms than freer models. Figures 1 and 2 depict the atoms of\nthe freest and least free models of the toy vertical-line problem in 2 ×2 dimension.\n29We want to keep our generalizing models reasonably away from the memorizing models.\nBecause we speciﬁcally seek for small models staying away from least-free models, that are also\nsmall, is fundamental. We do not need to worry about free models that behave as memories\nbecause they are large and cardinal minimization only ﬁnds small solutions. The ingredient we\nneed to complement cardinal minimization is algebraic freedom.\nFigure 1:Atoms of the freest model that satisfy the training examples for the toy problem of identifying vertical\nlines in dimension 2×2. A black square with a white border represents a pixel whose black color and white color\nconstants contain the atom.\nFigure 2: Atoms of the least free model that satisfy the training examples for the toy problem of identifying\nvertical lines in dimension 2 ×2.\nFigure 3: Atoms of a generalizing model that satisfy the training examples for the toy problem of identifying\nvertical lines in dimension 2 ×2.\nThe Sparse Crossing algorithm enforces positive relations one by one using crossing just\nlike the algorithm that ﬁnds the freest model. However the crossing is sparse in order to make\n30the model smaller (Figure 3). In this way, a balance between cardinal minimization and freedom\nis naturally obtained. In this balance many generalizing models can be found.\nIn both, the generalizing Sparse Crossing algorithm and the algorithm that ﬁnds the freest\nmodel, we start from an algebra that is very free and satisﬁes all the negative relations, and\nthen we make it less free by enforcing the positive relations using crossing until we get a\nmodel of R. If we use full crossing, we reduce freedom just by the minimal amount needed\nto accommodate the positive order relations (see Theorem 6). If we use Sparse Crossing, we\nreach some compromise between freedom and size.\nWe saw in Section 3.3 that a few atoms can distinguish images with vertical lines from\nimages without them. This is because the atoms are small, edged only to a few constants.\nLarger atoms are less discriminative, in the sense that they distinguish between fewer terms\nthan smaller atoms. The atoms that resolve the MNIST dataset [5] are also small, most edged\nto fewer than 20 constants, many edged to as few as 5 or 6 constants which is much less than\nthe 1568 constants needed to describe the images. Smaller atoms are more useful and increasing\nalgebraic freedom pushes for smaller atoms.\nFor the toy problem the atoms produced by least-free models are contained in 4 constants\neach plus v. The atoms of the freest memory are only in two constants and v. The generalizing\nsolution has 4 atoms all in two constants (plus v) each.\nAlgebraic freedom is also at the core of the batch learning method based on adding pinning\nrelations. Pinning relations are all negative. It can be proved that the pinning relations of a\nmodel M capture all negative relations between terms. This means that whatever model we\nbuild compliant with this negative pinning relations is going to be able to distinguish between\nthe pairs of terms that are discriminated in M. Any model that satisﬁes the pinning relations\nof M is strictly freer than M. See Theorem 3.\nIn favor of freedom maybe be argued that distinguishing between terms is by itself a\ndesirable property of a model that understands the world. Irrespectively of whether algebraic\nfreedom is fundamental or not, it is certainly a good counterbalance to cardinal minimization\nin semilattices.\n3.5 Small and random algebraic models imply high accuracy\nIn the previous section we saw that Sparse Crossing balances cardinal minimization with al-\ngebraic freedom to stay away from memorizing models. In this section we prove that to ﬁnd\na good generalizing model we only need to pick at random a small model of the training set\nR. We also show that Sparse Crossing corresponds well with this theoretical result for small\nerror. By seeking algebraic freedom, Sparse Crossing ﬁnds models of R at random, away from\n31easier-to-ﬁnd, non-random, least-free models that act as memories.\nSo far we have not worked with the idea of accuracy or error explicitly. Instead we have\nfocused in ﬁnding small algebras that obey the training examples. We thus need to establish a\nformal link between small models and accuracy. There is some intuition from Physics, Statistics\nor Machine Learning that simpler models can generalize better, but it is still not obvious that\nthe smaller the algebraic model the higher the accuracy.\nWe can demonstrate that for an algebra chosen at random among the ones that obey a\nlarge enough set R of training examples, the expected error ϵ in a test example is ( Appendix\nE)\nϵ= ln |ΩZ|−ln |ΩZ,R|\n|R| , (35)\nwith Ω Z the set of all possible atomizations with Z atoms using C constants and Ω Z,R\nthe set of atomizations that also satisfy R. The larger is the ﬁrst set, Ω Z, the more training\nexamples are going to be necessary to produce a desired error rate. On the other hand, the\nlarger is the second set, Ω Z,R, the fewer training relations are needed.\nThe quantity ln |ΩZ,R|measures the degeneracy of the solutions and is a subtracting term\nthat works in to further reduce test error. ln |ΩZ|is an easy to calculate value that only de-\npends upon Z and the number of constants and determines an upper bound for the number of\nexamples needed to produce an error rate ϵ. Assuming that our constants come in pairs so the\npresence of one constant in a term implies the absence of the other (like the white pixel and\nthe black pixel constant pair for images), the total number of atoms is 3 p with pthe number of\npixels or 3C/2 for the constants C = 2p. Then ln |ΩZ|≈ ln(3) ZC\n2 . Ignoring, for the moment, the\nbeneﬁcial eﬀect of ln |ΩZ,R|and substituting above we can determine a worse-case relationship\nϵ= ln 3\n2\nC\nκ, (36)\nwhere κis the compression ratio κ= |R|/Z. This expression implies that the more the algebra\ncompresses the R training examples into Z atoms, the smaller the test error, and with a\nconversion between the two rates upper bounded by the factor C ln(3)/2. Picking randomly\nan algebra that satisﬁes R with a given compression ratio κ would make for a good learning\nalgorithm. It would have much better performance than using the non-stochastic, memory-like\nalgebras of the previous section. The random picking, though, is an ideal algorithm we cannot\neﬃciently compute.\nThe term ln |ΩZ,R|can be estimated using the symmetries of the problem (see Appendix\nE.2). For the problem of detecting the presence of a vertical bar we can permute the rows and\n32the columns of an input image without aﬀecting its classiﬁcation. For d×d image, we derived\nln |ΩZ,R|>2 Z ln(d!), giving\nϵ= d2 ln 3−2 ln(d!)\nk (37)\nwhere C = 2p= 2d2 has been used.\nWe have compared this prediction with the experimental results using the Sparse Crossing\nalgorithm, and found that Sparse Crossing performs always better than this theoretical value.\nWe have observed, however, that the higher the size of R and the smaller ϵbecomes the closer\nare the observed values to this theoretical result. We also noticed that the harder the problem\nis, the faster the approach to the theoretical result as |R|increases. In order to determine if\nwe can ﬁnd the theoretical prediction for a large value of |R|, we thus used a harder, albeit\nsimilar, problem to the detection of vertical bars. Instead of detecting the presence of vertical\nbars, we used the much harder problem of separating images with an even number of complete\nbars from images with an odd number of complete vertical bars in the presence of noise. This\nproblem has the same symmetries than the simpler vertical bar problem so it should obey the\nsame relation we have derived relating compression and error rates.\nWe generated a large number of training images. We did so by adding to an otherwise\nwhite image a random number of black vertical lines in random positions. The white background\npixels are then turned into black with some probability. The training protocol started with 200\ntraining images. If the test error increased (decreased) in the next epoch we used 2% more\n(2% less) training images in the next batch. Experimental results show a clear proportionality\nbetween ϵand κ−1 with a proportionality constant that increases slowly as error rate decreases\nuntil clearly stabilizing (see Figure 13) at a value that diﬀers from the theoretical prediction\nin less than 10% for 7 ×7 and about 5% for 10 ×10 images. To get to this point we had to use\nas many as 37 million examples.\nWe plotted the theoretical prediction in Equation (37) as a straight line, in green for 7 ×7\nimages and in blue for 10 ×10 (Figure 4). The experimental results from Sparse Crossing are\nplotted as dots, again in green and blue for 7 ×7 and 10 ×10 images, respectively. Figure 4a\nis a logarithmic plot that allows depicting the behavior of Sparse Crossing results for all values\nof error and compression obtained during learning. The linear scale in Figure 4b is used to\nshow the behavior at low errors, where algebraic learning and the theoretical expression show\na good match.\nWe see a remarkable match between observed and theoretical values when error rates\nare small. For fewer training examples and higher error rates, Sparse Crossing appears more\neﬃcient that the theoretical result. This could be due to the assumptions made to derive the\ntheoretical relation (high values of Rand low values of ϵ) or, perhaps, Sparse Crossing is indeed\n33more eﬃcient than a random picking. Sparse Crossing searches the model space far away from\nmemorizing least-free models and that can give it some edge over the random picking. However\nthe volume taken by memorizing models compared with the overall volume of the space of\nmodels of R is small, so we speculate that the measured superiority of Sparse Crossing over\nthe theoretical derivation for the random picking is just due to the restricted validity of the\ntheoretical result to very low error rates.\nFigure 4: Testing the relationship between compression and error rate in Equation (37) a . Loga-\nrithm of error, ln(ϵ) versus logarithm of compression, ln(ϵ) for the problem of separating noisy images with even\nfrom odd number of vertical bars. Theoretical expression plotted as straight line, in green for 7×7 images (noise\n5%) and in blue for 10 ×10 images (noise 2.5%). Algebraic learning results plotted as dots, using the same\ncolor scheme as for lines. b Same as a. but in linear scale to show results at low error and high compression.\n4 Classiﬁcation of hand-written digits\nOur ﬁrst example of the vertical bar problem was simple enough to facilitate analysis. In this\ncase there is a simple formula that separates positive from negative examples. In this section we\nshow that algebraic learning also works in real-world problems for which there is no formal or\n34simple description. For this, we chose the standard example of hand-written digit recognition.\nA digit cannot be precisely deﬁned in mathematical terms as was the case with the vertical\nbar, diﬀerent people can write them diﬀerently and the standard dataset we use, MNIST [5],\nhas miss-labels in the training set, all factors making it a simple real-world case.\nWe used the 28×28 binary version of images of the MNIST dataset, with no pre-processing.\nThe embedding technique is the same we applied to the toy problem of the vertical bar. An\nimage is represented as an idempotent summation of 784 constants representing pixels in black\nor white. Digits are treated as independent binary classiﬁers. The speciﬁc task is to learn to\ndistinguish one digit from the rest in a supervised manner. We use one constant per digit, each\nplaying a similar role than the constant v of the toy problem, in total 1 ,578 constants.\nOur training protocol was as follows. We used 60,000 images for training. Training epochs\nstarted with batches containing 100 positive and 100 negative examples. When identiﬁcation\naccuracy in training did not increase with training epoch, the number of examples was increased\nby a 5% until a maximum of 2,000 positive and 2,000 negative examples per batch. Increasing\nbatch size and balancing of positive and negative examples seemed to accelerate convergence\nto some limited extent, but we did not ﬁnd an impact in ﬁnal accuracy values. Each digit was\ntrained separately in a regular laptop.\nFor standard machine learning systems, data are separated into training, validation and\ntest. Validation data is used to ﬁnd the value of training hyperparameters that give highest\naccuracy in a dataset diﬀerent to the one used in training. This is done to try to avoid\noverﬁtting, that is, learning speciﬁc features of the training data that decrease accuracy in the\ntest set. We found no overﬁtting using algebraic learning ( Figure 5, top). This ﬁgure gives\nthe error rate in the test set for the recognition of digits “0” to “9” as a function of the training\nepoch. The error decreases until training epoch 200, from which it stays constant except for\nsmall ﬂuctuations. As we did not ﬁnd overﬁtting using algebraic learning, we did not need to\nuse a validation dataset in our study of hand-written recognition.\nAfter training, we found that the error rate in the test dataset (a total of 10 ,000 images)\nvaries from 1 .63% for digit “1” to 6 .46% for digit “8” (see Table 1(A) for all digits), and an\naverage error rate of 4 .0%.\nMost atoms found consist of scattered white and black pixels, Figure 6. After training\nwith a batch, the positive examples of the batch contain all master atoms while the negative\nexamples contain less than all master atoms. This translates into master atomizations for which\neach atom is contained in at least one pixel of each positive example. Each atom corresponds\nto groups of pixels shared more frequently by positive examples (to give a minimum number of\natoms) that appear less frequently in negative examples (to produce atoms of a minimum size\nso algebraic freedom is maximized). Also, pixels containing many atoms are correlated with\n35Figure 5: Learning to distinguish one hand-written digit from the rest. Error rate in test set for\nthe recognition of MNIST digits “0” to “9” in dimension 28 ×28 at diﬀerent training epochs using a single\natomization. Digits are trained separately as binary classiﬁers. Learning takes place within the ﬁrst two hundred\nepochs. Repeated training afterwards with the same examples does not aﬀect error rate.\nthe pixels more frequently found in the inverse of most negative examples. In this way the\nprobability for a negative example to contain all atoms is small.\nMost atoms are contained in only a few pixels but we found a few atoms that resemble the\ninverse of rare versions of digits in the negative class. For example, a “6” that is very rotated\nin the third row and four column of Figure 6 is an atom found during the algebraic training\nof digit “5” versus the rest of digits. These untypical training examples are learned by forming\na speciﬁc memory with a single atom and in this way their inﬂuence in the form of the other\natoms can be negligible. This may a reason for algebraic learning not being severely aﬀected\nby mislabelings.\n36Figure 6: Atoms in digit recognition. Example atoms in the training for digit “5”.\n37A: Single master atomization\nDigit Error (%) FPR(%) FNR (%)\n0 2.17 2.15 2.35\n1 1.63 4.15 1.50\n2 4.54 5.47 7.94\n3 5.75 3.87 8.22\n4 4.29 4.02 8.15\n5 4.32 2.40 7.40\n6 2.65 3.70 5.01\n7 3.92 6.08 5.84\n8 6.46 4.84 9.96\n9 5.06 2.15 7.04\nAverage 4.08 3.83 6.34\nB: 10 master atomizations\nDigit Error (%) FPR(%) FNR (%)\n0 0.97 0.99 0.82\n1 0.69 0.65 0.97\n2 1.60 1.40 3.29\n3 2.44 2.40 2.77\n4 1.80 1.68 2.85\n5 1.61 1.57 2.02\n6 1.48 1.41 2.09\n7 1.36 1.14 3.31\n8 2.54 2.47 3.18\n9 2.29 2.12 3.77\nAverage 1,68 1.52 2.41\nTable 1:Errors, false positives and false negatives in recognition of hand-written digits in test set for (A) one\nmaster atomization, and (B) for 10 master atomizations requiring 5 or more agreements to classify an example\nas positive and fewer than 5 as negative.\n384.1 Using several master atomizations\nThe result of embedding a batch of training examples is an atomization satisfying all the\nexamples in the batch. At each epoch, a suitable atomization of the dual is chosen of the\nmany possible and then the Sparse Crossing algorithm produces an atomization of the master\nconsistent with the training set and the pinning relations or a subset of them. Enforcing of\nthe batch is carried out using a stochastic algorithm over the chosen atomization of the dual,\nwhich contains the pinning terms learned in previous epochs. The enforcing of a batch then\nresults in one of the many suitable atomizations of the master algebra.\nNothing prevents us from using more than one atomization for a training or a test batch.\nChanging the atomization for the dual results in diﬀerent trace constraints and, hence, in a\ndiﬀerent atomization for the master. For the hadwritten digits problem, we counted how many\namong 10 atomizations classify the positive test images as positive. In Figure 7 we show\nresults for digits “0” and “9”. For digit “0”, for example, approximately 90% of the positive\ntest images have the 10 atomizations agreeing in that a digit is indeed a digit “0”. For less\nthan 8% of the test cases, it is 9 out of the 10 atomizations that agree in that the digit is a “0”.\nAgreement of less of the algebras meet with even smaller percentages of the cases. We found\nthat, for digit “0”, more algebras agree the more round the digit ( Figure 7, top insets). For\ndigit “9”, disagreement exists for incomplete and rotated version of the digit ( Figure 7, top\ninsets). Using more than one atomization is a simple procedure that extracts more information\nfrom the algebra. In Table 1B we give test set results using the atomizations obtained from\nthe last 10 epochs of training.\nThe number of examples correctly classiﬁed as positive in Figure 7 increases approximately\nas an exponential with the number of agreements. A very similar exponentially-looking function\nis observed when we plot the number of negative examples versus the number of atomizations\ncorrectly agreeing on a negative (plot not shown). The exponential increase might be un-\nderstood with each image I having a probability pI of misclassiﬁcation when using a single\natomization. The value pI is typically low for most example images but it may be high (even\ncloser to 1) for some diﬃcult images. For each image, a binomial distribution describes the\nnumber of times it is misclassiﬁed among the 10 tests corresponding with 10 diﬀerent atom-\nizations. The distribution for all test images should be a mixture of binomial distributions\nwith diﬀerent values of pI and, with most images been easy to identify, the weight of the easily\nidentiﬁable examples dominates producing the exponentially-looking distribution of Figure 7.\nThere is a subset of the test examples for which a small probability of misclassiﬁcation\nexists even though they may look very clear to a human. However, the risk of misclassiﬁcation\ndue to this intrinsic probability of failure goes away exponentially if multiple atomizations are\nused. A few atomizations should suﬃce to classify correctly these examples with small pI. On\n39Figure 7: Agreement of 10 master atomizations. Top: Count of the number of test examples correctly\nclassiﬁed as digit “0” by 0,1,2,..., 10 master atomizations out of 10 atomizations. Most images are correctly\nidentiﬁed by all the atomizations. Insets are examples of images of digit “0” for complete agreement, disagree-\nment and complete lack of identiﬁcation. Bottom: Same but for digit “9”\nthe other hand, doesn’t matter how many atomizations we use we cannot expect to correctly\nclassify the examples with high pI. In this case only additional training with new examples can\nimprove the rate of success. Training with the same examples neither increases nor decreases\nthe error rate.\nUsing the criterion that at least 7 or more of the atomizations need to agree that an image\nis a “0” to declare it a “0”, obtains an error rate of 0 .6% for this digit, with false positive and\nnegative ratios of FPR = 0 .5% and FNR = 1 .22%, respectively. The same criterion ﬁnds for\ndigit “9” an error of 1 .36%, and FPR = 0 .8% and FNR = 6 .5%. The average over all digits is\nfound to give an error rate of 1 .07% and FPR = 0.56% and FNR = 5.6%.\nA criterion consisting of requiring 5 or more atomizations to agree that an example is\n40Figure 8: False positive and negative ratios using multiple master atomizations. Left: The logarithm\nof the false positive and false negative ratios for digit “5” in dimension 28 ×28 requiring 5 or more agreements\nout of 10 master atomizations to classify an example as positive and fewer than 5 as negative. False positive and\nnegative ratios decrease fast at ﬁrst but subsequent training using the same training examples does not improve,\nnor deteriorate, results. Right: Same criterion applied to the classiﬁcation of even vs odd number of vertical\nbars in images of size 10 ×10 in the presence of 2.5% noise. In this case new training examples are used at\neach epoch which results in monotonically decreasing false positive and negative ratios.\npositive gives more balanced false and negative ratios, FPR = 1 .5% and FNR = 2 .4% but a\nhigher total error rate of 1 .68% (see Table 1B for all digits). A higher false negative ratio is\nconsistent with the fact then we have 10 times more negative examples than positive examples.\nThe MNIST dataset has a limited training set that does not allow to see the eﬀect of\nmultiple atomizations in test results at very low error rates. For the problem of separating\nnoisy images with even vs odd number of vertical bars we have an unlimited supply of training\nexamples. In this case we get the results of Figure 8, on the right. The false positive and\nnegative ratios using 10 atomizations decrease with the training epochs and at all times during\nthe training remain signiﬁcantly smaller than the error rate obtained with a single atomization.\nAn almost perfect exponential dependence of the example count with the number of agreements\n(like in Figure 7 ) is also observed for both, the positive and negative examples (data not\nshown). It doesn’t matter how much training we do there is always an advantage in using a\nfew master atomizations to extract the most information from the algebra.\nFor the MNIST dataset, using 10 master atomizations leads to a reduction of the overall\nerror rate from 4 .0% to 1 .07%. We asked if this is the best we can do. To answer this, in the\nfollowing we investigate how much information can be extracted from the pinning terms.\nFor the handwritten digits, a single atomization in the master has of the order of few\nhundred atoms. As a a consequence of cardinal minimization of the algebra, each negative\nexample of a training batch contains typically all atoms except one. Cardinal minimization is\nﬁnding the right atoms but is not optimizing error rate. Error rate decreases as a side eﬀect of\ncardinal minimization. In fact there is no need other than reducing the size of the representation\nfor requiring a single atom miss to separate negative from positive examples.\n41To further reduce error rate, we may consider a separation of positive from negative ex-\namples using more than a single atom. Pinning terms are derived from atoms so atoms can\nbe recovered from pinning terms. If we convert all pinning terms back into atoms, negative\nexamples are separated from positive examples by many atom misses. However, many positive\nexamples now also have a few atom misses. We thus proceed in the following way. Deﬁne\nmisses cut-oﬀ as the arbitrary maximum number of misses allowed for an example to be de-\nclared positive. For digit “0”, for example, we ﬁnd an interval of misses cut-oﬀ of 10 −50 with\nan error rate below 1%, with a minimum of 0 .36% error rate for 23 misses. Digits diﬀer in the\noptimal misses cut-oﬀ, with values from 13 to 27, but all have quite ﬂat error rates in a wide\ninterval. Diﬀerent cut-oﬀs could be deﬁned to minimize error, false positive or false negative\nratios. The best error rate obtained gives an overall 0 .78% for the 10 digits. Error rates for all\ndigits are given in the table of Figure 9 for the cut-oﬀs that minimize error.\nThis value of 0.78% for the error rate is lower but similar to the 1 .07% error rate obtained\nusing 10 master atomizations.\n42Digit Error (%) Misses FPR(%) FNR (%)\n0 0,36 24 0,19 1,90\n1 0,28 15 0,11 1,80\n2 0,75 26 0,45 3,5\n3 1,083 17 0,51 6,24\n4 0,841 27 0,49 4,00\n5 0,792 27 0,28 5,40\n6 0,76 13 0,20 5,80\n7 0,735 20 0,33 4,38\n8 1,235 22 0,75 5,60\n9 1,017 19 0,36 6,93\nAverage 0,78 0,37 4,55\nFigure 9: How pinning terms best distinguish positive from negative test images using diﬀerent\nnumber of allowed misses (misses cut-oﬀ). Top: Error rate and false positive ratios for test images of\ndigit “0” using all the atoms recovered from the pinning terms of the algebra. Errors are shown as a function\nthe misses cut-oﬀ. Middle: Same as top but for digit “9”. Bottom: For each digit, minimal error rate and\ncorresponding false positive and negative ratios.\n435 Solving the N-Queens Completion Problem\nSo far we have used three supervised learning examples in which an algebra is trained to learn\nfrom data. Algebras can learn from examples but they can also incorporate formal relationships,\nfor example symmetries or known constraints. They are also capable of learning in unsupervised\nmanner.\nIn the following we study the N-Queens Completion problem as such a case in which we\nincorporate several relationships. In this problem, we ﬁx N queens to N positions of a M×M\nchessboard, and we want the algebra to ﬁnd how to add M −N queens to the board so none\nof the M queens attack each other. In the following we detail how to embed this problem into\nthe algebra.\n5.1 Board description\nA simple and eﬀective embedding uses of 2 N2 constants to describe the board, two constants\nfor each board square. A constant Qxy describes that board position ( x,y) contains a queen.\nA constant Exy describes that board position ( x,y) is empty. There is no need to distinguish\nwhite and black squares.\nA board or subset of the board is represented by a termBthat is an idempotent summation\nof some constants Qxy and Exy.\n5.2 Attack rules\nTo encode the queen attack rules we used an additional constant U. Let A(x,y) be the set of\nboard squares attacked by a queen at ( x,y), and let’s agree that A(x,y) does not include the\nsquare (x,y). When avoiding attacks, represented by the presence of constant U, a queen at\n(x,y) implies the presence of empty squares at each position in the set A(x,y),\n∀x∀y∀i∀j ((i,j) ∈A(x,y) ⇒Eij <U ⊙Qxy) . (38)\nBy adding these rules to the training set, the idempotent summation of constant U and a board\nB (or a subset of a board) with one or more queens results in an extended board that has the\nempty square constants at positions attacked by the queens in B.\n5.3 Rule to add queens\nThe previous rule adds empty squares to a board subset. We can also write a rule that adds\nqueens by extending the deﬁnition of constant U. When the term B contains a subset of a\n44board with a row or column of empty squares missing just one square, the summation of U and\nB completes the column or row by adding a queen,\n∀x∀y(Qxy <U ⊙i,i̸=x Eiy) (39)\n∀x∀y(Qxy <U ⊙j,j̸=y Exj), (40)\nwhere we used ⊙i,i̸=x to represent the idempotent summation of all values of i except i= x.\n5.4 Deﬁnition of Rx and Cy\nTo place M non-attacking queens on a M ×M board, all rows and all columns should have a\nqueen. We introduced an additional set of 2 N constants, Rx and Cy, to require that a queen\nmust be present at every row x and at every column y of the board B. We deﬁne these two\nconstants with the help of the order relations\n∀x∀y(Rx ⊙Cy <Qxy), (41)\nand the negative relations\n∀x(Rx ̸<(⊙ij,i̸=xQij) ⊙(⊙ijEij)), (42)\nand\n∀y(Cx ̸<(⊙ij,j̸=yQij) ⊙(⊙ijEij)), (43)\nwhere idempotent summations run along all possible values of indexes i and j and ⊙ij,i̸=x\nrepresents a summation for all board positions except those with row equal to x.\n5.5 Independence rules\nNow let’s encode the independence of board square constants. No term B representing a\ncomplete or partial chessboard should contain a queen at ( x,y), Qxy ̸< B, unless Qxy is a\ncomponent in the explicit deﬁnition of term B. To capture this we require that\n∀x∀y(Qxy ̸<(⊙ij,(i,j)̸=(x,y)Qij) ⊙(⊙ijEij)), (44)\nand analogously for empty spaces,\n∀x∀y(Exy ̸<(⊙ij,(i,j)̸=(x,y)Eij) ⊙(⊙ijQij)). (45)\nIf Exy is not one of the components deﬁning term Bthen it follows that B <(⊙ij,(i,j)̸=(x,y)Eij)⊙\n(⊙ijQij) and than Exy <B contradicts the independence relation above. Note that if the rule\nto add a queen and the attack rules were deﬁned without using the extra constantU they would\ncontradict the independence rules.\n45Similar relations can be written for constants Rx and Cy representing any queen in a row\nx or in a column y as\n∀x(Rx ̸<(⊙ij,i̸=xQij) ⊙(⊙ijEij)) (46)\n∀y(Cy ̸<(⊙ij,j̸=yQij) ⊙(⊙ijEij)) (47)\nWe can add additional independence order relations such as\n∀x∀y(Qxy ̸<U ⊙Exy), (48)\n∀x∀y(Exy ̸<U ⊙Qxy), (49)\n∀x∀y(Qxy ̸<U ⊙i Ri ⊙j Cj), and (50)\n∀x∀y(Exy ̸<U ⊙i Ri ⊙j Cj). (51)\n5.6 Embedding an N Queens Completion game\nWe refer to all the above rules as “the rule set”. Now we are going to add additional relations\nto encode a particular N-completion game. We use a constant S to represent the solution we\nare looking for. We want to ﬁnd a completion for a board already with, say, two queens ﬁxed at\npositions (p,q) and (r,s). To require a solution with the two ﬁxed queens we add the relation:\nQpq ⊙Qrs <S. (52)\nThe solution S should be a particular conﬁguration of queens and empty positions on a board,\nand must therefore be contained in the set of all possible conﬁgurations, or equivalently in the\nidempotent summation of all board squares both empty and with a queen,\nS <⊙ij (Eij ⊙Qij). (53)\nHowever, no board position can be simultaneously empty and with a queen, that is, it cannot\ncontain both Exy and Qxy,\n∀x∀y(Exy ⊙Qxy) ̸<U ⊙S. (54)\nWe also know that M non-attacking queens on a M ×M chessboard must occupy each row,\n∀x(Rx <S ), (55)\nand also each column,\n∀y(Cy <S ). (56)\n465.7 Solving the 2-blocked 8 ×8 completion problem\nThe “rule set” and the rules to “embed an N-queen completion game” are the complete set R\nof input order relations. We enforce R at each epoch. Figure 10 gives the results of several\nepochs of algebraic learning. We chose the initial state to be two queens in positions b4 and\nd5 (Figure 10, queens in blue). After the ﬁrst run of the Sparse Crossing algorithm, we get\nan incomplete board ( Figure 10, epoch 1). The board is plotted by querying at each board\nsquare if relation Qxy < Sor relation Exy < Sis satisﬁed. When we ﬁnd that neither of the\ntwo relations are satisﬁed we add a question mark to that position in Figure 10. In this ﬁrst\nepoch, all positions except those attacked by the two initial queens are in question mark.\nThe second epoch has some pinning terms and pinning relations deﬁned from the atoms\ngenerated in the ﬁrst epoch. We then ﬁnd a new atomization for the dual that also satisﬁes the\npinning relations generated in the ﬁrst epoch. R is already satisﬁed in the master before the\nsecond epoch starts, however, the trace constraints are not because the atomization of the dual\nhas changed. Enforcing trace constraints introduce new atoms in the master that have to be\ncrossed. As a result we get a diﬀerent atomization for the solution S and the other constants.\nThis is not very diﬀerent from what we did for the handwriten character recognition. Using\nmultiple master atomizations, extracts more information from the pinning terms. With each\natomization of the master, the algebra looks at the solution from a “diﬀerent angle” and it can\nlearn from it by creating new pinning terms and relations.\nEventually, at epoch 12, a complete board is found. It is remarkable that a solution is\nfound without searching for a particular conﬁguration. Following the chessboards generated at\neach epoch, the solution seems to appear “out of the blue” in the sense of not showing any\nintermediate boards. The algebra is learning the structure of the search space. When enough\npinning terms are added, the algebra can produce board solutions.\nThe approach seems to beneﬁt form inserting idle cycles (epochs) for which no ﬁxed queens\nare set and only the order relations of the rule set are enforced. In the problem of Figure 10,\niddle cycles were used in epochs 8,9 and 10 and 19, 20 and 21. In this way, we could ﬁnd the\ntwo diﬀerent completions compatible with the initial queens in epochs 12 and 28.\n47Figure 10: Algebraic learning of the 2-blocked 8 ×8 queens problem. Chessboards at diﬀerent learning\nepochs arranged in increasing epoch order. They are generated by querying the algebra at each board position\nfor presence of queen, presence of empty square or absence of both, marked as ’?’.\nFigure 11: A subset of atoms for the 2-blocked 13 ×13 queens problem. Master atoms for epoch 42,\nwith white corresponding to empty, black to queen, red both, and gray none.\n485.8 Algebraic learning in larger chessboards\nThe straightforward approach of the previous section works well for 8 ×8 or larger boards, like\n13 ×13. At least for low dimensions, algebraic learning is capable of ﬁnding complete boards\nat once, without apprently following intermediate steps.\nWe found, not surprisingly, that building solutions step by step works better for larger\nboards. At each epoch we inserted a queen at some random but legal position. The algebra\nhad no instructions regarding what to do with the inserted queens. It can keep them or eliminate\nthem. A queen is inserted in the board by adding the relation Qxy <S in just one single epoch.\nIn following epochs it is up to the algebra to keep it there or not.\nWe studied the case of a 17×17 chessboard. When queens are located at legal but random\npositions the usual outcome is a board with fewer than 17 queens. Adding more queens is not\npossible without attacking others. The situation is diﬀerent for algebraic learning.\nConsider the following experiment. We ran 33 attempts at ﬁnding a complete board for 17\nqueens with one blocked. Each attempt consisted of 20 epochs, 17 epochs were a legal queen is\nadded and 3 additional idle epochs. The ﬁrst attempt starts with a blocked queen at position\nc10 and in every epoch a legal queen is added, if possible. For the ﬁrst 26 attempts no complete\nboard was achieved.\nAt attempt 27 a complete board was found ( Figure 12, left). In this ﬁgure, the initial\nblocked queen is in blue at position c10, in red the queens we randomly introduced and in\nblack the queens the algebra found.\nThe simpliﬁed dynamics of this experiment can be summarized in the following way. The\nboard starts with a blocked queen (1 queen on board), algebraic learning rans for an epoch\nand resulted a board with two other queens (3), we then added a queen randomly to a legal\nposition (4). A new learning epoch kept this queen but eliminated the previous ones (2), then\nﬁve queens were inserted randomly one by one and kept (7 queens on board). Another queen\nwas then inserted and the algebra added a new one (9 on board), then again three more queens\nwere inserted (12). Finally, one queen was randomly inserted and 4 more created by the algebra\nat once, making a total of 17 and the board was completed.\nAt attempt 33 another complete board is found ( Figure 12, right). The simpliﬁed dy-\nnamics at this attempt was: starting with a blocked queen at c10 (1 queen on board), we\nadded two queens (3 queens on board) and two were inserted by the algebra (5 on board), more\nqueens were randomly inserted for 5 steps (10 on board) and then 7 appeared at once, added\nby the algebra to a total of 17 legal queens.\nMany atoms obtained resemble legal or almost legal board subsets. Most of these atoms\n49correspond to boards with a few queens, some a single one, but others look like boards with\nsmall groups and some with larger groups of queens ( Figure 12, bottom). Atoms look similar\nwhen no queens are manually inserted ( Figure 11).\nWe repeated the same experiment 10 times with a 17×17 board. Each experiment consists\non a number of attempts to produce a complete board, typically around 60 (seeTable 2). Each\nof these attempts consists in adding at most 17 queens. We also tested that these results of\nalgebraic learning cannot be explained by random placement of queens on legal positions. We\ncompared the results of the 10 experiments with a purely random placement of 17 legal queens.\nThe purely random case produces a board in an attempt with probability p = 0 .008. For\neach experiment we give in Table 2 the probability to produce by chance a similar or better\nresult. Overall, this probability is less than p = 7 ×10−12. In contrast to the random case,\nalgebraic learning seems in many cases to take a number of attempts to produce a full board\n(say epoch 24 in Experiment 1 or 37 in Experiment 4) and then can quickly produce more\nboards, sometimes 6, 7 and 8 complete boards are found in very few attempts.\nIn some of the experiments the same full board conﬁguration is found more than once\n(marked with an r in the table). Since ﬁnding repeated boards by chance is unlikely this is\nprobbaly due to the recall of previous attempts. To compute the p-values, repeated boards\nwere not counted as valid.\n50Figure 12: Algebraic learning of the 1-blocked 17 ×17 queens problem. Top: Two solutions found by\nalgebraic learning. Starting from the blocked blue queen in each epoch a legal queen (red) is inserted. Inserted\nqueens can be kept or discarded. In black queens placed by the algebra. Bottom: A subset of atoms of M for\nepoch 420. Each atom is represented by the constants that contain it, with white corresponding to the empty\nsquare constant, black to the queen, red both constants, and gray none.\n51Exp Epoch with full board Attempts Full boards p-value\n1 24, 25, 26, 29, 32r, 39, 60, 67r 77 8 (2 repeated) 4 ×10−5\n2 19, 35 51 2 0 .06\n3 39 58 1 0 .37\n4 37, 40, 46, 49, 50r, 54 55 6 (1r) 8 .5 ×10−5\n5 None 70 0 1\n6 9, 22 66 2 0 .1\n7 18, 21, 26, 40, 44, 47r, 48r, 66r 79 8 (3r) 4 .7 ×10−4\n8 34, 41, 52 54 3 0 .01\n9 5, 6, 20, 41, 42r, 71, 74 77 7 (1r) 4 .0 ×10−5\n10 28, 34 37 2 0 .04\nTable 2:10 experiments in algebraic learning for the 1-blocked 17 ×17 queens problem\n526 Discussion\nWe have shown how algebraic representations can be used to learn from data. The algebraic\nlearning technique we propose is a parameter-free method. A small algebraic model grows out\nof operations on the data and it can generalize without overﬁtting. We used four examples to\nillustrate some of its properties. In the following we summarize the results obtained to point\nto open questions.\nWe used the simple case of learning whether an image contains a vertical bar to introduce\nthe reader to Sparse Crossing. In this case the result of learning is a compression of examples\ninto a few atoms that correctly classify the images. We also used this example as a simple case\nin which it is possible to obtain the deﬁnition of a class from the atoms, obtaining explicitly\na deﬁnition of a bar in an image. This goes beyond the classiﬁcation problem and we believe\nthat further development of these techniques may lead to the ability to derive formal concepts\nfrom data. To understand the challenge, we suggest reading Appendix D as it deals with the\ninverse problem of predicting atoms from known formal descriptions.\nThe vertical bar problem was also used to demonstrate explicitly that the number of\npossible solutions of the system with low but non-zero error is astronomically large and that\nthe algebra only needs to ﬁnd one. This helps understanding why ﬁnding a generalizing model\nwith algebraic learning is not as hard as ﬁnding a needle in a haystack. In addition, we show\nthat these solutions can be small.\nAs algebraic learning is only aimed at producing small algebraic models we had to show\nthat the compression of a large number of input constraints (training examples) into small\nrepresentations translates into accuracy. We proved that an algebra picked at random among\nthose that correctly model the training data has a test error that is inversely proportional to\nhow much it compresses training examples into atoms. We have observed this inverse propor-\ntionality in all problems for which many training examples were available so error could be\nmade arbitrarily small. We proved that for a randomly chosen model the dependence has the\nform ϵ= log(3) C/2k. We used the problem of distinguishing images with even or odd number\nof bars to show that algebraic learning using Sparse Crossing also obeys this theoretical result\nwhen error rates are low.\nWe argued that the search for small generalizing models requires avoiding non-free models\nso increasing algebraic freedom should also be pursued when searching for models of small\ncardinal. If algebraic freedom is not sought, the models obtained are far from random and do\nnot generalize.\nWe improved accuracy results using several master atomizations instead of a single one.\nThe way algebraic learning works, the information learned is accumulated in pinning terms\n53and pinning relations and not only in master atoms of a single batch. To better extract the\ninformation contained in the pinning relations we used 10 master atomizations. As diﬀerent\nmaster atomizations express diﬀerent pinning terms, using multiple atomizations stochastically\nsamples the information gathered by pinning relations. We showed that using a majority\nvoting of multiple atomizations gives a much higher accuracy for the problem of identifying\nhand-written digits. Each time the input data is represented in the algebra with a diﬀerent\natomization, additional information is extracted.\nAn alternative but not very diﬀerent method would be to use many algebras in parallel.\nIndeed, one key advantage of algebras is that learning is crystalized into atoms and pinning\nrelations, and these atoms and relations can be shared among algebras, which makes the system\nparallelizable at large scale. Learning can occur in disconnected algebras working in the same\nor diﬀerent problems, in parallel, for as long as necessary before information is shared.\nPerhaps the more conceptually interesting example was how algebraic learning can solve\nthe N-blocked M×M Queen Completion problem [6]. At the technical level, it illustrates how\nalgebraic learning can naturally incorporate any kind of extra relations, in this case teaching\nthe system what the board, the legal moves and the goal of the game are. At the conceptual\nlevel, it stands out as an interesting case of unsupervised learning. In this case, algebraic\nlearning learns the form of the search space within the given constraints and uses it to boost\nthe search for a solution of the puzzle. For large chess boards, we accelerated the learning by\nusing some stochastic input of legal queens. The method to ﬁnd a solution is very diﬀerent to\nothers proposed before. There is no systematic search that guarantees a solution, there is no\nbacktracking and the method is not speciﬁc for the task. Algebraic learning may be useful as\na general purpose technique to explain to a machine which are the rules of a problem (a game)\nso the machine can learn to search for solutions compatible with the rules. These solutions\nare relevant as they correspond with small algebraic representations of the current game, its\ngeneral rules and the previously gathered experience.\nAlgebraic Learning shows instances of human-to-algebra, algebra-to-algebra and algebra-\nto-human communication. Human-to-algebra communication was exempliﬁed in how we taught\nthe game rules of the N Queens Completion problem to an algebra. Algebra-to-algebra com-\nmunication takes place when algebras share pinning terms and relations. A simple example\nof algebra-to-human communication was given for an algebra conceptualizing that positive\nexample images contained a vertical bar.\nOur semilattices produced new atoms in learning, but a more general approach should\nalso modify the constants, which should allow for an improved conversion of compression into\naccuracy. Note that test error is proportional to the number of constants C in the formula that\nrelates error to compression. While Sparse Crossing leaves the constants ﬁxed the proportion-\nality of error with C indicates that further developments should involve learning new constants\n54to reduce not only the number of atoms but also the number of constants they depend upon.\nWe have used semilattices because they have a rich structure [7] despite their simplicity.\nThey are simplest algebras with idempotent operators. Other idempotent algebras such as\nsemilattices extended with unary operators may be relevant to machine learning. Unary oper-\nators can be used to easily extend the techniques in this paper to ﬁnitely-generated[3] inﬁnite\nmodels, which could be used to apply machine learning to more abstract domains.\nWhile algebraic learning is not intended to be a model of a brain, there are some interesting\nparallels. First, atoms relate to constants by an OR operation, that parallels the activation\nof neurons by one (or a small subset) of its inputs. Second, algebraic learning uses a “dual”\nalgebra that handles all the accumulated experience and a master algebra that conforms better\nto current inputs. This resembles the separation into working and long-term memory in the\nbrain. The interactions between both algebras can be thought as feedback connectivity, also\npresent in brains. Even more relevant, this interaction is used to make hypotheses, and a\nsimilar role has been proposed for feedbacks in the brain [8]. Third, identiﬁcation of patterns\nin the cortex is fast compared to the ﬁring speed of neurons [9]. This hints to an important\nrole of wide processing in the brain as that naturally produced by semilattice embeddings.\nPattern identiﬁcation in an algebra occurs with no more processing than a wide representation\nof the sensory information as a subset of learned atoms. Complex problems such as the Queen\nCompletion problem can be solved despite the lack of sequential processing in layers.\nThese parallels make us think of the potential usefulness of extrapolating from Algebraic\nLearning to Artiﬁcial Neural Networks [10] and vice versa. We hope that our ﬁndings regarding\nthe relationship between error and compression rate, as well as the role of balancing algebraic\nfreedom with size minimization may have applications into neural networks, in particular to\nregularization or as an alternative deriving principle for neural processing diﬀerent from error\nminimization. Also, combining or embedding algebras and neural networks might be useful\nto obtain the versatility of neural networks while having the ability of algebras to incorporate\ntop-down information.\nWe have presented an approach to Machine Learning based on Abstract or Universal\nAlgebra [3]. While the relationship of Algebraic Learning with Universal Algebra is more direct,\nthere are other areas in Mathematics with potential connexions. We mention the Minimum\nDescription Length principle [11], Constraint Propagation Theory [12], Compressive Sensing\n[13, 14], Formal Concept Analysis [15] or Ramsey Theory [16] as some candidate areas.\nAcknowledgements\nWe acknowledge funding from Champalimaud Foundation (to G.G.d.P.).\n55Appendix A Notation\nWe use C(S) and A(S) for the subset of constants and atoms respectively of a set S. We also\nuse C(M) and A(M) for the constants or atoms respectively of a model M. The lower and\nupper segments of an element x is deﬁned as L(x) = {y : y ≤x}and U(x) = {y : y >x}. To\ndistinguish the algebra from its graph we use the preﬁx G, so the lower and upper segments\nfor the graph are deﬁned as: GL(x) = {y : (y →x) ∨(y = x)}and GU(x) = {y : x →y}\nassuming always the graphs are transitively closed. Finally, the superscript a is used to denote\nthe intersection with the atoms: GLa(x) = GL(x) ∩A(M) assuming x ∈M. If x belongs to\nM∗the intersection is with the atoms A(M∗). We also use La(x) = L(x) ∩A(M). We also use\nthe intersection with the constants Uc(x) = U(x) ∩C(M).\nThe discriminant dis(a,b) is the set of atoms GLa(a) \\GLa(b). Relation a < bholds if\nand only if dis(a,b) is empty.\nWe say an atom φ is larger than atom η if for any constant c, (η <c) ⇒(φ<c ).\nAppendix B Theorems\nTheorem 1. Let elements a and b satisfy [b] →[a]. Let Φ be the set of atoms involved in the\nfull crossing of a in b, e.g. φ ∈Φ is edged to a or to b but not to both. Assume φ becomes\nφ= ⊙jϕj as a result of the crossing. Then\n∀φ(φ∈Φ)(Tr(φ) = Tr(⊙jϕj)) ⇔ {Tr(b) ⊂Tr(a)}\nProof. Before crossing, φ is a minima and by deﬁnition Tr(φ) ≡GLa([φ]). After the crossing\nthe new minima are ϕj and then Tr(⊙jϕj) ≡∩jGLa([ϕj]). The left side of the equivalence in\nthe theorem then becomes ∀φ(φ∈Φ)(GLa([φ]) = ∩jGLa([ϕj])).\nSince we assumed that φ is not in both a and b, we have either ( φ < a) ∧¬(φ < b) or\n¬(φ<a ) ∧(φ<b ).\nAn atom φ initially in b always preserves its trace due to the corresponding element φ′\nedged only to φ, so Tr(φ) = Tr(φ′). Therefore, ∀φ(φ∈GLa(b))(GLa([φ]) = ∩jGLa([ϕj]))) is\ntrue for any crossing. The case ( φ<a ) ∧¬(φ<b ) remains. We have to show\n∀φ(φ∈GLa(a)) {GLa([φ]) = ∩jGLa([ϕj]))} ⇔ {Tr(b) ⊂Tr(a)}.\nAssume that, before crossing, b was atomized as b = ⊙jεj. For each εj, a new atom ϕj is\ncreated and edges ( ϕj →φ) ∧(ϕj →εj) are appended to the graph. New edges are appended\nto the graph of M∗: ([ φ] →[ϕj])∧([εj] →[ϕj]). Since no other elements are edged to [ ϕj], then\nGLa([ϕj]) = GLa([φ]) ∪GLa([εj]),\n56giving\nTr(⊙jϕj) = ∩jGLa([ϕj])\n= ∩j{GLa([φ]) ∪GLa([εj])}\n= GLa([φ]) ∪{∩jGLa([εj])}\n= GLa([φ]) ∪Tr(b),\nwhich says that when φ is crossed into b the trace of φ gains the set Tr(b). Tr(φ) remains\ninvariant if and only if it contained Tr(b) before crossing. Therefore, if all atoms of a remain\ntrace-invariant, then Tr(b) ⊂Tr(a). Conversely, if Tr(b) ⊂Tr(a), each Tr(φ) for any φ < a\nshould contain the trace Tr(b), and therefore remain trace-invariant.\nTheorem 2. Let term k= ⊙ici with component constants ci. For any atom ξ in M∗ it holds\nthat (ξ <[k]) ⇔¬( φξ < k) if and only if k satisﬁes GLa([k]) = ∩iGLa([ci]). See Section\n3.4 for a deﬁnition of φξ.\nProof. We built atom φξ to satisfy for each constant cof M the relation ¬( φξ <c) ⇔( ξ <[c]).\nFor a term k:\n¬( φξ <k) ⇔∀i¬( φξ <ci) ⇔∀i( ξ <[ci]) ⇔∀i( ξ ∈GLa([ci])) ⇔ξ ∈∩iGL([ci]).\nGLa([k]) = ∩iGLa([ci]) is equivalent to ∀ξ{ξ ∈∩iGLa([ci]) ⇔ξ <[k]}, so it follows that\n{GLa([k]) = ∩iGLa([ci])}⇔∀ ξ{¬( φξ <k) ⇔ξ <[k]},\nwhich completes the proof.\nTheorem 3. Let N and M be two semilattices over the same set C of constants and assume\nM is atomized. If N satisﬁes the pinning relations Rp(M) then for any pair a, b of terms over\nC it holds:\ni) N |= (a<b ) ⇒ M |= (a<b ).\nii) The set Rp(M) captures all negative order relations of M.\niii) For each φ∈M there is at least one atom η∈N such that φ is as large or larger than\nη.\niv) N is as free or freer than M.\nProof. Let u and v be two terms, and assume M satisﬁes ¬(u<v ). There should be an atom\nin the discriminant φ∈disM(u,v) ⊂M and a component constant c∈C of usuch that φ<c\nand (c < u) ∧(v < Tφ) where Tφ is the pinning term of φ (see Section 2.9 for a deﬁnition\n57of Tφ). This is not only true for M it is also true for the term algebra over C and therfore\nfor any model, i.e. it is also satisﬁed by N. In addtion, φ < cimplies ¬(c < Tφ) ∈Rp(M)\nand because we have assumed N satisﬁes Rp(M) then N also models ¬(c < Tφ). Therefore\nN satisﬁes ( c < u) ∧(v < Tφ) ∧¬(c < Tφ) which implies ¬(u < v) and it follows that if\n¬(u<v ) is true for M is also true for N which proves iv and also proves that Rp(M) captures\nall negative relations of M. By negating both sides of this implication we get the equivalent\nN |= (a<b ) ⇒ M |= (a<b ).\nAssume N is atomized. To prove the third claim select any pinning relation of atomφ∈M,\ne.g. ¬(c < Tφ) where c is some constant. We have assumed that N |= ¬(c < Tφ) so there is\nsome atom η ∈disN(c,Tφ) ⊂N, which implies ¬(η < Tφ) and inmediatelly follows Tφ ≤Tη\nand φ is as large or larger than η, i.e. for each constant d such η <dwe have φ<d .\nTheorem 4. Assume ¬p∧R ⇒q, where p and q are two positive order relations, ¬p is a\nnegative order relation, and R is a set of positive and negative order relations. Then R⇒q.\nProof. Without loss of generality we may assume that ¬p∧R∧q has a model M1. The\nhypothesis requires ¬p∧R∧¬q has no model. Either R∧¬q has a model M2, or R alone\nimplies q. Assume M2 exists. We can always atomize both models with two disjoint atom\nsets, one set atomizing M1 and the other M2. Make a new model M3 atomized by the union\nof the atoms in both models and deﬁned by La\nM3 (c) = La\nM1 (c) ∪La\nM2 (c) for each constant c.\nImmediately follows that M3 is a model that satisﬁes R and all the negative relations of M1\nand M2. In fact M3 |= ¬p∧R∧¬q contradicting ¬p∧R ⇒q. Therefore M2 does not exist\nand R⇒q.\nTheorem 5. Let atom φbe redundant in model M if for each constant csuch that φ<c there\nis at least one atom η <cin M such that φis larger than η. An atom can be eliminated without\naltering M if and only if it is redundant.\nProof. Let R+ be the set of all positive relations satisﬁed by the constants and terms of M.\nSince positive relations do not become negative when atoms are eliminated, taking out φ from\nM produces a model N of R+.\nTo prove that a redundant atom can be eliminated let aand bbe a pair of elements (con-\nstants or terms, not atoms) and ¬(a<b ) a negative relation satisﬁed by M and discriminated\nby a redundant atom φ <c≤a where c is some constant. There is an atom η < cin M such\nthat φ is larger than η. Suppose η <b. There is a constant e such that η <e≤b. Because φ\nis larger, φ < e≤b contradicting our assumption that φ ∈disM(a,b). We have proved that\nN |= ¬(η < b) so any negative relation of M is also satisﬁed by N. If N models the same\n58positive and negative relations than M then the subalgebras of M and N spawned by constants\nand terms are isomorphic.\nConversely, assume atom φ can be eliminated without altering M. For each constant c\nsuch φ<c it holds φ∈disM(c<T φ) where Tφ is the pinning term of φ(see Section 2.9 for a\ndeﬁnition of Tφ). If φcan be eliminated there should be some other atom ηc <c discriminating\nc̸<Tφ which implies Tφ ≤Tηc and φis as large or larger than ηc. Since for each constant such\nφ<c there is an ηc ∈M, φ is redundant.\nTheorem 6. Let Mi be a model, d and e elements of Mi such ¬(d < e) and T+(Mi) the set\nof all positive relations between terms formed with the constants of Mi. Let model Mf be the\nresult of enforcing relation d<e using Full or Sparse Crossing.\ni) Mf is strictly less free than Mi, i.e. if Mf |= ¬(a<b ) then Mi |= ¬(a<b ).\nii) Mf |= (a<b ) if and only if T+(Mi) ∪(d<e ) ⇒(a<b ) in case full crossing is used.\nProof. Suppose a<b is true before full crossing. The atoms of aare a subset of the atoms of b\nso any replacement of atoms for others aﬀects bothaand band cannot introduce discriminating\natoms. Hence, all positive relations of Mi are true after crossing and Mf is as free or less free\nthan Mi. In addition, d < eis true after crossing and false before which proves that Mf is\nstrictly less free than Mi and proves claim i.\nSuppose a < bis false before full crossing but it turns true after. Let φ ∈disi(a,b) a\ndiscriminating atom for this relation. If atom φ is no longer discriminat in Mf is because the\natoms at φ's row in the crossing matrix are also edged to b. This can only occur if e < bin\nMi. In addition, all discriminating atoms have been transformed by crossing which means that\nthey were also atoms of d. We have disi(a,b) ⊂La(d) which proves that Mi |= (a⊙d<b ⊙d).\nModel Mi satisﬁes:\n(a⊙d<b ⊙d) ∪(e<b ) ∈T+(Mi).\nTogether with d<e these relations imply:\n(a⊙d<b ⊙d) ∧(e<b ) ∧(d<e ) ⇒(a<b ),\nwhich proves ii.\nTheorem 7. The trace constraints can be enforced using algorithms 1 and 2 if the relation set\nR is consistent.\nProof. By adding a new atom to a constantc∈M and only to this constant it is always possible\nto make Tr(c) = GLa([c]). In the same way, by adding new atoms to the component constants\n59of a term k(one new atom per constant) it is possible to enforce Tr(k) = GLa([k]) unless there\nis an atom ζ ∈M∗ in the lower segment of all the duals of the component constants of k. In\nsuch case a new edge ζ →[k] should be added to the graph of M∗, which we do while enforcing\npositive trance constraints, and obtain Tr(k) = GLa([k]). Therefore, if xis a constant or term\nof M we can make Tr(x) = GLa([x]) by adding atoms to M and edges to M∗.\nWe want to enforce trace constraints for positive relations ( d < e) ∈R+ and negative\nrelations ¬(a < b) ∈R−. By adding new atoms to M and edges to M∗ we can enforce the\ntrace constraints Tr(e) ⊂Tr(d) and Tr(b) ̸⊂Tr(a) if we can enforce the simpler constraints\nGLa([e]) ⊂Tr(d) and Tr(b) ̸⊂GLa([a]).\nAlgorithms 1 and 2 add new atoms to some constants of M and edges to some atoms of\nM∗. These constants and atoms existed before the algorithms are applied. Constants of M and\natoms of M∗ are ﬁnite and adding more than one new atom under a constant of M and only\nunder this constant has no eﬀect in the traces or any other algebraically meaningful property.\nThe same is true for the edges added to initially existing atoms of M∗. At some ﬁnite time it\nis possible to transform the original constraints into the simpler constraints which may or may\nnot happen while running the algorithms but it can always happen, if needed, to enforce the\ntrace constraints.\nEnforcing negative trace constraints is carried out by adding new atoms to M∗. Adding\nnew atoms can violate already holding positive trace constraints and ﬁxing these imply adding\nedges to M∗ that can violate other negative trace constraints and so on. We are about to see\nthat this process ends if it is possible to enforce the dual relations of R in M∗.\nAssume that it is possible to enforce the duals of the relations of R, i.e. to enforce [ e] <[d]\nfor ( d < e) ∈R+ and ¬([b] < [a]) for ¬(a < b) ∈R−. Then we can enforce the positive\nconstraints GLa([e]) ⊂Tr(d) because it is always true GLa([d]) ⊂Tr(d) and, for negative\nconstraints, Tr(b) ̸⊂GLa([a]) follows from GLa([b]) ⊂Tr(b) and ¬([b] <[a]). This proves that\nby adding atoms to M and edges to M∗we can enforce the trace constraints if it is possible to\nenforce the dual relations of R, which we can always do unless R is inconsistent.\n60Appendix C Algorithms\nAlgorithm 1: enforce negative trace constraints\nforeach (a̸<b) ∈R− do\nif Tr(b) ⊂Tr(a) then\ndo\nc= findStronglyDiscriminantConstant (a,b);\nif cj∅then\nchoose h∈C(M∗) so h∈GLc([b])\\GL([a]);\nadd new atom ζ to M∗ and edge ζ →h;\nwhile cj∅;\nadd new atom φ to M and edge φ→c;\nFunction ﬁndStronglyDiscriminantConstant(a, b)\ncalculate the set Ω(a) ≡{[c] : c∈GL(a) ∩C(M)};\ninitialize U ≡Tr(b);\nwhile U ̸= ∅ do\nchoose atom ζ ∈U and remove it from U;\nif Ω(a)\\GU(ζ) not empty then\nchoose [c] ∈Ω(a)\\GU(ζ);\nreturn c;\nreturn ∅;\n61Algorithm 2: enforce positive trace constraints\nforeach (d<e ) ∈R+ do\nwhile Tr(e) ̸⊂Tr(d) do\nchoose an atom ζ ∈Tr(e)\\Tr(d) at random;\ncalculate Γ(ζ,e) ≡{c∈GL(e) ∩C(M) : ζ ̸∈ GL([c])};\nif Γ(ζ,e) = ∅ then\nadd edge ζ →[d];\nelse\nchoose c∈Γ(ζ,e) at random;\nadd new atom φ to M and edge φ→c;\nAlgorithm 3: Sparse Crossing of a into b\ncalculate A≡dis(a,b) ≡GLa(a)\\GL(b);\nforeach φ∈A do\ninitialize sets U ≡∅, B ≡GLa(b) and ∆ ≡A(M∗)\\GL([φ]);\ndo\nchoose an atom ϵ∈B at random;\ncalculate ∆′≡∆ ∩GL([ϵ]);\nif ∆′̸= ∆ or ∆ = ∅then\ncreate new atom ψ and edges ψ→φ and ψ→ϵ;\nreplace ∆ by ∆′;\nadd ϵ to U;\nsubstract ϵ from B;\nwhile ∆ ̸= ∅;\nforeach ϵ∈U do\ncreate new atom ϵ′ and edge ϵ′→ϵ;\ndelete all atoms in U ∪A;\n62Algorithm 4: atom set reduction\ninitialize sets Q≡∅ and Λ ≡C(M);\ndo\nchoose c∈Λ at random and remove it from Λ;\ncalculate Sc ≡Q∩GL(c);\nif Sc = ∅then\ndeﬁne Wc ≡A(M∗);\nelse\ncalculate Wc ≡∩φ∈ScGLa([φ]);\ncalculate Φc ≡{[φ] : φ∈GLa(c)};\nwhile Wc ̸= Tr(c) do\nchoose an atom ξ ∈Wc\\Tr(c) at random;\nchoose an atom φ such that [φ] ∈Φc\\GU(ξ) at random;\nadd φ to set Q;\nreplace Wc with Wc ∩GLa([φ]);\nwhile Λ ̸= ∅;\ndelete all atoms in the set A(M) \\Q;\nAlgorithm 5: atom set reduction for the dual algebra\ninitialize sets Q≡∅ and S ≡R−;\nwhile S ̸= ∅do\nchoose r∈S at random and remove it from S. Let r≡¬(a<b );\nif disM∗ ([b],[a]) ∩Q= ∅ then\nchoose an atom ξ ∈disM∗ ([b],[a]) and add it to Q;\ndelete all atoms in the set A(M∗) \\Q;\nAlgorithm 6: generation of pinning terms and relations\nlet Rp be a new or exisitng set of pinning relations;\nforeach φ∈M do\ncalculate the set H = C(M)\\U(φ);\ncreate the pinning term Tφ = ⊙c∈H c;\nforeach c∈C(M) ∩U(φ) do\nadd r≡¬(c<T φ) to the set Rp;\n63Graphs are assumed to be transitively closed at all times. This requirement, however, can\nbe delayed at some steps to speed up calculations. Always when atoms or edges are added to\nthe graph of M the corresponding duals and reverted edges should also be added to the graph\nof M∗. When an element is deleted its dual should also be deleted from the graph of M∗.\nAppendix D Exact atomizations\nConsider again our toy problem of the vertical lines. We want constant v to satisfy v < Iif\nand only if I is (the term of) an image that has a vertical line. Using subscript i for rows and\nj for columns we can write:\n(v <I) ⇔∨j ∧i (cijb <I ) (A.1)\nwhich simply states that the image should have a black pixelcijb at every row iof some column\nj. The boldface index b stands for the particular value (color back).\nRecapitulating from Section 3.2, an element bhas an atom φif and only if bcontains any\nof the constants that contain φ. We say\n(φ<b ) ⇔∨k(cφk <b), (A.2)\nwhere index k at the disjunction runs along the constants cφk that contain atom φ. Relation\na<b holds if and only if\n(a<b ) ⇔∧φ∈a(φ<b ) ⇔∧φ∈a ∨k (cφk <b), (A.3)\nwhere the conjunction runs along all atoms in a.\nD.1 The map index\nIf we compare the solution of the vertical bar problem and the general form for ( a<b ) we see\nthat they diﬀer only in the order of the connectors. The representation of elements in atomized\nsemilattices corresponds with a ﬁrst-order formula with a conjunction followed by a disjunction\nwhich is known as conjunctive normal form, CNF. We have to swap the connectors ∨and ∧to\nunderstand how the vertical lines look represented in the semilattice. Interchanging connectors\ncan be done by using the distributive law the same way we can interchange the multiplication\nand addition operators of linear algebra:\n⊗j ⊕i cij = ⊕j→i ⊗j cij\nWe introduced the “map index” j →i in Section 3.2 to represent an index that runs\nalong all possible functions from j to i. For ⊕j→i ⊗j cij each summand is characterized by a\n64particular function from j to i. If j takes ”J” possible values and i takes I possible values the\nsummation now has IJ summands each summand a multiplication of J factors. To make more\nexplicit the functional dependence we can write ⊕j→i ⊗j cij(i) to emphasize that the value of\ni on each factor depends upon the factor j through a function i(j) that is diﬀerent for each\nsummand. The handy map index has the following properties:\n⊕i→jk = ⊕i→j⊕i→k (A.4)\n⊕i→(j→k) = ⊕ij→k. (A.5)\nThese properties also apply to both, conjunction and disjunction. Unlike multiplication and\naddition, conjunction and disjunction are both distributive with respect to each other so we\ncan interchange them in any order.\n∧j ∨icij = ∨j→i ∧j cij(i) (A.6)\n∨j ∧icij = ∧j→i ∨j cij(i) (A.7)\nWe can now interchange connectors for the vertical line problem:\n(v <I) ⇔∨j ∧i (cijb <I ) ⇔∧j→i ∨j (ci(j)jb <I ). (A.8)\nFrom the structure of the CNF form we know that the exact embedding into a semilattice of\nthe vertical line problem has IJ atoms of the form:\nφj→i = ∨jci(j)jb, (A.9)\nwhere each atom is characterized by a function i(j). Each atom is in one black pixel per column\nand it is characterized by a particular choice of a row per column.\nIn this case we know the formal solution of the problem in advance and then we can work\nout the form of the exact atomization using the map index. Usually we have examples and a\ngeneral expression is unknown.\nD.2 Calculating atomizations for complex descriptions\nThe map index just introduced is powerful enough to characterize the form of any embedding\nprovided that we have a ﬁrst-order formula with or without quantiﬁers. We are dealing only with\nﬁnite algebras so universal quantiﬁers can be treated as conjunctions and existential quantiﬁers\nas disjunctions. We ﬁrst write the formula as a sequence of conjunctions and disjunctions. This\nis always possible by extending indexes and perhaps adding some trivial clauses that are always\ntrue or always false. For example,\n[∨i ∧j (aij <I )] ∧[∨u(bu <I )] = ∧s ∨r=i ∪u ∧j gsrj, (A.10)\n65with gsrj\ngsrij =\n\n\n\ns= 0, r ∈i aij <I\ns= 0, r ∈u false\ns= 1, r ∈i false\ns= 1, r ∈u br <I\n\n\n\n, (A.11)\nThe trick is simply to extend the scope of the index at the disjunction to r= i ∪u, so it can\ntake all possible values of i and u by adding some trivial clauses equal to false. To extend an\nindex in a conjunction we would add extra true clauses.\nSuppose we want to ﬁnd the exact embedding for a problem with a solution:\n(h<I ) ⇔∨a ∧b ∨c¬∧d ∨egabcde (A.12)\nwhere g is a function that maps a tupla of indexes abcdeto true, false or some clause (ck <I ),\ngabcde = {ck <I, true, false}. (A.13)\nTo transform a chain of connectors to CNF, we ﬁrst get rid of the negations:\n∨a ∧b ∨c ¬∧d ∨egabcde = ∨a ∧b ∨c ∨d ∧e¬gabcde = ∨a ∧b ∨cd ∧e ¯gabcde, (A.14)\nand then move the connectors where we want them by using the map index,\n∨a ∧b ∨cd ∧e ¯gabcde = ∧a→b ∨a ∨cd ∧e ¯gab(a)cde = ∧a→b ∧acd→e ∨acd ¯gab(a)cde(acd). (A.15)\nFrom the index structure of the conjunctions, we know that the exact model contains at most\nBAEACD atoms, each atom of the form\nφa→b,acd→e = ∨acd ¯gab(a)cde(acd), (A.16)\ncontained in at most ACD constants, and characterized for two functions, A : a →b and\nE : acd→e.\nThe inverse problem looks very diﬀerent and it can be much easier or harder to learn,\n(¯h<I ) ⇔¬∨a ∧b ∨c ¬∧d ∨egabcde = ∧a ∨b ∧cd ∨e gabcde = ∧a ∧b→cd ∨begabc(b)d(b)e. (A.17)\nThe exact model for the inverse problem contains at most A(CD)B atoms each atom contained\non at most BE constants, with the form\nψa,b→cd = ∨begabc(b)d(b)e. (A.18)\nWe say ”at most” because the exact models may contain fewer atoms than the expected from\nthe structure of the conjunction indexes. First, notice that disjunctions with trivial trueclause\nare always satisﬁed and never become atoms. Some atoms may be identical to others. Some\nother atoms are contained in a constant and in its inverse constant which become disjunctive\n66clauses that are always satisﬁed so they can be ignored. Other atoms we can discard are the\nones that are redundant as in Theorem 5. Redundant atoms add nothing to the atomization\nthat is not already required by other (smaller) atoms. Smaller atoms are contained in fewer\nconstants than larger atoms. We say an atom is smaller than other if the other is larger as\ndeﬁned in Appendix A.\nFrom the form of the disjunctive clause ψa,b→cd we see that atoms in this model are in at\nmost BE constants. Because gabcde maps to a clauses with a mapping that is not necessarily\ninjective the same constant may appear multiple times in the disjunctive expression of an\natom. Additionally false clauses also result in missing constants so at the end an atom may\nbe included in signiﬁcantly less than BE constants. Because of the diﬀerence in atom sizes it\nis possible for some atoms to be supersets of others.\nConsider that we potentially have a large set of symbols gabcde with as many as ABCDE\nsymbols that correspond with at most the number of constants deﬁned for the problem. We\nshould expect many repetitions in problems with many indexes (many connectors). When\ncalculated using a computer we often ﬁnd for many problems that their prefect models have by\nfar fewer atoms than calculated from the conjunction indexes. In any case, the exact model is\nusually very large.\nInterestingly, the fact that the excat model of a problem is larger than the exact model\nof another problem does not necessarily mean that the ”larger” problem is harder to learn. In\ngeneral the size of the atoms of a model is a much better indicator of problem hardness. The\nsmaller the atoms the easier is to ﬁnd an approximated solution to the problem.\nWe ﬁnish this section with an interesting property. Any atom in a constant xintersects in\nat least one constant any other atom (albeit reverted) of its inverse constant ¬x. For example,\nany two ¯φa→b,acd→e and ψa,b→cd always intersect in the constant inclusion clause,\n¯gab(a)c(b(a))d(b(a))e(ac(b(a))d(b(a))) (A.19)\nor the negation of this clause if we choose to revert ¯ψ instead of φ. To see why this is true, ﬁrst\nnotice that ψa,b→cd sets a value a for index a. Once we have a ﬁxed, we just need to look into\nthe expression of ¯φa→b,acd→e to ﬁnd out that ﬁxing a sets a value b(a) for bwhich in turn, going\nback to ψa,b→cd, ﬁxes c(b(a)) and d(b(a)) that ﬁnally sets the value e(ac(b(a))d(b(a))) using\nagain the expression of ¯φa→b,acd→e. We have been jumping from one atom to the other selecting\nvalues for indexes until we ﬁnd the intersecting clause. This clause corresponds always with a\nconstant inclusion and never with a trivial true or false clause because atoms do not contain\ntrue clauses. An atom may contain false clauses but to intersect in a false clause with the\ninverse of another atom requires a true clause in this one.\n67Appendix E Error is smaller the higher the compression\nE.1 Derivation\nAssume that we sample Qtest questions from a distribution Dtest and that we have a learning\nalgorithm that answers all the questions correctly. Let the failure rate be the probability for\nour algorithm to fail in one test question sampled using distribution Dtest.\nThe probability to have a failure rate greater than ε and still answer the Q questions\ncorrectly is bounded by:\nP(Q tests correct |failure >ε) <(1 −ε)Q. (A.20)\nSuppose that we have a set Ω of possible algorithms (or parameters) and we select one from\nthis set. Assume the selected algorithm correctly responds the Q test questions. We want to\nderive an upper bound for the failure rate ε based on the fact that it responded to all the test\nquestions correctly. We have:\nP(failure >ε |Q tests correct) < P(failure >ε)(1 −ε)Q\nP(Q tests correct) , (A.21)\nwhere P(failure > ε) is the probability to pick an algorithm from Ω that has an error rate\nlarger than ε, and P(Q tests correct) is the probability to pick an algorithm that answers all Q\nquestions correctly. If ε is small we may safely assume that P(failure >ε) ≈1, and write:\nδ≡ (1 −εδ)Q\nP(Q tests correct) , (A.22)\nwhere δ is (an overestimation of) the risk we are willing to accept for the error rate to be larger\nthan εδ. Solving for the error rate:\nεδ = 1 −δ\n1\nQ P(Q)\n1\nQ . (A.23)\nThe smaller the risk the larger is the error rate we have to accept. εδ has been derived from an\nupper bound of P(Q tests correct |failure >ε) so the actual error rate we expect to measure is\nlower than εδ.\nOf course, this calculation is meaningless unless there is a well-deﬁned distribution p(ϵ):\nP(Q) = Σϵp(ϵ)p(Q|ϵ) = Σϵp(ϵ)(1 −ϵ)Q (A.24)\nwhere p(ϵ) is the probability to pick an algorithm that has an error rate equal to ϵ and the\nsummation runs along all possible error rates.\nIf we don’t know P(Q) we cannot derive εδ. It is tempting to use εδ = 1 −δ\n1\nQ and, in fact,\nit may work well to approach the average value of ϵ when Q is not too large. However, when\nQ is large enough this approach dangerously underestimates εδ and cannot be used.\n68It is also tempting to approximate P(Q) ≈0.5Q if we know that the proportion of algo-\nrithms in Ω that are expected to do well in test questions is extremely small compared with\nthe cardinal of Ω. Even when the distribution p(ϵ) is very biased towards randomly responding\nalgorithms for a suﬃciently large value of Qthe distribution P(Q) is always dominated by the\nalgorithms that do well in test questions. If Qis large enough P(Q) becomes much larger than\n0.5Q and the approximation does not work. In general there is no way to derive an error rate\nunless we know P(Q).\nSo, let’s assume that we know P(Q). As the cardinal of Qgrows we get P(Q) <<δ quite\nrapidly for any reasonable δ. If Q is large enough, the term P(Q)\n1\nQ dominates over δ\n1\nQ and\nεδ becomes independent of δ. In general P(Q) dominates unless we demand the risk δ to be\nextremely small, and there is no need for that. It is interesting and unintuitive that we get a\nmeaningful value of εδ even if we let the risk to be as large as δ = 1. When Q is large there is\na limit value:\nε= 1 −P(Q)\n1\nQ . (A.25)\nthat is independent of the risk.\nNow that we know how to calculate an error rate from test example results we are going\nto apply a similar reasoning to training examples.\nAssume we sample diﬀerent training examples from a distribution Dtrain. If multiple learn-\ning batches are used the algorithm may not remember well all the examples seen, particularly\ntraining examples seen in past epochs. Let’s deﬁne R as the number of training examples that\nhave been correctly “retained” by the algorithm. When the error rate is small we expect the\ndiﬀerence between R and the total number of training examples to become small compared to\nR.\nAgain, εis the probability for our algorithm to fail in one test question randomly sampled\nusing distribution Dtest. We are going to assume test and train distributions equal, i.e. Dtest =\nDtrain.\nWe are interested in algebraic learning with semilattices here, so our algorithms in Ω are\nsemilattice models. Imagine we have a random picking mechanism that selects one model\namong all models consistent with R. Assume the chosen model has Z atoms. The probability\nto get a model with an error rate worse than ε is given by:\nP(failure >ε |R correct ∧Z atoms) = (A.26)\n= P(Z atoms) P(failure >ε ∧R correct |Z atoms)\nP(R correct ∧Z atoms) . (A.27)\nThe models we can handle in practice are very small compared with the number of diﬀerent\natoms (2C for C constants) a model could have, so realistic models are not very far (compared\n69to 2C) from the minimal size of Z for which there is some model consistent with R. In this\nrange of Z values we hypothesize\nP(failure >ε ∧R correct |Z atoms) ≤P(failure >ε ∧R correct). (A.28)\nThis inequality occurs when the proportion of small models that perform bad within the set of\nsmall models is not greater than the proportion of small models that perform bad in the set of\nlarge models. We can expect this to be the case based on the fact that there are many more\nlarge models than small models; with more atoms we get more models consistent with R but\nwe also get an even greater number of models inconsistent with R.\nUsing the inequality above it is possible to derive an upper bound for the conditional\nprobability:\nP(failure >ε |R correct ∧Z atoms) < P(failure >ε)(1 −ε)R\nP(R correct|Z atoms) , (A.29)\nwhich is almost the same result we got before for test examples withP(Q tests correct) replaced\nby P(R correct |Z atoms). Again, we can safely use the approachP(failure >ε) ≈1 and replace\nthe conditional probability in the denominator by:\nP(R correct |Z atoms) = |ΩR∧Z|\n|ΩZ| , (A.30)\nwhere ΩZ is the number of models with Z atoms, and Ω R∧Z is the number of models with Z\natoms and consistent with R. There is a bound for Ω Z:\n|ΩZ|<\n(2C\nZ\n)\n, (A.31)\nthat we can use to get an upper bound for the probability:\nP(failure >ε |R correct ∧Z atoms) <\n(2C\nZ\n)\n(1 −ε)R\n|ΩR∧Z| . (A.32)\nThe combinatorial number corresponds with all possible atomizations of size Z. It does not\ncorrespond with the number of possible models of sizeZbecause there are multiple atomizations\nthat produce the same model. This is a consequence of redundant atoms (see theorem 5), but\nit provides an upper bound for Ω Z|.\nIf the risk we are willing to accept to get a bad performing model of size Z is set to σ:\nσ≡\n(2C\nZ\n)\n(1 −εσ)R\nΩR∧Z\n, (A.33)\nwe can derive an upper bound for the error rate εσ.\nThe logarithm of the combinatorial number can be estimated assuming Z <<2C with:\nln\n(2C\nZ\n)\n≈ln(2)ZC + O(max(Z, C)), (A.34)\n70that can be derived from Stirling’s factorial formula. Substituting this estimation in the equa-\ntion above and using ln(1 −ε) ≈−ε+ O(ϵ2)\nln(2)ZC −εσ R−ln(|ΩR∧Z|) = ln(σ). (A.35)\nThe error rate is dominated by P(R correct |Z atoms) and it becomes independent of the risk σ\nfor any reasonable value, just as it happened before with test examples. The quantitylog2|ΩR∧Z|\nmeasures the degeneracy of the solutions. We have 1 ≤|ΩR∧Z|<<|ΩZ|, and even if |ΩR∧Z|is\na very large number, it is going to be very small compared to |ΩZ|. We expect:\nO(ln(|ΩZ|) −ln(|ΩR∧Z|)) ≈O(ln(|ΩZ|)) (A.36)\nso the contribution of ln( |ΩR∧Z|) is small (albeit not necessarily negligible) compared to NC.\nIf we neglect ln(|ΩR∧Z|) the following equation gives us the error rate we expect to get for\na model selected using the random picking algorithm:\nεR= ln 2ZC. (A.37)\nReorganizing the equation and introducing the compression rateκwe get for the random picking\nalgorithm:\nκ≡R\nZ (A.38)\nwe ﬁnally get\nε= ln 2C\nκ (A.39)\nwhich says that error and compression rates are inversely proportional and their product de-\npends only upon the number of constants or degrees of freedom of our data.\nThe random picking algorithm would actually be a valid learning algorithm if we could\nchoose a low Z value at will. We may do better than the random picking algorithm but what\nis actually easy is to do worse! We can do much worse, for example, if we use one of the\nmemorizing algorithms described in Section 3.4.\nExperimental results suggest that the Sparse Crossing algorithm may learn faster than the\nrandom picking when the error is large. However, it seems that when the error rate gets small\nthe Sparse Crossing algorithm asymptotically approaches the exact performance of the random\npicking algorithm. We also have to consider that the approximations made here for the random\npicking algorithm assume a low error rate, so we do not really know the performance of the\nrandom picking algorithm at high error rates.\nWhen the input constants are divided in pairs, so the presence of one constant in the\npair implies the absence of the other (like the white and black pixel constants) the number of\ndiﬀerent atoms is 3\nC\n2 rather than 2 C. Each atom can be either in one of the constants of the\npair or in none of them: in total three states per constant pair. Atoms that have both constants\n71of the same pair in its upper segment do not appear in simple classiﬁcation problems. It is very\neasy to see why. Suppose we are classifying images. If an atom is in both, the white and its\ncorresponding black pixel’s constant, then it is in the lower segment of every term representing\nan image and has no use. In this case the proportionality law reads:\nεκ= ln 3\n2 C. (A.40)\nThis equation and its proportionality constant are in good agreement with experimental results.\nWe compare the theoretical values with experimental results in the next section of the appendix\nand in section Section 3.5. The inverse proportionality between error and compression rates\nis clear.\nFor classiﬁcation problems for which there are symmetries of the input data that do not\nalter the hidden classes we can give a better estimation of the relation between error and\ncompression rates. This is the subject of the next section.\nE.2 The role of symmetries\nIn Appendix D we showed how to derive the atoms of a constant for which we have a formal\ndescription as a ﬁrst order formula. The atoms can be described by combinatorial variations of\nother constants determined by the map index. We departed from a known formal description\nthat uses some explicit indexes that map to constants. When learning from data the formal\ndescription is not known and the indexes are hidden but are still implicit in the atoms learned.\nPairs of atoms of the exact atomization are related by one or multiple swappings of two con-\nstants. Two diﬀerent values of the same index, map to two constants that can be swapped.\nThe structure of the hidden problem gets reﬂected into the symmetries of the atoms.\nIn the same way, if input data has a symmetry, meaning that some constants can be\ninterchanged without aﬀecting the hidden classes, the atoms also display the symmetry. To be\nmore speciﬁc, consider the problem of separating images with an even count of vertical bars\nfrom images with an odd count. We can take an input image and permute the columns and\nalso permute the rows without aﬀecting in which class the image should be classiﬁed. In this\ncase the atoms also manifest the same symmetry, i.e. we can apply the same permutations to\nan atom and obtain another atom of the exact atomization.\nThis is potentially useful in practice. If a problem has a known symmetry new atoms can\nbe derived and added to a model by applying the symmetry to the existing atoms. We get new\natoms “for free” without the need to learn them, i.e. without the need of extensively train for\nall possible values that the symmetry can take. For example, we could use this technique to\nimprove accuracy of translation-invariant pattern recognition with fewer training examples.\nIn section 3.5 we studied the problem of separating even from odd using Sparse Crossing.\n72We showed that the relation between error and compression ﬁts well the theoretical predictions\nfor the random picking at low error rates. The proportionality between error and the inverse of\nthe compression rate is clear. For grids of size 7 ×7 and 10 ×10 the measured proportionality\nconstant and the predicted proportionality constant for the random picking only diﬀer in about\n20% and 35% respectively. Not bad for an adimensional quantity that can take any value.\nWe are going to use our knowledge of the symmetries of the even-versus-odd separation\nproblem to improve our theoretical predictions. The term ln(|ΩR∧Z|) that we considered small\ncompared with ln(|ΩZ|) corresponds with the logarithm of the number of atomizations with Z\natoms that satisfy R. This is a subtracting term that measures degeneracy of the solutions, so\nthe larger it is the more eﬃcient is the transformation of compression into accuracy. For each\natom there are other ( d!)2 atoms in the exact atomization that correspond with a permutation\nof rows and a permutation of columns (where d×d is the dimension of the grid). For the\nnumber of solutions of R with Z atoms we should also expect to have ( d!)2 as a multiplying\nfactor:\n|ΩR∧Z|≈ α(R,Z)(d!)2Z, (A.41)\nwhere α(R,Z) is some quantity larger than 1. If we use this estimation we get:\nln(3)\n2 ZC −εR −2 ln(d!)Z−ln(α(R,Z)) = 0. (A.42)\nand solving for εκ:\nεκ = ln(3)\n2 C−2 ln(d!) −ln(α(R,Z))\nZ . (A.43)\nNeglecting the last term and substituting C = 2d2, we get the new proportionality constant:\nεκ = ln(3)d2 −2 ln(d!). (A.44)\nWith the new estimation the observed discrepancy between measured and experimental val-\nues of this constant drop to about 10% for dimensions 7 ×7 and 5% for dimension 10 ×10.\nConvergence to the theoretical prediction is reached when the error becomes small enough, see\n(Figure 13).\n73Figure 13: Convergence of learning by Sparse Crossing to theoretical predictions in the problem\nof distinguishing whether an image has an even or odd number of vertical bars. Lines indicate\ntheoretical prediction at low error, to which Sparse Crossing approximately converges. blue: 7×7 images. green:\n10 ×10 images.\nReferences\n[1] Nils J. Nilsson. Logic and artiﬁcial intelligence. Artiﬁcial Intelligence, 47(1-3):31–56, jan\n1991.\n[2] Marc Pouly, Jurg Kohlas, and Wiley InterScience (Online service). Generic Inference : a\nUnifying Theory for Automated Reasoning . Wiley, 2011.\n[3] Stanley. Burris and H. P. Sankappanavar. A course in universal algebra . Springer-Verlag,\n1981.\n[4] Katrin Tent and Martin. Ziegler. A course in model theory . Cambridge University Press,\n2012.\n[5] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[6] Ian P Gent, Christopher Jeﬀerson, and Peter Nightingale. Complexity of n-Queens Com-\npletion. Journal of Artiﬁcial Intelligence Research , 59:815–848, 2017.\n[7] Dona Papert. Congruence Relations in Semi-Lattices. Journal of the London Mathematical\nSociety, s1-39(1):723–729, jan 1964.\n[8] Jean Bullier. Integrated model of visual processing. Brain Research Reviews, 36(2-3):96–\n107, oct 2001.\n74[9] Simon Thorpe, Denis Fize, and Catherine Marlot. Speed of processing in the human visual\nsystem. Nature, 381(6582):520–522, jun 1996.\n[10] Christopher M. Bishop. Neural networks for pattern recognition. Clarendon Press, 1995.\n[11] Peter D. Grunwald. The minimum description length principle . MIT Press, 2007.\n[12] Kim; Peter J. Stuckey Marriott. Programming with constraints: An introduction. MIT\nPress.\n[13] D.L. Donoho. Compressed sensing. IEEE Transactions on Information Theory ,\n52(4):1289–1306, apr 2006.\n[14] Emmanuel J. Cand` es, Justin K. Romberg, and Terence Tao. Stable signal recovery from\nincomplete and inaccurate measurements. Communications on Pure and Applied Mathe-\nmatics, 59(8):1207–1223, aug 2006.\n[15] Frano ˇSkopljanac-Maˇ cina and Bruno Blaˇ skovi´ c. Formal Concept Analysis Overview and\nApplications. Procedia Engineering, 69:1258–1267, jan 2014.\n[16] Stevo. Todorcevic. Introduction to Ramsey spaces. Princeton University Press, 2010.\n75"
    }
}