{
    "title": "ReviewArticle",
    "content": {
        "page_content": "ReviewArticle\nDeep Learning for Computer Vision: A Brief Review\nAthanasios Voulodimos ,1,2 Nikolaos Doulamis,2\nAnastasios Doulamis,2 and Eftychios Protopapadakis2\n1DepartmentofInformatics,TechnologicalEducationalInstituteofAthens,12210Athens,Greece\n2National Technical Universityof Athens, 15780 Athens, Greece\nCorrespondenceshouldbeaddressedtoAthanasiosVoulodimos;thanosv@mail.ntua.gr\nReceived 17 June 2017; Accepted 27 November 2017; Published 1 February 2018\nA\ncademicEditor:DiegoAndina\nCopyright © 2018 AthanasiosVoulodimosetal.ThisisanopenaccessarticledistributedundertheCreativeCommonsAttribution\nLicense, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly\ncited.\nOver thelastyears deeplearningmethods have been shown tooutperformprevious state-of-the-artmachine learningtechniques\ninseveralfields,withcomputervisionbeingoneofthemostprominentcases.Thisreviewpaperprovidesabriefoverviewofsome\nof the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep\nBoltzmannMachinesandDeepBeliefNetworks,andStackedDenoisingAutoencoders.Abriefaccountoftheirhistory,structure,\nadvantages,andlimitationsisgiven,followedbyadescriptionoftheirapplicationsinvariouscomputervisiontasks,suchasobject\ndetection,facerecognition,actionandactivityrecognition,andhumanposeestimation.Finally,abriefoverviewisgivenoffuture\ndirectionsindesigningdeeplearningschemesforcomputervisionproblemsandthechallengesinvolvedtherein.\n1. Introduction\nDeep learning allows computational models of multiple\nprocessing layers to learn and represent data with multiple\nlevels of abstraction mimicking how the brain perceives and\nunderstandsmultimodalinformation,thusimplicitlycaptur-\ning intricate structures of large-scale data. Deep learning is\na rich family of methods, encompassing neural networks,\nhierarchical probabilistic models, and a variety of unsuper-\nvised and supervised feature learning algorithms.The recent\nsurge of interest in deep learning methods is due to the fact\nthat they have been shown to outperform previous state-of-\nthe-art techniques in several tasks, as well as the abundance\nof complex data from different sources (e.g., visual, audio,\nmedical,social,andsensor).\nTheambitiontocreateasystemthatsimulatesthehuman\nbrain fueled the initial development of neural networks. In\n1943, McCulloch and Pitts [1] tried to understand how the\nbrain could produce highly complex patterns by using inter-\nconnected basic cells, called neurons. The McCulloch and\nP i t t sm o d e lo fan e u r o n ,c a l l e daM C Pm o d e l ,h a sm a d ea n\nimportantcontributiontothedevelopmentofartificialneural\nnetworks. A series of major contributions in the field is pre-\nsented in Table1, including LeNet [2] and Long Short-Term\nMemory [3], leading up to today’s “era of deep learning.”\nOne of the most substantial breakthroughs in deep learning\ncame in 2006, when Hinton et al. [4] introduced the Deep\nBeliefNetwork,withmultiplelayersofRestrictedBoltzmann\nMachines, greedily training one layer at a time in an unsu-\npervised way. Guiding the training of intermediate levels\nof representation using unsupervised learning, performed\nlocally at each level, was the main principle behind a series\nofdevelopmentsthatbroughtaboutthelastdecade’ssurgein\ndeeparchitecturesanddeeplearningalgorithms.\nAmong the most prominent factors that contributed to\nthe huge boost of deep learning are the appearance of large,\nhigh-quality, publicly available labelled datasets, along with\ntheempowermentofparallelGPUcomputing,whichenabled\nthe transition from CPU-based to GPU-based training thus\nallowingforsignificantaccelerationindeepmodels’training.\nAdditionalfactorsmayhaveplayedalesserroleaswell,such\nasthealleviationofthevanishinggradientproblemowingto\nthedisengagementfromsaturatingactivationfunctions(such\nashyperbolictangentandthelogisticfunction),theproposal\nHindawi\nComputational Intelligence and Neuroscience\nVolume 2018, Article ID 7068349, 13 pages\nhttps://doi.org/10.1155/2018/70683492 ComputationalIntelligenceandNeuroscience\nTable1:Importantmilestonesinthehistoryofneuralnetworksandmachinelearning,leadinguptotheeraofdeeplearning.\nMilestone/contribution Contributor,year\nMCPmodel,regardedastheancestoroftheArtificialNeuralNetwork McCulloch&Pitts,1943\nHebbianlearningrule Hebb, 1949\nFirstperceptron Rosenblatt,1958\nBackpropagation Werbos, 1974\nNeocognitron,regardedastheancestoroftheConvolutionalNeuralNetwork Fukushima,1980\nBoltzmannMachine Ackley,Hinton&Sejnowski,1985\nRestrictedBoltzmannMachine(initiallyknownasHarmonium) Smolensky,1986\nRecurrentNeuralNetwork Jordan, 1986\nAutoencoders Rumelhart,Hinton&Williams,1986\nBallard,1987\nLeNet,startingtheeraofConvolutionalNeuralNetworks LeCun,1990\nLSTM Hochreiter&Schmidhuber,1997\nDeepBeliefNetwork,usheringthe“ageofdeeplearning” Hinton,2006\nDeepBoltzmannMachine Salakhutdinov&Hinton,2009\nAlexNet,startingtheageofCNNusedforImageNetclassification Krizhevsky,Sutskever,&Hinton,2012\nof new regularization techniques (e.g., dropout, batch nor-\nmalization, and data augmentation), and the appearance of\npowerful frameworks like TensorFlow [5], theano [6], and\nmxnet[7],whichallowforfasterprototyping.\nDeep learning has fueled great strides in a variety of\ncomputer vision problems, such as object detection (e.g.,\n[8,9]),motiontracking(e.g.,[10,11]),actionrecognition(e.g.,\n[12,13]),humanposeestimation(e.g.,[14,15]),andsemantic\nsegmentation (e.g., [16, 17]). In this overview, we will con-\ncisely review the main developments in deep learning archi-\ntectures and algorithms for computer vision applications. In\nthis context, we will focus on three of the most important\ntypes of deep learning models with respect to their applica-\nbility in visual understanding, that is, Convolutional Neural\nNetworks (CNNs), the “Boltzmann family” including Deep\nBelief Networks (DBNs) and Deep Boltzmann Machines\n(DBMs) and Stacked (Denoising) Autoencoders. Needless\nto say, the current coverage is by no means exhaustive;\nfor example, Long Short-Term Memory (LSTM), in the\ncategory of Recurrent Neural Networks, although of great\nsignificanceasadeeplearningscheme,isnotpresentedinthis\nreview,sinceitispredominantlyappliedinproblemssuchas\nlanguage modeling, text classification, handwriting recogni-\ntion,machinetranslation,speech/musicrecognition,andless\nso in computer vision problems. The overview is intended\nto be useful to computer vision and multimedia analysis\nresearchers, as well as to general machine learning research-\ners,whoareinterestedinthestateoftheartindeeplearning\nf o rc o m p u t e rv i s i o nt a s k s ,s u c ha so b j e c td e t e c t i o na n d\nrecognition, face recognition, action/activity recognition,\nandhumanposeestimation.\nThe remainder of this paper is organized as follows. In\nSection2, the three aforementioned groups of deep learning\nmodel are reviewed: Convolutional Neural Networks, Deep\nBeliefNetworksandDeepBoltzmannMachines,andStacked\nAutoencoders. The basic architectures, training processes,\nrecent developments, advantages, and limitations of each\ngroup are presented. In Section3, we describe the contribu-\ntionofdeeplearningalgorithmstokeycomputervisiontasks,\nsuch as object detection and recognition, face recognition,\naction/activity recognition, and human pose estimation; we\nalso provide a list of important datasets and resources for\nbenchmarking and validation of deep learning algorithms.\nFinally, Section4 concludes the paper with a summary of\nfindings.\n2. Deep Learning Methods and Developments\n2.1. Convolutional Neural Networks.Convolutional Neural\nNetworks(CNNs)wereinspiredbythevisualsystem’sstruc-\nture, and in particular by the models of it proposed in [18].\nThe first computational models based on these local con-\nnectivities between neurons and on hierarchically organized\ntransformationsoftheimagearefoundinNeocognitron[19],\nwhichdescribesthatwhenneuronswiththesameparameters\nare applied on patches of the previous layer at different\nlocations,aformoftranslationalinvarianceisacquired.Yann\nLeCun and his collaborators later designed Convolutional\nNeuralNetworksemployingtheerrorgradientandattaining\nverygoodresultsinavarietyofpatternrecognitiontasks[20–\n22].\nA CNN comprises three main types of neural layers,\nnamely, (i) convolutional layers, (ii) pooling layers, and (iii)\nfullyconnectedlayers.Eachtypeoflayerplaysadifferentrole.\nFigure1 shows a CNN architecture for an object detection\nin image task. Every layer of a CNN transforms the input\nvolumetoanoutputvolumeofneuronactivation,eventually\nleading to the final fully connected layers, resulting in a\nmapping of the input data to a 1D feature vector. CNNs have\nbeen extremely successful in computer vision applications,\nsuchasfacerecognition,objectdetection,poweringvisionin\nrobotics,andself-drivingcars.\n(i) Convolutional Layers.In the convolutional layers, a CNN\nutilizes various kernels to convolve the whole image asComputationalIntelligenceandNeuroscience 3\n{}\n{}\n{}\nConvolutions\nInput data\nPooling Convs\nLinear\nclassiﬁer\nObject\nCategories/positions\nF4 maps\nC3 feature maps\nS2 feature maps\nC1 feature maps\nat (xi,y i)\nat (xj,y j)\nat (xk,y k)\nFigure1:ExamplearchitectureofaCNNforacomputervisiontask(objectdetection).\nwell as the intermediate feature maps, generating various\nfeature maps. Because of the advantages of the convolution\noperation,several works (e.g., [23, 24]) have proposed it as a\nsubstitute for fully connected layers with a view to attaining\nfasterlearningtimes.\n(ii)PoolingLayers. Poolinglayersareinchargeofreducingthe\nspatial dimensions (width× height) of the input volume for\nthenextconvolutionallayer.Thepoolinglayerdoesnotaffect\nthedepthdimensionofthevolume.Theoperationperformed\nby this layer is also called subsampling or downsampling, as\nthe reduction of size leads to a simultaneous loss of infor-\nmation. However, such a loss is beneficial for the network\nbecausethedecreaseinsizeleadstolesscomputationalover-\nheadfortheupcominglayersofthenetwork,andalsoitworks\nagainst overfitting.Average poolingandmaxpoolingarethe\nmostcommonlyusedstrategies.In[25]adetailedtheoretical\nanalysis of max pooling and average pooling performances\ni sg i v e n ,w h e r e a si n[ 2 6 ]i tw a ss h o w nt h a tm a xp o o l i n gc a n\nlead to faster convergence, select superior invariant features,\nand improve generalization. Also there are a number of\no t h e rv a r i a t i o n so ft h ep o o l i n gl a y e ri nt h el i t e r a t u r e ,e a c h\ninspired by different motivations and serving distinct needs,\nforexample,stochasticpooling[27],spatialpyramidpooling\n[28,29],anddef-pooling[30].\n(iii) Fully Connected Layers.Following several convolutional\nand pooling layers, the high-level reasoning in the neural\nnetwork is performed via fully connected layers. Neurons in\nafullyconnectedlayerhavefullconnectionstoallactivation\nin the previous layer, as their name implies. Their activation\ncanhencebecomputedwithamatrixmultiplicationfollowed\nby a bias offset. Fully connected layers eventually convert\nthe 2D feature maps into a 1D feature vector. The derived\nvector either could be fed forward into a certain number of\ncategories for classification [31] or could be considered as a\nfeaturevectorforfurtherprocessing[32].\nThe architecture of CNNs employs three concrete ideas:\n(a) local receptive fields, (b) tied weights, and (c) spatial\ns u b s a m p l i n g .B a s e do nl o c a lr e c e p t i v efi e l d ,e a c hu n i ti na\nconvolutionallayerreceivesinputsfromasetofneighboring\nunits belonging to the previous layer. This way neurons are\ncapableofextractingelementaryvisualfeaturessuchasedges\nor corners. These features are then combined by the subse-\nquent convolutional layers in order to detect higher order\nfeatures.Furthermore,theideathatelementaryfeaturedetec-\ntors, which are useful on a part of an image, are likely to be\nusefulacrosstheentireimageisimplementedbytheconcept\nof tied weights. The concept of tied weights constraints a set\nof units to have identical weights. Concretely, the units of\na convolutional layer are organized in planes. All units of a\nplane share the same set of weights. Thus, each plane is res-\nponsible for constructing a specific feature. The outputs of\nplanes are called feature maps. Each convolutional layer\nconsists of several planes, so that multiple feature maps can\nbeconstructedateachlocation.\nDuringtheconstructionofafeaturemap,theentireimage\nisscannedbyaunitwhosestatesarestoredatcorresponding\nlocations in the feature map. This construction is equivalent\ntoaconvolutionoperation,followedbyanadditivebiasterm\nandsigmoidfunction:\ny(𝑑)=𝜎( Wy(𝑑−1)+ b), (1)\nwhere𝑑 stands for the depth of the convolutional layer,W is\ntheweightmatrix,and b isthebiasterm.Forfullyconnected\nneural networks, the weight matrix is full, that is, connects\nevery input to every unit with different weights. For CNNs,\ntheweightmatrixW isverysparseduetotheconceptoftied\nweights.Thus, W hastheformof\n[[[[[[\n[\nw 0⋅ ⋅ ⋅0\n0 w ⋅⋅⋅ 0\n...⋅ ⋅ ⋅ d\n...\n0⋅ ⋅ ⋅0 w\n]]]]]]\n]\n, (2)\nwhere w are matrices having the same dimensions with the\nunits’ receptive fields. Employing a sparse weight matrix\nreducesthenumberofnetwork’stunableparametersandthus\nincreases its generalization ability. MultiplyingW with layer\ninputsislikeconvolvingtheinputwith w,whichcanbeseen\nasatrainablefilter.Iftheinputto 𝑑−1 convolutionallayerisof4 ComputationalIntelligenceandNeuroscience\ndimension𝑁×𝑁 andthereceptivefieldofunitsataspecific\nplane of convolutional layer𝑑 is of dimension𝑚×𝑚 ,t h e n\nthe constructed feature map will be a matrix of dimensions\n(𝑁−𝑚+1)×(𝑁−𝑚+1) .Specifically,theelementoffeature\nmapat( 𝑖,𝑗)loca tio nwillbe\ny(𝑑)\n𝑖𝑗 =𝜎 (𝑥(𝑑)\n𝑖𝑗 +𝑏 ) (3)\nwith\n𝑥(𝑑)\n𝑖𝑗 =\n𝑚−1\n∑\n𝛼=0\n𝑚−1\n∑\n𝑏=0\n𝑤𝛼𝑏y(𝑑−1)\n(𝑖+𝛼)(𝑗+𝑏), (4)\nwherethebiasterm 𝑏 isscalar.Using(4)and(3)sequentially\nforall( 𝑖,𝑗)positionsofinput,thefeaturemapforthecorres-\npondingplaneisconstructed.\nOne of the difficulties that may arise with training of\nC N N sh a st od ow i t ht h el a r g en u m b e ro fp a r a m e t e r st h a t\nhave to be learned, which may lead to the problem of\noverfitting.T othisend,techniquessuchasstochasticpooling,\ndropout, and data augmentation have been proposed. Fur-\nthermore,CNNsareoftensubjectedtopretraining,thatis,to\na process thatinitializesthenetworkwithpretrainedparam-\netersinsteadofrandomlysetones.Pretrainingcanaccelerate\nt h el e a r n i n gp r o c e s sa n da l s oe n h a n c et h eg e n e r a l i z a t i o n\ncapabilityofthenetwork.\nOverall, CNNs were shown to significantly outperform\ntraditional machine learning approaches in a wide range of\ncomputervisionandpatternrecognitiontasks[33],examples\nof which will be presented in Section3. Their exceptional\nperformance combined with the relative easiness in training\nare the main reasons that explain the great surge in their\npopularityoverthelastfewyears.\n2.2. Deep Belief Networks and Deep Boltzmann Machines.\nDeep Belief Networks and Deep Boltzmann Machines are\ndeeplearningmodelsthatbelonginthe“Boltzmannfamily,”\nin the sense that they utilize the Restricted Boltzmann\nMachine (RBM) as learning module. The Restricted Boltz-\nmann Machine (RBM) is a generative stochastic neural net-\nwork. DBNs have undirected connections at the top two\nlayers which form an RBM and directed connections to the\nlowerlayers.DBMshaveundirectedconnectionsbetweenall\nlayers of the network. A graphic depiction of DBNs and\nDBMscanbefoundinFigure2.Inthefollowingsubsections,\nwewilldescribethebasiccharacteristicsofDBNsandDBMs,\nafterpresentingtheirbasicbuildingblock,theRBM.\n2.2.1. Restricted Boltzmann Machines.A Restricted Boltz-\nmann Machine ([34, 35]) is an undirected graphical model\nwith stochastic visible variablesk ∈{0,1}𝐷and stochastic\nhidden variablesh ∈{0,1}𝐹, where each visible variable is\nconnectedtoeachhiddenvariable.AnRBMisavariantofthe\nBoltzmannMachine,withtherestrictionthatthevisibleunits\nandhiddenunitsmustformabipartitegraph.Thisrestriction\nallowsformoreefficienttrainingalgorithms,inparticularthe\ngradient-basedcontrastivedivergencealgorithm[36].\nThe model defines the energy function 𝐸: {0,1}𝐷×\n{0,1}𝐹→ R:\n𝐸(k,h;𝜃) =−\n𝐷\n∑\n𝑖=1\n𝐹\n∑\n𝑗=1\n𝑊𝑖𝑗V𝑖ℎ𝑗−\n𝐷\n∑\n𝑖=1\n𝑏𝑖V𝑖−\n𝐹\n∑\n𝑗=1\n𝛼𝑗ℎ𝑗, (5)\nwhere 𝜃={ a,b,W} are the model parameters; that is,𝑊𝑖𝑗\nrepresents the symmetric interaction term between visible\nunit𝑖 andhiddenunit 𝑗,and 𝑏𝑖,𝑎𝑗arebiasterms.\nThejointdistributionoverthevisibleandhiddenunitsis\ngivenby\n𝑃(k,h;𝜃) = 1\nZ(𝜃) exp(−𝐸(k,h;𝜃)),\nZ(𝜃) = ∑\nk\n∑\nh\nexp(−𝐸(k,h;𝜃)),\n(6)\nwhereZ(𝜃) isthenormalizingconstant.Theconditionaldis-\ntributionsoverhidden h and visiblev vectorscanbederived\nby(5)and(6)as\n𝑃(h |k;𝜃) =\n𝐹\n∏\n𝑗=1\n𝑝(ℎ𝑗|k),\n𝑃(k |h;𝜃) =\n𝐷\n∏\n𝑖=1\n𝑝(V𝑖|h).\n(7)\nGiven a set of observations{k𝑛}𝑁\n𝑛=1the derivative of the log-\nlikelihood with respect to the model parameters can be de-\nrivedby(6)as\n1\n𝑁\n𝑁\n∑\n𝑛=1\n𝜕log𝑃(k𝑛;𝜃)\n𝜕𝑊𝑖𝑗\n= E𝑃data\n[V𝑖ℎ𝑗]− E𝑃model\n[V𝑖ℎ𝑗], (8)\nwhere E𝑃data\ndenotes an expectation with respect to the data\ndistribution𝑃data(h,k;𝜃)=𝑃( h | k;𝜃)𝑃data(k),with 𝑃data(k)=\n(1/𝑁)∑𝑛𝛿(k − kn) representing the empirical distribution\nandE𝑃model isanexpectationwithrespecttothedistribution\ndefinedbythemodel,asin(6).\nA detailed explanation along with the description of a\npractical way to train RBMs was given in [37], whereas [38]\ndiscusses the main difficulties of training RBMs and their\nunderlying reasons and proposes a new algorithm with an\nadaptive learning rate and an enhanced gradient, so as to\naddresstheaforementioneddifficulties.\n2.2.2. Deep Belief Networks.Deep Belief Networks (DBNs)\nare probabilistic generative models which provide a joint\nprobabilitydistributionoverobservabledataandlabels.They\nare formed by stacking RBMs and training them in a greedy\nmanner,aswasproposedin[39].ADBNinitiallyemploysan\nefficient layer-by-layer greedy learning strategy to initialize\nthe deep network, and, in the sequel, fine-tunes all weights\njointly with the desired outputs. DBNs are graphical models\nwhich learn to extract a deep hierarchical representation ofComputationalIntelligenceandNeuroscience 5\nDeep Belief Network Deep Boltzmann Machine\nh3\nh2\nh1\nv\nh3\nh2\nh1\nv\nW2\nW1\nW3\nW2\nW1\nW3\nFigure 2: Deep Belief Network (DBN) and Deep Boltzmann Machine (DBM). The top two layers of a DBN form an undirected graph and\ntheremaininglayersformabeliefnetworkwithdirected,top- downconnections.InaDBM,allconnectionsareundirected.\nthe training data. They model the joint distribution between\nobservedvector x andthe 𝑙 hiddenlayers h𝑘asfollows:\n𝑃(x,h1,...,h 𝑙) = (\n𝑙−2\n∏\n𝑘=0\n𝑃(h𝑘| h𝑘+1))𝑃(h𝑙−1,h𝑙), (9)\nwhere x = h0, 𝑃(h𝑘| h𝑘+1) is a conditional distribution for\nthevisibleunitsatlevel 𝑘 conditionedonthehiddenunitsof\nthe RBM at level𝑘+1 ,a n d𝑃(h𝑙−1| h𝑙) is the visible-hidden\njointdistributioninthetop-levelRBM.\nThe principle of greedy layer-wise unsupervised training\ncanbeappliedtoDBNswithRBMsasthebuildingblocksfor\neachlayer[33,39].Abriefdescriptionoftheprocessfollows:\n(1) Train the first layer as an RBM that models the raw\ninputx = h0asitsvisiblelayer.\n(2) Use that first layer to obtain a representation of the\ni n p u tt h a tw i l lb eu s e da sd a t af o rt h es e c o n dl a y e r .\nTwocommonsolutionsexist.Thisrepresentationcan\nbechosenasbeingthemeanactivation𝑃(h1=1|h 0)\norsamples of𝑃(h1| h0).\n(3) Train the second layer as an RBM, taking the trans-\nformeddata(samplesormeanactivation)astraining\nexamples(forthevisiblelayerofthatRBM).\n(4) Iterate steps ((2) and (3)) for the desired number of\nlayers, each time propagating upward either samples\normeanvalues.\n(5) Fine-tunealltheparametersofthisdeeparchitecture\nwith respect to a proxy for the DBN log- likelihood,\nor with respect to a supervised training criterion\n(afteraddingextralearningmachinerytoconvertthe\nlearned representation into supervised predictions,\ne.g.,alinearclassifier).\nTherearetwomainadvantagesintheabove-describedgreedy\nlearningprocessoftheDBNs[40].First,ittacklesthechallenge\nof appropriate selection of parameters, which in some cases\ncanleadtopoorlocaloptima,therebyensuringthatthenet-\nworkisappropriatelyinitialized.Second,thereisnorequire-\nment for labelled data since the process is unsupervised.\nNevertheless, DBNs are also plagued by a number of short-\ncomings, such as the computational cost associated with\ntraining a DBN and the fact that the steps towards further\no p t i m i z a t i o no ft h en e t w o r kb a s e do nm a x i m u ml i k e l i h o o d\ntraining approximation are unclear [41]. Furthermore, a\nsignificantdisadvantageofDBNsisthattheydonotaccount\nfor the two-dimensional structure of an input image, which\nmay significantly affect their performance and applicabil-\nity in computer vision and multimedia analysis problems.\nHowever, a later variation of the DBN, the Convolutional\nDeep Belief Network (CDBN) ([42, 43]), uses the spatial\ninformation of neighboring pixels by introducing convolu-\ntional RBMs, thus producing a translation invariant gener-\native model that successfully scales when it comes to high\ndimensionalimages,asisevidencedin[44].\n2.2.3.DeepBoltzmannMachines. DeepBoltzmannMachines\n(DBMs) [45] are another type of deep model using RBM as\ntheir building block. The difference in architecture of DBNs\nis that, in the latter, the top two layers form an undirected\ngraphical model and the lower layers form a directed gen-\ne r a t i v em o d e l ,w h e r e a si nt h eD B Ma l lt h ec o n n e c t i o n sa r e\nundirected.DBMshavemultiplelayersofhiddenunits,where\nunits in odd-numbered layers are conditionally indepen-\ndent of even-numbered layers, and vice versa. As a result,\ninference in the DBM is generally intractable. Nonetheless,\nan appropriate selection of interactions between visible and\nhiddenunitscanleadtomoretractableversionsofthemodel.\nDuring network training, a DBM jointly trains all layers of\na specific unsupervised model, and instead of maximizing\nthelikelihooddirectly,theDBMusesastochasticmaximum\nlikelihood(SML)[46]basedalgorithmtomaximizethelower6 ComputationalIntelligenceandNeuroscience\nboundonthelikelihood.Suchaprocesswouldseemvulner-\nabletofallinginpoorlocalminima[45],leavingseveralunits\neffectivelydead.Instead,agreedylayer-wisetrainingstrategy\nwas proposed [47], which essentially consists in pretraining\nthelayersoftheDBM,similarlytoDBN,namely,bystacking\nRBMs and training each layer to independently model the\noutput of the previous layer, followed by a final joint fine-\ntuning.\nRegarding the advantages of DBMs, they can capture\nmany layers of complex representations of input data and\nthey are appropriate for unsupervised learning since they\ncan be trained on unlabeled data, but they can also be fine-\nt u n e df o rap a r t i c u l a rt a s ki nas u p e r v i s e df a s h i o n .O n eo f\nt h ea t t r i b u t e st h a ts e t sD B M sa p a r tf r o mo t h e rd e e pm o d e l s\nis that the approximate inference process of DBMs includes,\na p a r tf r o mt h eu s u a lb o t t o m - u pp r o c e s s ,at o p - d o w nf e e d -\nback, thus incorporating uncertainty about inputs in a more\neffective manner. Furthermore, in DBMs, by following the\napproximate gradient of a variational lower bound on the\nlikelihoodobjective,onecanjointlyoptimizetheparameters\nof all layers, which is very beneficial especially in cases of\nlearning models from heterogeneous data originating from\ndifferentmodalities[48].\nA sf a ra st h ed r a w b a c k so fD B M sa r ec o n c e r n e d ,o n eo f\nthe most important ones is, as mentioned above, the high\ncomputational cost of inference, which is almost prohibitive\nwhen it comes to joint optimization in sizeable datasets.\nSeveralmethodshavebeenproposedtoimprovetheeffective-\nness of DBMs. These include accelerating inference by using\nseparatemodelstoinitializethevaluesofthehiddenunitsin\nall layers [47, 49], or other improvements at the pretraining\nstage[50,51]oratthetrainingstage[52,53].\n2.3. Stacked (Denoising) Autoencoders. Stacked Autoen-\ncoders use the autoencoder as their main building block,\nsimilarlytothewaythatDeepBeliefNetworksuseRestricted\nBoltzmannMachinesascomponent.Itisthereforeimportant\ntobrieflypresentthebasicsoftheautoencoderanditsdenois-\ning version, before describing the deep learning architecture\nofStacked(Denoising)Autoencoders.\n2.3.1.Autoencoders. Anautoencoderistrainedtoencodethe\ninputx into a representationr(x) in a way that input can be\nreconstructedfrom r(x)[33].Thetargetoutputoftheautoen-\ncoder is thus the autoencoder input itself. Hence, the output\nvectors have the same dimensionality as the input vector.\nIn the course of this process, the reconstruction error is\nbeing minimized, and the correspondingcode is the learned\nfeature. If there is one linear hidden layer and the mean\nsquarederrorcriterionisusedtotrainthenetwork,thenthe𝑘\nhiddenunitslearntoprojecttheinputinthespanofthefirst\n𝑘principal components of the data [54]. If the hidden layer\nis nonlinear, the autoencoder behaves differently from PCA,\nwith the ability to capture multimodal aspects of the input\ndistribution[55].Theparametersofthemodelareoptimized\nso that the average reconstruction error is minimized. There\nare many alternatives to measure the reconstruction error,\nincludingthetraditionalsquarederror:\nHidden node Reconstruct error\nReconstructionInputCorrupted input\nFigure 3: Denoising autoencoder [56].\n𝐿= ‖x − f (r (x))‖2, (10)\nwherefunction f isthe decoderandf(r(x)) isthereconstruc-\ntionproducedbythemodel.\nIf the input is interpreted as bit vectors or vectors of bit\nprobabilities, then the loss function of the reconstruction\ncouldberepresentedbycross-entropy;thatis,\n𝐿=− ∑\n𝑖\nx𝑖logf𝑖(r (x)) +( 1−x 𝑖)log(1 −f𝑖(r (x))). (11)\nThe goal is for the representation (orcode) r(x) to be a\ndistributed representation that manages to capture the coor-\ndinatesalongthemainvariationsofthedata,similarlytothe\nprinciple of Principal Components Analysis (PCA). Given\nthatr(x) is not lossless, it is impossible for it to constitute a\nsuccessful compression for all inputx. The aforementioned\noptimization process results in low reconstruction error on\ntest examples from the same distribution as the training\nexamplesbutgenerallyhighreconstructionerroronsamples\narbitrarilychosen fromtheinputspace.\n2.3.2. Denoising Autoencoders.The denoising autoencoder\n[56]isastochasticversionoftheautoencoderwheretheinput\nis stochastically corrupted, but the uncorrupted input is still\nused as target for the reconstruction. In simple terms, there\nare two main aspects in the function of a denoising autoen-\ncoder: first it tries to encode the input (namely, preserve the\ninformationabouttheinput),andsecondittriestoundothe\neffect of a corruption process stochastically applied to the\ninput of the autoencoder (see Figure3). The latter can only\nbedonebycapturingthestatisticaldependenciesbetweenthe\ninputs.Itcanbeshownthatthedenoisingautoencodermax-\nimizes a lower bound on the log-likelihood of a generative\nmodel.\nIn[56],thestochasticcorruptionprocessarbitrarilysetsa\nnumberofinputstozero.Thenthedenoisingautoencoderis\ntrying to predict the corrupted values from the uncorrupted\nones, for randomly selected subsets of missing patterns. In\nessence, the ability to predict any subset of variables from\nthe remaining ones is a sufficient condition for completely\ncapturing the joint distribution between a set of variables. It\nshould be mentioned that using autoencoders for denoising\nwasintroducedinearlierworks(e.g.,[57]),butthesubstantial\ncontributionof[56]liesinthedemonstrationofthesuccess-\nfuluseofthemethodforunsupervisedpretrainingofadeep\narchitecture and in linking the denoising autoencoder to a\ngenerativemodel.ComputationalIntelligenceandNeuroscience 7\n2.3.3.Stacked(Denoising)Autoencoders. Itispossibletostack\ndenoising autoencoders in order to form a deep network by\nfeedingthelatentrepresentation(outputcode)ofthedenois-\ning autoencoder of the layer below as input to the current\nlayer.Theunsupervisedpretrainingofsuchanarchitectureis\ndone one layer at a time. Each layer is trained as a denoising\nautoencoder by minimizing the error in reconstructing its\ninput(whichistheoutputcodeofthepreviouslayer).When\nthe first𝑘 layers are trained, we can train the(𝑘 + 1)th layer\nsince it will then be possible compute the latent representa-\ntionfromthelayerunderneath.\nWhen pretraining of all layers is completed, the network\ngoes through a second stage of training called fine-tuning.\nHeresupervisedfine-tuningisconsideredwhenthegoalisto\noptimizepredictionerroronasupervisedtask.Tothisend,a\nlogistic regression layer is added on the output code of the\noutput layer of the network. The derived network is then\ntrained like a multilayer perceptron, considering only the\nencodingpartsofeachautoencoderatthispoint.Thisstageis\nsupervised,sincethetargetclassistakenintoaccountduring\ntraining.\nAs is easily seen, the principle for training stacked auto-\nencoders is the same as the one previously described for\nDeep Belief Networks, but using autoencoders instead of\nR e s t r i c t e dB o l t z m a n nM a c h i n e s .An u m b e ro fc o m p a r a t i v e\nexperimentalstudiesshowthatDeepBeliefNetworkstendto\noutperform stacked autoencoders ([58, 59]), but this is not\nalways the case, especially when DBNs are compared to\nStackedDenoisingAutoencoders[56].\nOne strength of autoencoders as the basic unsupervised\ncomponent of a deep architecture is that, unlike with RBMs,\nthey allow almost any parametrization of the layers, on\ncondition that the training criterion is continuous in the\nparameters. In contrast, one of the shortcomings of SAs is\nthat they do not correspond to a generative model, when\nwithgenerativemodelslikeRBMsandDBNs,samplescanbe\ndrawntochecktheoutputsofthelearningprocess.\n2.4. Discussion.Some of the strengths and limitations of the\npresenteddeeplearningmodelswerealreadydiscussedinthe\nrespectivesubsections.Inanattempttocomparethesemod-\nels (for a summary see Table2), we can say that CNNs have\ngenerally performed better than DBNs in current literature\non benchmark computer vision datasets such as MNIST. In\ncases where the input is nonvisual, DBNs often outperform\nothermodels,butthedifficultyinaccuratelyestimatingjoint\nprobabilities as well as the computational cost in creating a\nDBNconstitutesdrawbacks.AmajorpositiveaspectofCNNs\nis “feature learning,” that is, the bypassing of handcrafted\nfeatures, which are necessary for other types of networks;\nhowever, in CNNs features are automaticallylearned. On the\nother hand, CNNs rely on the availability of ground truth,\nthatis, labelled trainingdata, whereasDBNs/DBMs andSAs\ndonothavethislimitationandcanworkinanunsupervised\nmanner. On a different note, one of the disadvantages of\nautoencoders lies in the fact that they could become ineffec-\ntive if errors are present in the first layers. Such errors may\ncause the network to learn to reconstruct the average of the\ntraining data. Denoising autoencoders [56], however, can\nTable 2: Comparison of CNNs, DBNs/DBMs, and SdAs with\nrespect to a number of properties. + denotes a good performance\nin the property and− denotes bad performance or complete lack\nthereof.\nModelproperties CNNs DBNs/DBMs SdAs\nUnsupervisedlearning −+ +\nTrainingefficiency −− +\nFeaturelearning +− −\nScale/rotation/translationinvariance +− −\nGeneralization ++ +\nretrievethecorrectinputfromacorruptedversion,thuslead-\ni n gt h en e t w o r kt og r a s pt h es t r u c t u r eo ft h ei n p u td i s t r i b u -\ntion.Intermsoftheefficiencyofthetrainingprocess,onlyin\nthe case of SAs is real-time training possible, whereas CNNs\nand DBNs/DBMs training processes are time-consuming.\nFinally,oneofthestrengthsofCNNsisthefactthattheycan\nbeinvarianttotransformationssuchastranslation,scale,and\nrotation. Invariance to translation, rotation, and scale is one\nofthemostimportantassetsofCNNs,especiallyincomputer\nvision problems, such as object detection, because it allows\nabstractinganobject’sidentityorcategoryfromthespecifics\nof the visual input (e.g., relative positions/orientation of the\ncamera and the object), thus enabling the network to effec-\ntivelyrecognizeagivenobjectincaseswheretheactualpixel\nvaluesontheimagecansignificantlydiffer.\n3. Applications in Computer Vision\nIn this section, we survey works that have leveraged deep\nlearning methods to address key tasks in computer vision,\nsuchasobjectdetection,facerecognition,actionandactivity\nrecognition,andhumanposeestimation.\n3.1. Object Detection. Object detection is the process of\ndetecting instances of semantic objects of a certain class\n(such as humans, airplanes, or birds) in digital images and\nvideo (Figure4). A common approach for object detection\nframeworks includes the creation of a large set of candidate\nwindowsthatareinthesequelclassifiedusingCNNfeatures.\nForexample,themethoddescribedin[32]employsselective\nsearch[60]toderiveobjectproposals,extractsCNNfeatures\nfor each proposal, and then feeds the features to an SVM\nclassifier to decide whether the windows include the object\nor not. A large number of works is based on the concept of\nRegions with CNN features proposed in [32]. Approaches\nfollowing the Regions with CNN paradigm usually have\ngood detection accuracies (e.g., [61, 62]); however, there is\nas i g n i fi c a n tn u m b e ro fm e t h o d st r y i n gt of u r t h e ri m p r o v e\nthe performance of Regions with CNN approaches, some of\nwhich succeed in finding approximate object positions but\noften cannot precisely determine the exact position of the\no b j e c t[ 6 3 ] .T ot h i se n d ,s u c hm e t h o d so ft e nf o l l o waj o i n t\nobjectdetection—semanticsegmentationapproach[64–66],\nusuallyattaininggoodresults.\nA vast majority of works on object detection using deep\nlearning apply a variation of CNNs, for example, [8, 67, 68]8 ComputationalIntelligenceandNeuroscience\n(a)\n (b)\n (c)\nFigure 4: Object detection results comparison from [66]. (a) Ground truth; (b) bounding boxes obtained with [32]; (c) bounding boxes\nobtainedwith[66].\n(in which a new def-pooling layer and new learning strategy\nare proposed), [9] (weakly supervised cascaded CNNs), and\n[69] (subcategory-aware CNNs). However, there does exist\na relatively small number of object detection attempts using\nother deep models. For example, [70] proposes a coarse\nobject locating method based on a saliency mechanism in\nconjunction with a DBN for object detection in remote\nsensingimages;[71]presentsanewDBNfor3Dobjectrecog-\nnition, in which the top-level model is a third-order Boltz-\nmann machine, trained using a hybrid algorithm that com-\nbines both generative and discriminative gradients; [72]\nemploys a fused deep learning approach, while [73] explores\nthe representation capabilities of a deep model in a semisu-\npervised paradigm. Finally, [74] leverages stacked autoen-\ncodersformultipleorgandetectioninmedicalimages,while\n[75]exploitssaliency-guidedstackedautoencodersforvideo-\nbasedsalientobjectdetection.\n3.2. Face Recognition.Face recognition is one of the hottest\ncomputer vision applications with great commercial interest\nas well. A variety of face recognition systems based on the\nextraction of handcrafted features have been proposed [76–\n79]; in such cases, a feature extractor extracts features from\nan aligned face to obtain a low-dimensional representation,\nbasedonwhichaclassifiermakespredictions.CNNsbrought\na b o u tac h a n g ei nt h ef a c er e c o g n i t i o nfi e l d ,t h a n k st ot h e i r\nfeature learning and transformation invariance properties.\nThefirstworkemployingCNNsforfacerecognitionwas[80];\ntoday light CNNs [81] and VGG Face Descriptor [82] are\namong the state of the art. In [44] a Convolutional DBN\nachievedagreatperformanceinfaceverification.\nMoreover, Google’s FaceNet [83] and Facebook’s Deep-\nFace [84] are both based on CNNs. DeepFace [84] models\na face in 3D and aligns it to appear as a frontal face. Then,\nthe normalized input is fed to a single convolution-pooling-\nconvolution filter, followed by three locally connected layers\nand two fully connected layers used to make final predic-\ntions. Although DeepFace attains great performance rates,\nits representation is not easy to interpret because the faces\nof the same person are not necessarily clustered during the\ntrainingprocess.Ontheotherhand,FaceNetdefinesatriplet\nlossfunctionontherepresentation,whichmakesthetraining\nprocess learn to cluster the face representation of the same\nperson.Furthermore,CNNsconstitutethecoreofOpenFace\n[85], an open-source face recognition tool, which is of\ncomparable (albeit a little lower) accuracy, is open-source,\nand is suitable for mobile computing, because of its smaller\nsizeandfastexecutiontime.\n3.3. Action and Activity Recognition. H u m a na c t i o na n d\nactivity recognition is a research issue that has received a lot\nofattentionfromresearchers[86,87].Manyworksonhuman\nactivity recognition based on deep learning techniques have\nbeen proposed in the literature in the last few years [88]. In\n[89]deeplearningwasusedforcomplexeventdetectionand\nrecognitioninvideosequences:first,saliencymapswereusedComputationalIntelligenceandNeuroscience 9\nfor detecting and localizing events, and then deep learning\nwas applied to the pretrained features for identifying the\nmost important frames that correspond to the underlying\nevent. In [90] the authors successfully employ a CNN-based\napproach for activity recognition in beach volleyball, sim-\nilarly to the approach of [91] for event classification from\nlarge-scale video datasets; in [92], a CNN model is used for\nactivity recognition based on smartphone sensor data. The\nauthors of [12] incorporate a radius–margin bound as a reg-\nularizationtermintothedeepCNNmodel,whicheffectively\nimproves the generalization performance of the CNN for\nactivityclassification.In[13],theauthorsscrutinizetheappli-\ncability of CNN as joint feature extraction and classification\nmodel for fine-grained activities; they find that due to the\nchallenges of large intraclass variances, small interclass vari-\nances, and limited training samples per activity, an approach\nthatdirectlyusesdeepfeatureslearnedfromImageNet inan\nSVMclassifierispreferable.\nDriven by the adaptability of the models and by the\navailability of a variety of different sensors, an increasingly\npopular strategy for human activity recognition consists in\nfusing multimodal features and/or data. In [93], the authors\nmixedappearanceandmotionfeaturesforrecognizinggroup\nactivities in crowded scenes collected from the web. For the\ncombination of the different modalities, the authors applied\nmultitaskdeeplearning.Theworkof[94]explorescombina-\ntionofheterogeneousfeaturesforcomplexeventrecognition.\nTh ep r o b l e mi sv i e w e da st w od i ff e r e n tt a s k s :fi r s t ,t h em o s t\ninformativefeaturesforrecognizingeventsareestimated,and\nthen the different features are combined using an AND/OR\ngraph structure. There is also a number of works combining\nmorethanonetypeofmodel,apartfromseveraldatamodal-\nities. In [95], the authors propose a multimodal multistream\ndeep learning framework to tackle the egocentric activity\nrecognition problem, using both the video and sensor data\nand employing a dual CNNs and Long Short-Term Memory\narchitecture. Multimodal fusion with a combined CNN and\nLSTMarchitectureisalsoproposedin[96].Finally,[97]uses\nDBNs for activity recognition using input video sequences\nthatalsoincludedepthinformation.\n3.4. Human Pose Estimation.The goal of human pose esti-\nmation is to determine the position of human joints from\nimages, image sequences, depth images, or skeleton data as\nprovided by motion capturing hardware [98]. Human pose\nestimation is a very challenging task owing to the vast range\nofhumansilhouettesandappearances,difficultillumination,\nand cluttered background. Before the era of deep learning,\npose estimation was based on detection of body parts, for\nexample,throughpictorialstructures[99].\nMoving on to deep learning methods in human pose\nestimation, we can group them into holistic and part-based\nmethods, depending on the way the input images are pro-\ncessed. The holistic processing methods tend to accomplish\ntheir task in a global fashion and do not explicitly define a\nmodelforeachindividualpartandtheirspatialrelationships.\nDeepPose[14]isaholisticmodelthatformulatesthehuman\npose estimation method as a joint regression problem and\ndoes not explicitly define the graphical model or part detec-\ntors for the human pose estimation. Nevertheless, holistic-\nbased methods tend to be plagued by inaccuracy in the\nhigh-precision region due to the difficulty in learning direct\nregression of complex pose vectors fromimages.\nOn the other hand, the part-based processing methods\nf o c u so nd e t e c t i n gt h eh u m a nb o d yp a r t si n d i v i d u a l l y ,f o l -\nlowedbyagraphicmodeltoincorporatethespatialinforma-\ntion.In[15],theauthors,insteadoftrainingthenetworkusing\nthe whole image, use the local part patches and background\npatches to train a CNN, in order to learn conditional prob-\nabilities of the part presence and spatial relationships. In\n[100]theapproachtrainsmultiplesmallerCNNstoperform\nindependent binary body-part classification, followed with a\nhigher-levelweakspatialmodeltoremovestrongoutliersand\nto enforce global pose consistency. Finally, in [101], a multi-\nresolution CNN is designed to perform heat-map likelihood\nregression for each body part, followed with an implicit\ngraphicmodeltofurtherpromotejointconsistency.\n3.5. Datasets. The applicability of deep learning approaches\nhas been evaluated on numerous datasets, whose content\nvariedgreatly,accordingtheapplicationscenario.Regardless\no ft h ei n v e s t i g a t e dc a s e ,t h em a i na p p l i c a t i o nd o m a i ni s\n(natural) images. A brief description of utilized datasets\n(traditional and new ones) for benchmarking purposes is\nprovided below.\n(1)GrayscaleImages. Themostusedgrayscaleimagesdataset\nisMNIST[20]anditsvariations,thatis,NISTandperturbed\nNIST. The application scenario is the recognition of hand-\nwrittendigits.\n( 2 )R G BN a t u r a lI m a g e s .Caltech RGB image datasets [102],\nfor example, Caltech 101/Caltech 256 and the Caltech Sil-\nhouettes, contain pictures of objects belonging to 101/256\ncategories.CIFARdatasets[103]consistofthousandsof32 ×\n32colorimagesinvariousclasses.COILdatasets[104]consist\nofdifferentobjectsimagedateveryangleina360rotation.\n(3) Hyperspectral Images.SCIEN hyperspectral image data\n[105] and AVIRIS sensor based datasets [106], for example,\ncontainhyperspectralimages.\n(4)FacialCharacteristicsImages. Adiencebenchmarkdataset\n[107] can be used for facial attributes identification, that\nis, age and gender, from images of faces. Face recognition\nin unconstrained environments [108] is another commonly\nuseddataset.\n(5) Medical Images. Chest X-ray dataset [109] comprises\n112120 frontal-view X-ray images of 30805 unique patients\nwith the text-mined fourteen disease image labels (where\neachimagecanhavemultilabels).LymphNodeDetectionand\nSegmentation datasets [110] consist of Computed Tomogra-\nphyimagesofthemediastinumandabdomen.\n(6) Video Streams.The WR datasets [111, 112] can be used\nfor video-based activity recognition in assembly lines [113],\nc o n t a i n i n gs e q u e n c e so f7c a t e g o r i e so fi n d u s t r i a lt a s k s .\nYouTube-8M [114] is a dataset of 8 million YouTube video\nURLs,alongwithvideo-levellabelsfromadiversesetof4800\nKnowledgeGraphentities.10 ComputationalIntelligenceandNeuroscience\n4. Conclusions\nThesurgeofdeeplearningoverthelastyearsistoagreatex-\ntent due to the strides it has enabled in the field of computer\nvision.Thethreekeycategoriesofdeeplearningforcomputer\nvision that have been reviewed in this paper, namely, CNNs,\nthe “Boltzmann family” including DBNs and DBMs, and\nSdAs,havebeenemployedtoachievesignificantperformance\nratesinavarietyofvisualunderstandingtasks,suchasobject\ndetection, face recognition, action and activity recognition,\nhuman pose estimation, image retrieval, and semantic seg-\nmentation. However, each category has distinct advantages\nand disadvantages. CNNs have the unique capability of\nfeature learning, that is, of automatically learning features\nbasedonthegivenda taset.CNN sarealsoin varian ttotrans-\nformations,whichisagreatassetforcertaincomputervision\napplications. On the other hand, they heavily rely on the\nexistence of labelled data, in contrast to DBNs/DBMs and\nS d A s ,w h i c hc a nw o r ki na nu n s u p e r v i s e df a s h i o n .O ft h e\nmodels investigated, both CNNs and DBNs/DBMs are com-\nputationally demanding when it comes to training, whereas\nSdAscanbetrainedinrealtimeundercertaincircumstances.\nAsaclosingnote,inspiteofthepromising—insomecases\nimpressive—resultsthathavebeendocumentedinthelitera-\nture,significantchallengesdoremain,especiallyasfarasthe\ntheoretical groundwork that would clearly explain the ways\nt od e fi n et h eo p t i m a ls e l e c t i o no fm o d e lt y p ea n ds t r u c t u r e\nf o rag i v e nt a s ko rt op r o f o u n d l yc o m p r e h e n dt h er e a s o n s\nfor which a specific architecture or algorithm is effective\nin a given task or not. These are among the most impor-\ntant issues that will continue to attract the interest of the\nmachinelearningresearchcommunityintheyearstocome.\nConflicts of Interest\nThe authors declare that there are no conflicts of interest\nregardingthepublicationofthispaper.\nAcknowledgments\nThisr esear chisim plemen tedthr o ughIKYscholarshi psp r o-\ngramme and cofinanced by the European Union (European\nSocial Fund—ESF) and Greek national funds through the\naction titled “Reinforcement of Postdoctoral Researchers,”\nin the framework of the Operational Programme “Human\nResources Development Program, Education and Lifelong\nLearning” of the National Strategic Reference Framework\n(NSRF)2014–2020.\nReferences\n[1] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas\nimmanentinnervousactivity,” BulletinofMathematicalBiology ,\nvol.5,no .4,p p .115–13 3,1943.\n[2] Y. LeCun, B. Boser, J. Denker et al., “Handwritten digit recog-\nnitionwithaback-propagationnetwork,”in AdvancesinNeural\nInformation Processing Systems 2 (NIPS*89),D .T o u r e t z k y ,E d . ,\nDenver,CO,USA,1990.\n[3] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural Computation,vol.9 ,no .8,p p .17 35–17 80,1997 .\n[4] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning\nalgorithmfordeepbeliefnets,” Neural Computation,vol.18,no.\n7, pp. 1527–1554, 2006.\n[5] TensorFlow,Availableonline:https://www.tensorflow.org.\n[6] B.Frederic,P.Lamblin,R.Pascanuetal.,“Theano:newfeatures\nand speed improvements,” in Deep Learning and Unsuper-\nvised Feature Learning NIPS 2012 Workshop, 2012, http://deep-\nlearning.net/software/theano/.\n[7] Mxnet,Availableonline:http://mxnet.io.\n[ 8 ]W .O u y a n g ,X .Z e n g ,X .W a n ge ta l . ,“ D e e p I D - N e t :O b j e c t\nDetection with Deformable Part Based Convolutional Neural\nNetworks,”IEEETransactionsonPatternAnalysisandMachine\nIntelligence,vol.39 ,no .7 ,p p .13 20–13 34,2017 .\n[9]A.Diba,V .Sharma,A.Pazandeh,H.Pirsiavash,andL.V .Gool,\n“Weakly Supervised Cascaded Convolutional Networks,” in\nProceedingsofthe2017IEEEConferenceonComputerVisionand\nPattern Recognition (CVPR), pp. 5131–5139, Honolulu, HI, July\n2017.\n[10] N. Doulamis and A. Voulodimos, “FAST-MDL: Fast Adaptive\nSupervised Training of multi-layered deep learning models for\nconsistent object tracking and classification,” inProceedings of\nthe2016IEEEInternationalConferenceonImaging Systemsand\nTechniques, IST 2016,pp.318–323,October2016.\n[11] N. Doulamis, “Adaptable deep learning structures for object\nlabeling/tracking under dynamic visual environments,”Multi-\nmed iaT oo lsa ndA p p l ica t io ns,p p .1 –39 ,2017 .\n[12] L.Lin,K.W ang,W .Zuo,M.W ang,J.Luo,andL.Zhang,“ Adeep\nstructured model with radius-margin bound for 3D human\nactivity recognition,”International Journal of Computer Vision,\nvol.118,no.2,pp.256–273,2016.\n[13] S.CaoandR.Nevatia,“Exploringdeeplearningbasedsolutions\nin fine grained activity recognition in the wild,” inProceedings\nofthe201623rdInternationalConferenceonPatternRecognition\n(ICPR),pp.384–389,Cancun,December2016.\n[14] A.ToshevandC.Szegedy,“DeepPose:Humanposeestimation\nvia deep neural networks,” inProceedings of the 27th IEEE\nConferenceonComputerVisionandPatternRecognition,CVPR\n2014,p p .165 3–1660,USA,J une201 4.\n[15] X. Chen and A. L. Yuille, “Articulated pose estimation by a\ngraphical model with image dependent pairwise relations,” in\nProceedingsoftheNIPS ,201 4.\n[16] H.Noh,S.Hong,andB.Han,“Learningdeconvolutionnetwork\nfor semantic segmentation,” inProceedings of the 15th IEEE\nInternational Conference on Computer Vision, ICCV 2015,p p .\n1520–1528,Santiago,Chile,December2015.\n[17] J.Long,E.Shelhamer,andT.Darrell,“Fullyconvolutionalnet-\nworks for semantic segmentation,” inProceedings of the IEEE\nConferenceonComputerVisionandPatternRecognition(CVPR\n’15),pp.3431–3440,IEEE,Boston,Mass,USA,June2015.\n[18] D. H. Hubel and T. N. Wiesel, “Receptive fields, binocular\ninteraction, and functional architecture in the cat’s visual\ncortex,”TheJournalofPhysiology ,vol.160,p p .106–154,1 96 2.\n[19] K. Fukushima, “Neocognitron: a self-organizing neural net-\nworkmodelforamechanismofpatternrecognitionunaffected\nby shift in position,”Biological Cybernetics,v o l .3 6 ,n o .4 ,p p .\n193–202,1980.\n[20] Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner,“Gradient-based\nlearning applied to document recognition,”Proceedings of the\nIEEE,vol.86,no.11,pp.2278–2323,1998.ComputationalIntelligenceandNeuroscience 11\n[21] Y.LeCun,B.Boser,J.S.Denkeretal.,“Backpropagationapplied\ntohandwrittenzipcoderecognition,” Neural Computation,vol.\n1,no.4,pp.541–551,1989.\n[ 2 2 ]M .T y g e r t ,J .B r u n a ,S .C h i n t a l a ,Y .L e C u n ,S .P i a n t i n o ,a n dA .\nSzlam,“Amathematicalmotivationforcomplex-valuedconvo-\nlutional networks,”Neural Computation,v o l .28,n o .5,p p .8 15–\n825,2016.\n[ 2 3 ]M .O q u a b ,L .Bo t t o u ,I .L a p t e v ,a n dJ .S i v i c ,“ I so b j e c tl o c a l i z a -\ntion for free? - Weakly-supervised learning with convolutional\nneural networks,” inProceedings of the IEEE Conference on\nComputerVisionandPatternRecognition,CVPR2015,pp .685–\n694,June2015.\n[24] C. Szegedy, W. Liu, Y. Jia et al., “Going deeper with convolu-\ntions,”inProceedingsoftheIEEEConferenceonComputerVision\nandPatternRecognition(CVPR’15) ,pp.1–9,Boston,Mass,USA,\nJune 2015.\n[25] Y. L. Boureau, J. Ponce, and Y. LeCun, “A theoretical analysis\nof feature pooling in visual recognition,” inProceedings of the\nICML,2010.\n[26] D. Scherer, A. M¨uller, and S. Behnke, “Evaluation of pooling\noperations in convolutional architectures for object recogni-\ntion,”Lecture Notes in Computer Science (including subseries\nLecture Notes in Artificial Intelligence and Lecture Notes in\nBioinformatics): Preface,vol.6354,no .3,p p .9 2–101,2010.\n[27] H.WuandX.Gu,“Max-PoolingDropoutforRegularizationof\nConvolutional Neural Networks,” inNeural Information Pro-\ncessing,v o l .9 48 9o fLecture Notes in Computer Science,p p .46 –\n54,SpringerInternationalPublishing,Cham,2015.\n[28] K.He,X.Zhang,S.Ren,andJ.Sun,“SpatialPyramidPoolingin\nDeepConvolutionalNetworksforVisualRecognition,”in Com-\nputerVision–ECCV2014 ,vol.8691ofLectureNotesinComputer\nScience,pp.346–361,SpringerInternationalPublishing,Cham,\n2014.\n[29] K.He,X.Zhang,S.Ren,andJ.Sun,“Spatialpyramidpoolingin\nconvolutional networks for visual recognition,”IEEE Transac-\ntionsonPatternAnalysisandMachineIntelligence ,vol.37 ,no .9 ,\npp.1904–1916,2015.\n[30] W.Ouyang,X.Wang,X.Zengetal.,“DeepID-Net:Deformable\ndeep convolutional neural networks for object detection,” in\nProceedings of the IEEE Conference on Computer Vision and\nPatternRecognition,CVPR2015 ,pp.2403–2412,USA,June2015.\n[31] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassifi-\ncationwithdeepconvolutionalneuralnetworks,”in Proceedings\nofthe26thAnnualConferenceonNeuralInformationProcessing\nSystems (NIPS ’12), pp. 1097–1105, Lake Tahoe, Nev, USA,\nDecember2012.\n[32] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation,” inProceedings of the 27th IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR ’14),p p .5 8 0 –\n587,Columbus,Ohio,USA,June2014.\n[33] Y. Bengio, “Learning deep architectures for AI,”Foundations\nand TrendsinMachineLearning,vol.2,no .1,p p .1 –27 ,2009 .\n[34] P. Smolensky, “Information processing in dynamical systems:\nFoundations of harmony theory,” inIn Parallel Distributed\nProcessing: Explorations in the Microstructure of Cognition,v o l .\n1,pp.194–281,MITPress,Cambridge,MA,USA,1986.\n[35] G. E. Hinton and T. J. Sejnowski, “Learning and Relearning in\nBoltzmannMachines,”vol.1,p.4.2,MITPress,Cambridge,MA,\n1986.\n[36] M. A. Carreira-Perpinan and G. E. Hinton, “On contrastive\ndivergence learning,” inProceedings of the tenth international\nworkshop on artificial intelligence and statistics., NP: Society for\nArtificialIntelligenceandStatistics,pp.33–40,2005.\n[37] G. Hinton, “A practical guide to training restricted Boltzmann\nmachines,”Momentum,vol.9 ,p .9 26,2010.\n[38] K. Cho, T. Raiko, and A. Ilin, “Enhanced gradient for training\nrestricted Boltzmann machines,”Neural Computation,v o l .2 5 ,\nn\no .3,p p .805–8 3 1,2013.\n[39] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimen-\nsionality of data with neural networks,”American Association\nfor the Advancement of Science: Science,v o l .3 1 3 ,n o .5 7 8 6 ,p p .\n504–507,2006.\n[ 4 0 ]I .A r e l ,D .C .R o s e ,a n dT .P .K a r n o w s k i ,“ D e e pm a c h i n el e a r n -\ning—a new frontier in artificial intelligence research,”IEEE\nComputational Intelligence Magazine,v o l .5 ,n o .4 ,p p .1 3 – 1 8 ,\n2010.\n[41] Y. Bengio, A. Courville, and P. Vincent, “Representation learn-\ning: a review and new perspectives,” IEEE Transactions on\nPattern Analysis and Machine Intelligence,v o l .3 5 ,n o .8 ,p p .\n1798–1828,2013.\n[42] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng, “Convolutional\ndeep belief networks for scalable unsupervised learning of\nhierarchical representations,”inProceedings of the 26th Annual\nInternationalConference(ICML’09) ,pp.609–616,ACM,Mon-\ntreal,Canada,June2009.\n[ 4 3 ]H .L e e ,R .G r o s s e ,R .R a n g a n a t h ,a n dA .Y .N g,“ U n s u p e rv i s e d\nlearning of hierarchical representations with convolutional\ndeepbeliefnetworks,” CommunicationsoftheACM ,vol.54,no .\n10,p p .95–103,2011.\n[44] G. B. Huang, H. Lee, and E. Learned-Miller, “Learning hierar-\nchical representations for face verification with convolutional\ndeepbeliefnetworks,”inProceedingsoftheIEEEConferenceon\nComputerVisionandPatternRecognition(CVPR’12) ,pp .2518–\n2525,June2012.\n[45] R. Salakhutdinov and G. Hinton, “Deep boltzmann machines,”\nin Proceedings of the International Conference on Artificial\nIntelligenceand Statistics,vol.2 4,p p .448–455,2009 .\n[46] L. Younes, “On the convergence of Markovian stochastic algo-\nrithmswithrapidlydecreasingergodicityrates,” Stochasticsand\nStochastics Reports,vol.65,no .3-4,p p .177 –228,1999 .\n[47] R.SalakhutdinovandH.Larochelle,“Efficientlearningofdeep\nBoltzmannmachines,”in ProceedingsoftheAISTATS ,2010.\n[48] N. Srivastava and R. Salakhutdinov, “Multimodal learning\nwith deep Boltzmann machines,”Journal of Machine Learning\nResearch,vol.15,p p .2949–2980,201 4.\n[49] R. Salakhutdinov and G. Hinton, “An efficient learning proce-\ndure for deep Boltzmann machines,”Neural Computation,v o l .\n2 4,no .8,p p .1 967 –2006,2012.\n[50] R.SalakhutdinovandG.Hinton,“AbetterwaytopretrainDeep\nBoltzmannMachines,”in Proceedingsofthe26thAnnualConfer-\nence on Neural Information Processing Systems 2012, NIPS 2012,\npp.2447–2455,usa,December2012.\n[51] K.Cho,T.Raiko,A.Ilin,andJ.Karhunen,“Atwo-stagepretrain-\ning algorithm for deep boltzmann machines,”Lecture Notes in\nComputerScience(includingsubseriesLectureNotesinArtificial\nIntelligence and Lecture Notes in Bioinformatics): Preface,v o l .\n813 1,p p .106–113,2013.\n[52] G. Montavon and K. M¨uller, “Deep Boltzmann Machines and\ntheCenteringTrick,”in NeuralNetworks:TricksoftheTrade ,vol.\n7700of LectureNotesinComputerScience ,pp.621–637 ,Springer\nBerlinHeidelberg,Berlin,Heidelberg,2012.12 ComputationalIntelligenceandNeuroscience\n[53] I. Goodfellow, M. Mirza, A. Courville et al., “Multi-prediction\ndeepBoltzmannmachines,”in ProceedingsoftheNIPS ,2013.\n[54] H.BourlardandY.Kamp,“Auto-associationbymultilayerper-\nceptrons and singular value decomposition,”Biological Cyber-\nnetics,vol.59,no.4-5,pp.291–294,1988.\n[55] N. Japkowicz, S. J. Hanson, and M. A. Gluck, “Nonlinear auto-\nassociationisnotequivalenttoPCA,” Neural Computation,vol.\n12,no .3,p p .5 3 1 –545,2000.\n[56] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Ex-\ntractingandcomposingrobustfeatureswithdenoisingautoen-\ncoders,” in in Proceedings of the Twenty-fifth International\nConference on Machine Learning (ICML’08),W .W .C o h e n ,A .\nMcCallum,andS.T.Roweis,Eds.,pp.1096–1103,ACM,2008.\n[57] P.Gallinari,Y.LeCun,S.Thiria,andF.Fogelman-Soulie,“Mem-\no i r e sa s s o c i a t i v e sd i s t r i b u e e s , ”i nProceedings of the in Proceed-\ningsofCOGNITIVA87 ,P aris,LaV illette,1987 .\n[58] H.Larochelle,D.Erhan,A.Courville,J.Bergstra,andY .Bengio,\n“An empirical evaluation of deep architectures on problems\nwith many factors of variation,” inProceedings of the 24th\nInternational Conference on Machine Learning (ICML ’07),p p .\n473–480,Corvallis,Ore,UA,June2007.\n[59] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy\nlayer-wise training of deep networks,” inAdvances in Neural\nInformation Processing Systems (NIPS06),B .S c h ,J .P l a t t ,a n d . ,\nT. Hoffman, and B. Sch¨olkopf, Eds., vol. 19, pp. 153–160, MIT\nPress, 2007.\n[60]J .R.R.U ijlings,K.E.A.V anDeSande,T .Gevers,andA.W .M.\nSmeulders, “Selective search for object recognition,”Interna-\ntional Journal of Computer Vision,v o l .1 0 4 ,n o .2 ,p p .1 5 4 – 1 7 1 ,\n2013.\n[61] R. Girshick, “Fast R-CNN,” inProceedings of the 15th IEEE\nInternational Conference on Computer Vision (ICCV ’15),p p .\n1440–1448,December2015.\n[62] S.Ren,K.He,R.Girshick,andJ.Sun,“FasterR-CNN:Towards\nReal-Time Object Detection with Region Proposal Networks,”\nIEEETransactionsonPatternAnalysisandMachineIntelligence,\nvol.39,no.6,pp.1137–1149,2017.\n[63] J. Hosang, R. Benenson, and B. Schiele, “How good are detec-\ntion proposals, really?” in Proceedings of the 25th British\nMachine Vision Conference, BMVC 2014,gb r ,Sep tem ber201 4.\n[64] B.Hariharan,P.Arbel ´aez,R.Girshick,andJ.Malik,“Simultane-\nous detection and segmentation,” inComputer Vision—ECCV\n2014,v o l .8 6 9 5o fLecture Notes in Computer Science,p p .2 9 7 –\n312, Springer, 2014.\n[65] J. Dong, Q. Chen, S. Yan, and A. Yuille, “Towards unified\nobject detection and semantic segmentation,”Lecture Notes in\nComputerScience(includingsubseriesLectureNotesinArtificial\nIntelligence and Lecture Notes in Bioinformatics): Preface,v o l .\n869 3,no .5,p p .299–3 1 4,201 4.\n[66] Y.Zhu,R.Urtasun,R.Salakhutdinov,andS.Fidler,“SegDeepM:\nExploiting segmentation and context in deep neural networks\nfor object detection,” inProceedings of the IEEE Conference on\nComputerVisionandPatternRecognition,CVPR2015,pp.4703–\n4711,USA,June2015.\n[67] J.Liu,N.Lay,Z.Weietal.,“ColitisdetectiononabdominalCT\nscans by rich feature hierarchies,” inProceedings of the Medical\nImaging 2016: Computer-Aided Diagnosis,v o l .9 7 8 5o fProceed-\nings of SPIE,SanDiego,Calif,USA,February2016.\n[68] G.Luo,R.An,K.W ang,S.Dong,andH.Zhang,“ ADeepLearn-\ning Network for Right Ventricle Segmentation in Short:Axis\nMRI,” inProceedings of the 2016 Computing in Cardiology\nConference.\n[69] T. Chen, S. Lu, and J. Fan, “S-CNN: Subcategory-aware convo-\nlutional networks for object detection,”IEEE Transactions on\nP\nattern Analysis and Machine Intelligence,2017 .\n[70] W. Diao, X. Sun, X. Zheng, F. Dou, H. Wang, and K. Fu,\n“Efficient Saliency-Based Object Detection in Remote Sensing\nImages Using Deep Belief Networks,”IEEE Geoscience and\nRemote Sensing Letters,vol.13,no .2,p p .137 –1 41,2016.\n[71] V. Nair and G. E. Hinton, “3D object recognition with deep\nbeliefnets,”in ProceedingsoftheNIPS ,2009 .\n[72] N. Doulamis and A. Doulamis, “Fast and adaptive deep fusion\nlearningfordetectingvisualobjects,” LectureNotesinComputer\nScience(includingsubseriesLectureNotesinArtificialIntelligence\nandLectureNotesinBioinformatics):Preface,vol.7585,no.3,pp.\n345–354,2012.\n[73] N.DoulamisandA.Doulamis,“Semi-superviseddeeplearning\nforobjecttrackingandclassification,”pp.848–852.\n[74] H.-C. Shin, M. R. Orton, D. J. Collins, S. J. Doran, and M. O.\nLeach,“Stackedautoencodersforunsupervisedfeaturelearning\nand multiple organ detection in a pilot study using 4D patient\ndata,”IEEE Transactions on Pattern Analysis and Machine\nIntelligence,vol.35,no .8,p p .19 30–1943,2013.\n[ 7 5 ]J .L i ,C .X i a ,a n dX .C h e n ,“ Ab e n c h m a r kd a t a s e ta n ds a l i e n c y -\nguided stacked autoencoders for video-based salient object\ndetection,”IEEE Transactions on Image Processing,2017 .\n[76] D.Chen,X.Cao,F .W en,andJ.Sun,“Blessingofdimensionality:\nhigh-dimensional feature and its efficient compression for face\nverification,” inProceedings of the 26th IEEE Conference on\nComputerVisionandPatternRecognition(CVPR’13) ,pp.3025–\n3032,June2013.\n[77] X.Cao,D.Wipf,F .W en,G.Duan,andJ.Sun,“ Apracticaltrans-\nferlearningalgorithmforfaceverification,”in Proceedingsofthe\n14th IEEE International Conference on Computer Vision (ICCV\n’13),pp.3208–3215,December2013.\n[78] T.BergandP.N.Belhumeur,“Tom-vs-Peteclassifiersandiden-\ntity-preserving alignment for face verification,” inProceedings\nof the 23rd British Machine Vision Conference (BMVC ’12),p p .\n1–11,September2012.\n[79] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun, “Bayesian face\nrevisited:ajointformulation,”in ComputerVision—ECCV2012:\n12th European Conference on Computer Vision, Florence, Italy,\nOctober 7–13, 2012, Proceedings, Part III,v o l .7 5 7 4o fLecture\nNotes in Computer Science, pp. 566–579, Springer, Berlin,\nGermany,2012.\n[80] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back, “Face\nrecognition: a convolutional neural-network approach,”IEEE\nTransactions on Neural Networks and Learning Systems,v o l .8 ,\nno .1,p p .98–113,1997 .\n[81] X.Wu,R.He,Z.Sun,andT.Tan,AlightCNNfordeepfacerep-\nresentationwithnoisylabels,https://arxiv.org/abs/1511.02683.\n[82] O.M.Parkhi,A.Vedaldi,andA.Zisserman,“DeepFaceRecog-\nnition,” inProceedings of the British Machine Vision Conference\n2015,pp.41.1-41.12,Swansea.\n[83] F. Schroff, D. Kalenichenko, and J. Philbin, “FaceNet: a unified\nembedding for face recognition and clustering,” inProceedings\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR ’15), pp. 815–823, IEEE, Boston, Mass, USA, June\n2015.\n[84] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf,“DeepFace:clos-\ning the gap to human-level performance in face verification,”\nin Proceedings of the IEEE Conference on Computer Vision and\nPatternRecognition(CVPR’14) ,pp.1701–1708,Columbus,Ohio,\nUSA,June2014.ComputationalIntelligenceandNeuroscience 13\n[85] B. Amos, B. Ludwiczuk, and M. Satyanarayanan, “Openface: a\ngeneral-purpose face recognition library with mobile applica-\ntions,” CMU-CS-16-118, CMU School of Computer Science,\n2016.\n[86] A.S.V oulodimos,D.I.Kosmopoulos,N.D.Doulamis,andT .A.\nVarvarigou,“Atop-downevent-drivenapproachforconcurrent\nactivityrecognition,”MultimediaToolsandApplications,vol.69,\nno.2,pp.293–311,2014.\n[87] A.S.V oulodimos,N.D.Doulamis,D.I.Kosmopoulos,andT .A.\nVarvarigou, “Improving multi-camera activity recognition by\nemploying neural network based readjustment,”Applied Arti-\nficialIntelligence,vol.26,no .1 -2,p p .97 –118,2012.\n[88] K. Makantasis, A. Doulamis, N. Doulamis, and K. Psychas,\n“Deep learning based human behavior recognition in indus-\ntrial workflows,” inProceedings of the 23rd IEEE International\nConference on Image Processing, ICIP 2016,p p .1 6 0 9 – 1 6 1 3 ,\nSeptember2016.\n[8 9]C.Gan,N .W ang,Y .Y ang,D .-Y .Y eung,andA.G.H a u p tmann,\n“DevNet: A Deep Event Network for multimedia event detec-\ntion and evidence recounting,” in Proceedings of the IEEE\nConferenceonComputerVisionandPatternRecognition,CVPR\n2015,pp.2568–2577,USA,June2015.\n[90] T. Kautz, B. H. Groh, J. Hannink, U. Jensen, H. Strubberg, and\nB.M.Eskofier,“Activityrecognitioninbeachvolleyballusinga\nDEEpConvolutionalNeuralNETwork:leveragingthepotential\nof DEEp Learning in sports,”Data Mining and Knowledge\nDiscovery,vol.3 1,no .6,p p .167 8–1705,2017 .\n[91] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,\nandF.-F.Li,“Large-scalevideoclassificationwithconvolutional\nneuralnetworks,”in Proceedingsofthe27thIEEEConferenceon\nComputerVisionandPatternRecognition,(CVPR’14) ,pp.1725–\n17 3 2,Col umb us,OH,USA,J une201 4.\n[92] C. A. Ronao and S.-B. Cho, “Human activity recognition with\nsmartphone sensors using deep learning neural networks,”\nExpertSystemswithApplications ,vol.59 ,p p .235–2 44,2016.\n[ 9 3 ]J .S h a o ,C .C .L o y ,K .K a n g ,a n dX .W a n g ,“ C r o w d e dS c e n e\nUnderstanding by Deeply Learned Volumetric Slices,”IEEE\nTransactions on Circuits and Systems for Video Technology,v o l .\n27 ,no .3,p p .613–6 23,2017 .\n[94] K.Tang,B.Yao,L.Fei-Fei,andD.Koller,“Combiningtheright\nfeatures for complex event recognition,” inProceedings of the\n2013 14th IEEE International Conference on Computer Vision,\nICCV2013,pp.2696–2703,Australia,December2013.\n[95] S.Song,V .Chandrasekhar,B.Mandaletal.,“MultimodalMulti-\nStream Deep Learning forEgocentric Activity Recognition,” in\nProceedings of the 29th IEEE Conference on Computer Vision\nandPatternRecognitionWorkshops,CVPRW2016,pp.378–385,\nUSA,July2016.\n[96] R.Kavi,V.Kulathumani,F.Rohit,andV.Kecojevic,“Multiview\nfusion for activity recognition using deep neural networks,”\nJ o u r n a lo fE l e c t r o n i cI m a g i n g,v o l .2 5 ,n o .4 ,A r t i c l eI D0 4 3 0 1 0 ,\n2016.\n[97] H. Yalcin, “Human activity recognition using deep belief net-\nworks,” inP r o c e e d i n g so ft h e2 4 t hS i g n a lP r o c e s s i n ga n dC o m -\nmunicationApplicationConference,SIU2016 ,pp.1649–1652,tur,\nMay 2016.\n[98] A.Kitsikidis,K.Dimitropoulos,S.Douka,andN.Grammalidis,\n“Danceanalysisusingmultiplekinectsensors,”in Proceedingsof\nthe9thInternationalConferenceonComputerVisionTheoryand\nApplications,VISAPP2014,pp.789–795,prt,January2014.\n[99] P. F. Felzenszwalb and D. P. Huttenlocher, “Pictorial struc-\nturesforobjectrecognition,” InternationalJournalofComputer\nVision,vol.61,no .1,p p .55–79 ,2005.\n[100] A. Jain, J. Tompson, and M. Andriluka, “Learning human pose\nestimation features with convolutional networks,” inProceed-\nings of the ICLR,201 4.\n[101] J. J. Tompson, A. Jain, Y. LeCun et al., “Joint training of a con-\nvolutional network and a graphical model for human pose\nestimation,”in ProceedingsoftheNIPS ,201 4.\n[102] L.Fei-Fei,R.Fergus,andP.Perona,“One-shotlearningofobject\ncategories,”IEEETransactionsonPatternAnalysisandMachine\nIntelligence,vol.28,no .4,p p .594–611,2006.\n[ 1 0 3 ]A .K r i z h e v s k ya n dG .H i n t o n ,L e a r n i n gm u l t i p l el a y e r so ff e a -\ntures from tinyimages, 2009.\n[104] S.A.Nene,S.K.Nayar,andH.Murase,Columbiaobjectimage\nlibrary(coil-20),1996.\n[105] T.SkauliandJ.Farrell,“Acollectionofhyperspectralimagesfor\nimagingsystemsresearch,”in ProceedingsoftheDigitalPhotog-\nra\nphy IX,USA,F eb ruary2013.\n[106] M.F .Baumgardner ,L.L.Biehl,andD.A.Landgrebe,“220band\navirishyperspectralimagedataset:June12,1992indianpinetest\nsite3,” Datasets,2015.\n[107] E. Eidinger, R. Enbar, and T. Hassner, “Age and gender estima-\ntionofunfilteredfaces,” IEEETransactionson Information For-\nensicsandSecurity ,vol.9 ,no .12,p p .2170–2179 ,201 4.\n[ 1 0 8 ]G .B .H u a n g ,M .R a m e s h ,T .B e r g ,a n dE .L e a r n e d - M i l l e r ,\n“Labeled faces in the wild: A database for studying face recog-\nnition in unconstrained environments,” Tech. Rep., University\nof Massachusetts, Amherst, 2007.\n[109] X.W ang,Y .Peng,L.Lu,Z.Lu,M.Bagheri,andR.M.Summers,\n“ChestX-Ray8: Hospital-scale chest x-ray database and bench-\nmarks on weakly-supervised classification and localization of\ncommonthoraxdiseases,”in Proceedingsofthe2017IEEECon-\nferenceonComputerVisionandPatternRecognition(CVPR) ,pp.\n3462–3471, Honolulu, HI, May 2017.\n[110] A. Seff, L. Lu, A. Barbu, H. Roth, H.-C. Shin, and R. M. Sum-\nmers, “Leveraging mid-level semantic boundary cues for auto-\nmated lymph node detection,”Lecture Notes in Computer\nScience(includingsubseriesLectureNotesinArtificialIntelligence\nand Lecture Notes in Bioinformatics): Preface,v o l .9 3 50 ,p p .5 3 –\n61,2015.\n[111] A.Voulodimos,D.Kosmopoulos,G.Vasileiouetal.,“Adataset\nforworkflowrecognitioninindustrialscenes,”in Proceedingsof\nthe201118thIEEEInternationalConferenceonImageProcessing,\nICIP2011,pp.3249–3252,Belgium,September2011.\n[112] A. Voulodimos, D. Kosmopoulos, G. Vasileiou et al., “A three-\nfold dataset for activity and workflow recognition in complex\nindustrial environments,”IEEE MultiMedia,v o l .1 9 ,n o .3 ,p p .\n42–52,2012.\n[113] D. I. Kosmopoulos, A. S. Voulodimos, and A. D. Doulamis, “A\nsystem for multicamera task recognition and summarization\nfor structured environments,”IEEE Transactions on Industrial\nInformatics,vol.9 ,no .1,p p .161 –171,2013.\n[114] S. Abu-El-Haija et al., “YouTube-8M: A large-scale video clas-\nsification benchmark,” Tech. Rep., 2016, https://arxiv.org/abs/\n1609.08675."
    }
}